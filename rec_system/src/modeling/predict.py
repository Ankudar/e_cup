import gc
import logging
import os
import pickle
import sys
import time
from datetime import datetime, timedelta

import dask.dataframe as dd
import numpy as np
import pandas as pd
from tqdm import tqdm

# --- –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)

MODEL_PATH = "/home/root6/python/e_cup/rec_system/src/models/lgbm_model_full.pkl"
TEST_USERS_PATH = "/home/root6/python/e_cup/rec_system/data/raw/test_users/*.parquet"
OUTPUT_PATH = "/home/root6/python/e_cup/rec_system/result/submission.csv"
TOP_K = 100
USER_BATCH_SIZE = 1000
CANDIDATES_PER_USER = 2000

# --- –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ---
logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ %s", MODEL_PATH)
with open(MODEL_PATH, "rb") as f:
    model_data = pickle.load(f)

# –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –≤ model_data
logger.info(f"–¢–∏–ø model_data: {type(model_data)}")

if isinstance(model_data, dict):
    logger.info(f"–ö–ª—é—á–∏ –≤ model_data: {list(model_data.keys())}")
    for key, value in model_data.items():
        logger.info(f"model_data['{key}'] —Ç–∏–ø: {type(value)}")

    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –º–æ–¥–µ–ª—å –∏ feature_columns
    model = None
    feature_columns = None

    # –ò—â–µ–º –º–æ–¥–µ–ª—å
    for key, value in model_data.items():
        if hasattr(value, "predict") or "lightgbm" in str(type(value)).lower():
            model = value
            logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å –≤ –∫–ª—é—á–µ: {key}")
            break

    # –ò—â–µ–º feature_columns
    for key, value in model_data.items():
        if (
            isinstance(value, (list, tuple))
            and len(value) > 0
            and isinstance(value[0], str)
        ):
            feature_columns = value
            logger.info(f"–ù–∞–π–¥–µ–Ω—ã feature_columns –≤ –∫–ª—é—á–µ: {key}")
            break

    if model is None:
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–º–æ–∂–Ω–æ model_data –∏ –µ—Å—Ç—å –º–æ–¥–µ–ª—å
        if hasattr(model_data, "predict"):
            model = model_data
            logger.info("model_data —è–≤–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å—é")
        else:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –º–æ–¥–µ–ª—å –≤ model_data")

    if feature_columns is None:
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å feature_columns –∏–∑ –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∏—á
        try:
            if hasattr(model, "feature_name_"):
                feature_columns = model.feature_name_
            elif hasattr(model, "feature_names"):
                feature_columns = model.feature_names
            else:
                # –°–æ–∑–¥–∞–µ–º feature_columns –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–∂–∏–¥–∞–µ–º—ã—Ö —Ñ–∏—á
                feature_columns = [
                    "user_count",
                    "user_mean",
                    "user_orders_count",
                    "item_count",
                    "item_orders_count",
                    "item_category",
                ]
                logger.warning(
                    f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ feature_columns: {feature_columns}"
                )
        except:
            feature_columns = [
                "user_count",
                "user_mean",
                "user_orders_count",
                "item_count",
                "item_orders_count",
                "item_category",
            ]
            logger.warning(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ feature_columns: {feature_columns}")

else:
    # model_data –Ω–µ —Å–ª–æ–≤–∞—Ä—å, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ —Å–∞–º–∞ –º–æ–¥–µ–ª—å
    if hasattr(model_data, "predict"):
        model = model_data
        logger.info("model_data —è–≤–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å—é")
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å feature_columns
        try:
            if hasattr(model, "feature_name_"):
                feature_columns = model.feature_name_
            elif hasattr(model, "feature_names"):
                feature_columns = model.feature_names
            else:
                feature_columns = [
                    "user_count",
                    "user_mean",
                    "user_orders_count",
                    "item_count",
                    "item_orders_count",
                    "item_category",
                ]
                logger.warning(
                    f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ feature_columns: {feature_columns}"
                )
        except:
            feature_columns = [
                "user_count",
                "user_mean",
                "user_orders_count",
                "item_count",
                "item_orders_count",
                "item_category",
            ]
            logger.warning(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ feature_columns: {feature_columns}")
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø model_data: {type(model_data)}")

logger.info(f"–¢–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")
logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á: {len(feature_columns)}")
logger.info(f"–ü–µ—Ä–≤—ã–µ 10 —Ñ–∏—á: {feature_columns[:10]}")


# ===== –í–ï–ö–¢–û–†–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ü–û–î–ì–û–¢–û–í–ö–ò –§–ò–ß =====
def prepare_features_vectorized(
    user_id, candidates, user_features_dict, item_features_dict, feature_columns
):
    """–í–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
    num_candidates = len(candidates)

    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features_matrix = np.zeros((num_candidates, len(feature_columns)), dtype=np.float32)

    # –ü–æ–ª—É—á–∞–µ–º user features
    user_feats = user_features_dict.get(user_id, {})

    # –ó–∞–ø–æ–ª–Ω—è–µ–º user features
    for j, feat_name in enumerate(feature_columns):
        if feat_name in user_feats:
            features_matrix[:, j] = user_feats[feat_name]

    # –ó–∞–ø–æ–ª–Ω—è–µ–º item features
    for j, feat_name in enumerate(feature_columns):
        if any(
            feat_name.startswith(prefix) for prefix in ["item_", "fclip_", "category_"]
        ):
            for i, item_id in enumerate(candidates):
                item_feats = item_features_dict.get(item_id, {})
                if feat_name in item_feats:
                    features_matrix[i, j] = item_feats[feat_name]

    return features_matrix


# ===== –ì–ï–ù–ï–†–ê–¶–ò–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô =====
def get_user_recommendations(user_id, top_k=100, **kwargs):
    """–°–£–ü–ï–†-–ë–´–°–¢–†–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è: –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π"""
    try:
        recent_items_get = kwargs.get("recent_items_get")
        popular_items_array = kwargs.get("popular_items_array")
        model = kwargs.get("model")
        feature_columns = kwargs.get("feature_columns")
        copurchase_map = kwargs.get("copurchase_map")
        item_to_cat = kwargs.get("item_to_cat")
        cat_to_items = kwargs.get("cat_to_items")
        item_map = kwargs.get("item_map")
        user_features_dict = kwargs.get("user_features_dict")
        item_features_dict = kwargs.get("item_features_dict")

        # –Ω–µ–¥–∞–≤–Ω–∏–µ —Ç–æ–≤–∞—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        recent_items = recent_items_get(user_id, [])

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates = set()

        N_RECENT = 5
        N_COPURCHASE = 5
        N_CATEGORY = 5
        N_POPULAR = 30

        # 1. –ù–µ–¥–∞–≤–Ω–∏–µ —Ç–æ–≤–∞—Ä—ã
        candidates.update(recent_items[:N_RECENT])

        # 2. Co-purchase —Ç–æ–≤–∞—Ä—ã
        for item in recent_items[:10]:
            co_items = copurchase_map.get(item, [])
            candidates.update(co_items[:N_COPURCHASE])

        # 3. –¢–æ–≤–∞—Ä—ã –∏–∑ —Ç–µ—Ö –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for item in recent_items[:5]:
            cat_id = item_to_cat.get(item)
            if cat_id and cat_id in cat_to_items:
                candidates.update(cat_to_items[cat_id][:N_CATEGORY])

        # 4. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –∫–∞–∫ fallback
        candidates.update(popular_items_array[:N_POPULAR])

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã
        candidates = [c for c in candidates if c in item_map]

        if not candidates:
            return popular_items_array[:top_k].tolist()

        # === üî• –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π cut-off –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ ===
        if len(recent_items) < 3:
            max_cands = 500  # –¥–ª—è "–Ω–æ–≤—ã—Ö" –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        else:
            max_cands = 300  # –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

        if len(candidates) > max_cands:
            # –æ—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ä–µ–¥–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            popularity_rank = {
                item: idx for idx, item in enumerate(popular_items_array)
            }
            candidates = sorted(candidates, key=lambda x: popularity_rank.get(x, 1e9))[
                :max_cands
            ]

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_candidate = prepare_features_vectorized(
            user_id, candidates, user_features_dict, item_features_dict, feature_columns
        )

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = model.predict(X_candidate)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –ø–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K
        sorted_indices = np.argsort(predictions)[::-1][:top_k]
        top_recs = [candidates[i] for i in sorted_indices]

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏, –µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
        if len(top_recs) < top_k:
            additional_items = []
            for item in popular_items_array:
                if item not in top_recs and item not in additional_items:
                    additional_items.append(item)
                if len(top_recs) + len(additional_items) >= top_k:
                    break
            top_recs.extend(additional_items)
            top_recs = top_recs[:top_k]

        return top_recs

    except Exception as e:
        logger.error(f"Error for user {user_id}: {e}")
        return popular_items_array[:top_k].tolist()


# ===== –ö–≠–® –î–õ–Ø –ü–û–•–û–ñ–ò–• –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô =====
user_recommendation_cache = {}
similar_user_threshold = 5


def get_user_recommendations_with_cache(user_id, top_k=100, **kwargs):
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    recent_items_get = kwargs.get("recent_items_get")

    if user_id in user_recommendation_cache:
        return user_recommendation_cache[user_id]

    recent_items = recent_items_get(user_id, [])

    for cached_user_id, cached_recs in user_recommendation_cache.items():
        cached_recent = recent_items_get(cached_user_id, [])
        if len(set(recent_items) & set(cached_recent)) >= similar_user_threshold:
            user_recommendation_cache[user_id] = cached_recs
            return cached_recs

    recs = get_user_recommendations(user_id, top_k, **kwargs)
    if len(user_recommendation_cache) < 10000:
        user_recommendation_cache[user_id] = recs

    return recs


# ===== –û–ë–†–ê–ë–û–¢–ö–ê –í–°–ï–• –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô =====
def generate_recommendations_for_users(
    test_users,
    model,
    feature_columns,
    recent_items_map,
    copurchase_map,
    item_to_cat,
    cat_to_items,
    user_features_dict,
    item_features_dict,
    item_map,
    popular_items,
    K,
    log_message,
    output_path=None,
):
    log_message("=== –ì–ï–ù–ï–†–ê–¶–ò–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô (super_fast + cache) ===")
    stage_start = time.time()

    popular_items_array = np.array(popular_items, dtype=np.int64)
    recent_items_get = recent_items_map.get

    recommendations = {}
    processed = 0
    batch_size = 100  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±–∞—Ç—á –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    header_written = False

    with tqdm(total=len(test_users), desc="–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π") as pbar:
        for i in range(0, len(test_users), batch_size):
            batch_users = test_users[i : i + batch_size]

            for user_id in batch_users:
                try:
                    recommendations[user_id] = get_user_recommendations_with_cache(
                        user_id,
                        K,
                        recent_items_get=recent_items_get,
                        popular_items_array=popular_items_array,
                        model=model,
                        feature_columns=feature_columns,
                        copurchase_map=copurchase_map,
                        item_to_cat=item_to_cat,
                        cat_to_items=cat_to_items,
                        item_map=item_map,
                        user_features_dict=user_features_dict,
                        item_features_dict=item_features_dict,
                    )
                except Exception as e:
                    recommendations[user_id] = popular_items_array[:K].tolist()
                    log_message(f"–û—à–∏–±–∫–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")

                processed += 1
                pbar.update(1)

                if output_path and processed % 10000 == 0:
                    save_recommendations_to_csv(
                        recommendations,
                        output_path,
                        log_message,
                        header=not header_written,
                    )
                    header_written = True
                    recommendations.clear()
                    gc.collect()

    if output_path and recommendations:
        save_recommendations_to_csv(
            recommendations,
            output_path,
            log_message,
            header=not header_written,
        )

    stage_time = time.time() - stage_start
    log_message(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

    return recommendations


def save_recommendations_to_csv(recommendations, output_path, log_message, header=True):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    mode = "w" if header else "a"
    with open(output_path, mode, encoding="utf-8", buffering=16384) as f:
        if header:
            f.write("user_id,item_id_1 item_id_2 ... item_id_100\n")
        for user_id, items in recommendations.items():
            items_str = " ".join(str(int(item)) for item in items)
            f.write(f"{int(user_id)},{items_str}\n")


if __name__ == "__main__":
    start_time = time.time()
    log_file = "/home/root6/python/e_cup/rec_system/predict_log.txt"

    def log_message(message: str):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        msg = f"[{timestamp}] {message}"
        print(msg)
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(msg + "\n")

    try:
        log_message("=== –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π ===")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        test_df = dd.read_parquet(TEST_USERS_PATH).compute()
        test_users = test_df["user_id"].unique().tolist()
        log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        data_paths = {
            "recent_items_map": "/home/root6/python/e_cup/rec_system/data/processed/recent_items_map.pkl",
            "copurchase_map": "/home/root6/python/e_cup/rec_system/data/processed/copurchase_map.pkl",
            "item_to_cat": "/home/root6/python/e_cup/rec_system/data/processed/item_to_cat.pkl",
            "cat_to_items": "/home/root6/python/e_cup/rec_system/data/processed/cat_to_items.pkl",
            "user_features_dict": "/home/root6/python/e_cup/rec_system/data/processed/user_features_dict.pkl",
            "item_features_dict": "/home/root6/python/e_cup/rec_system/data/processed/item_features_dict.pkl",
            "item_map": "/home/root6/python/e_cup/rec_system/data/processed/item_map.pkl",
            "popular_items": "/home/root6/python/e_cup/rec_system/data/processed/popular_items.pkl",
        }

        loaded_data = {}
        for name, path in data_paths.items():
            try:
                with open(path, "rb") as f:
                    loaded_data[name] = pickle.load(f)
                log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω {name}")
            except Exception as e:
                log_message(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {name}: {e}")
                raise

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        recommendations = generate_recommendations_for_users(
            test_users=test_users,
            model=model,
            feature_columns=feature_columns,
            recent_items_map=loaded_data["recent_items_map"],
            copurchase_map=loaded_data["copurchase_map"],
            item_to_cat=loaded_data["item_to_cat"],
            cat_to_items=loaded_data["cat_to_items"],
            user_features_dict=loaded_data["user_features_dict"],
            item_features_dict=loaded_data["item_features_dict"],
            item_map=loaded_data["item_map"],
            popular_items=loaded_data["popular_items"],
            K=TOP_K,
            log_message=log_message,
            output_path=OUTPUT_PATH,
        )

        log_message("=== –ó–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ ===")

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞: {str(e)}"
        log_message(error_msg)
        raise

# —á—Ç–æ —Å–µ–π—á–∞—Å —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è
# 1) –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
# 2) ALS —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ - –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è
# 3) –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è - –≤–µ—Å: 2.0 (page_view)
# 4) –î–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –∏–∑–±—Ä–∞–Ω–Ω–æ–µ - –≤–µ—Å: 5.0 (favorite)
# 5) –î–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –∫–æ—Ä–∑–∏–Ω—É - –≤–µ—Å: 10.0 (to_cart)
# 6) –§–∞–∫—Ç–æ—Ä –≤—Ä–µ–º–µ–Ω–∏ - –±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏–º–µ—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å
# 7) –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–æ–≤ - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å—á–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
# 8) –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã - 5 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö items
# 9) –ü–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã - –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –≤ ALS –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
# 10) –¢–æ–≤–∞—Ä—ã –∏–∑ —Ç–æ–π –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
# 11) –°–æ–≤–º–µ—Å—Ç–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏ - —Ç–æ–≤–∞—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ
# 12) FCLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ - –≤–∏–∑—É–∞–ª—å–Ω–æ-—Ç–µ–∫—Å—Ç–æ–≤—ã–µ embeddings —Ç–æ–≤–∞—Ä–æ–≤
# 13) User ALS —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
# 14) Item ALS —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤
# –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥ —Å –≤–µ—Å–∞–º–∏ ‚Üí —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é ‚Üí —Ç–æ–ø-100
