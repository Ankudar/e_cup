import gc
import glob
import glob as glob_module
import json
import os
import pickle
import random
import shutil
import tempfile
import time
import traceback
import uuid
import warnings
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from glob import glob
from pathlib import Path

import dask
import dask.dataframe as dd
import lightgbm as lgb
import numpy as np
import pandas as pd
import polars as pl
import psutil
import pyarrow as pa
import pyarrow.parquet as pq
import torch
import torch.nn.functional as F
import torch.sparse
from dask.diagnostics import ProgressBar
from implicit.als import AlternatingLeastSquares
from scipy.sparse import coo_matrix, csr_matrix, vstack
from sklearn.metrics import ndcg_score
from sklearn.model_selection import train_test_split
from torch import nn
from tqdm import tqdm
from tqdm.auto import tqdm

warnings.filterwarnings(
    "ignore", category=FutureWarning, message=".*Downcasting object dtype arrays.*"
)
# tqdm –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å pandas
tqdm.pandas()

MAX_FILES = 0  # —Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –±–µ—Ä–µ–º –≤ —Ä–∞–±–æ—Ç—É. 0 - –≤—Å–µ
MAX_ROWS = 100_000  # —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –±–µ—Ä–µ–º –≤ —Ä–∞–±–æ—Ç—É. 0 - –≤—Å–µ
ITER_N = 5  # —á–∏—Å–ª–æ —ç–ø–æ—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
EARLY_STOP = 10  # —Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è
EMD_LENGHT = 100


def find_parquet_files(folder):
    files = glob(os.path.join(folder, "**", "*.parquet"), recursive=True)
    files.sort()
    return files


# -------------------- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö --------------------
def load_train_data(max_parts=MAX_FILES, max_rows=MAX_ROWS):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º parquet-—Ñ–∞–π–ª—ã orders, tracker, items, categories_tree, test_users.
    –ò—â–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ –ø–∞–ø–∫–∞–º –≤—Å–µ .parquet —Ñ–∞–π–ª—ã. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫.
    """

    paths = {
        "orders": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_orders_data/",
        "tracker": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_tracker_data/",
        "items": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_items_data/",
        "categories": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_categories_tree/",
        "test_users": "/home/root6/python/e_cup/rec_system/data/raw/test_users/",
    }

    columns_map = {
        "orders": ["item_id", "user_id", "created_timestamp", "last_status"],
        "tracker": ["item_id", "user_id", "timestamp", "action_type"],
        "items": ["item_id", "itemname", "fclip_embed", "catalogid"],
        "categories": ["catalogid", "catalogpath", "ids"],
        "test_users": ["user_id"],
    }

    dtype_profiles = {
        "orders": {
            "user_id": "int32",
            "item_id": "int32",
            "created_timestamp": "datetime64[ns]",
            "last_status": "category",
        },
        "tracker": {
            "user_id": "int32",
            "item_id": "int32",
            "timestamp": "datetime64[ns]",
            "action_type": "category",
        },
        "items": {
            "item_id": "int32",
            "catalogid": "int32",
            "itemname": "string",
            "fclip_embed": "object",
        },
        "categories": {
            "catalogid": "int32",
            "catalogpath": "string",
            "ids": "string",
        },
        "test_users": {"user_id": "int32"},
    }

    def read_sample(
        folder, columns=None, name="", max_parts=max_parts, max_rows=max_rows
    ):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ–º parquet-—Ñ–∞–π–ª—ã —Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞.
        """
        files = find_parquet_files(folder)
        if not files:
            log_message(f"{name}: parquet —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {folder}")
            return dd.from_pandas(pd.DataFrame(), npartitions=1)

        current_dtypes = dtype_profiles.get(name, {})

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —á—Ç–µ–Ω–∏—è
        original_file_count = len(files)
        if max_parts > 0 and max_parts < original_file_count:
            files = files[:max_parts]
            used_files = max_parts
        else:
            used_files = original_file_count

        log_message(
            f"{name}: –Ω–∞–π–¥–µ–Ω–æ {original_file_count} —Ñ–∞–π–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º {used_files}"
        )

        try:
            # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            ddf = dd.read_parquet(
                files,
                engine="pyarrow",
                dtype=current_dtypes,
                columns=columns,
                gather_statistics=False,
                split_row_groups=False,
            )
        except Exception as e:
            log_message(f"{name}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ parquet ({e}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return dd.from_pandas(pd.DataFrame(), npartitions=1)

        # –ï—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–∏
        if max_rows == 0:
            try:
                count = ddf.shape[0].compute()
                mem_estimate = ddf.memory_usage(deep=True).sum().compute() / (1024**2)
                log_message(
                    f"{name}: {count:,} —Å—Ç—Ä–æ–∫ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_files} —Ñ–∞–π–ª–æ–≤), ~{mem_estimate:.1f} MB"
                )
            except Exception as e:
                log_message(f"{name}: –Ω–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä: {e}")
            return ddf

        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Å—Ç—Ä–æ–∫–∏
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            actual_rows = ddf.shape[0].compute()

            if actual_rows <= max_rows:
                # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ –ª–∏–º–∏—Ç–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—ë
                mem_estimate = ddf.memory_usage(deep=True).sum().compute() / (1024**2)
                log_message(
                    f"{name}: {actual_rows:,} —Å—Ç—Ä–æ–∫ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_files} —Ñ–∞–π–ª–æ–≤), ~{mem_estimate:.1f} MB"
                )
                return ddf
            else:
                # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–µ –ª–∏–º–∏—Ç–∞ - —Å–µ–º–ø–ª–∏—Ä—É–µ–º
                fraction = max_rows / actual_rows
                sampled_ddf = ddf.sample(frac=fraction, random_state=42)

                sampled_count = sampled_ddf.shape[0].compute()
                mem_estimate = sampled_ddf.memory_usage(deep=True).sum().compute() / (
                    1024**2
                )

                log_message(
                    f"{name}: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ {sampled_count:,} –∏–∑ {actual_rows:,} —Å—Ç—Ä–æ–∫ "
                    f"(—Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ {fraction:.1%}, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_files} —Ñ–∞–π–ª–æ–≤), ~{mem_estimate:.1f} MB"
                )
                return sampled_ddf

        except Exception as e:
            log_message(
                f"{name}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–∏ —Å—Ç—Ä–æ–∫ ({e}), –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ {max_rows} —Å—Ç—Ä–æ–∫"
            )

            # Fallback: –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ max_rows —Å—Ç—Ä–æ–∫
            try:
                limited_ddf = ddf.head(max_rows, npartitions=-1)
                limited_count = limited_ddf.shape[0].compute()
                mem_estimate = limited_ddf.memory_usage(deep=True).sum().compute() / (
                    1024**2
                )

                log_message(
                    f"{name}: {limited_count:,} —Å—Ç—Ä–æ–∫ (–≤–∑—è—Ç–æ –ø–µ—Ä–≤—ã—Ö {max_rows}, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_files} —Ñ–∞–π–ª–æ–≤), ~{mem_estimate:.1f} MB"
                )
                return limited_ddf
            except Exception as e2:
                log_message(
                    f"{name}: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame: {e2}"
                )
                return dd.from_pandas(pd.DataFrame(), npartitions=1)

    log_message("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    orders_ddf = read_sample(
        paths["orders"], columns=columns_map["orders"], name="orders"
    )
    tracker_ddf = read_sample(
        paths["tracker"], columns=columns_map["tracker"], name="tracker"
    )
    items_ddf = read_sample(paths["items"], columns=columns_map["items"], name="items")
    categories_ddf = read_sample(
        paths["categories"], columns=columns_map["categories"], name="categories"
    )
    test_users_ddf = read_sample(
        paths["test_users"], columns=columns_map["test_users"], name="test_users"
    )
    log_message("–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

    return (orders_ddf, tracker_ddf, items_ddf, categories_ddf, test_users_ddf)


# -------------------- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö --------------------
def filter_data(orders_ddf, tracker_ddf, items_ddf):
    """
    –§–∏–ª—å—Ç—Ä—É–µ–º: –æ—Å—Ç–∞–≤–ª—è–µ–º delivered_orders (–ø–æ–∑–∏—Ç–∏–≤) –∏ canceled_orders (–Ω–µ–≥–∞—Ç–∏–≤),
    –∞ —Ç–∞–∫–∂–µ –¥–µ–π—Å—Ç–≤–∏—è page_view, favorite, to_cart.
    –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –ø–æ—Ç–æ–º –ø—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.
    """
    log_message("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")

    orders_ddf = (
        orders_ddf.drop_duplicates(subset=["user_id", "item_id", "last_status"])
        .loc[lambda df: df["last_status"].isin(["delivered_orders", "canceled_orders"])]
        .assign(
            target=lambda df: df["last_status"].map(
                {"delivered_orders": 1, "canceled_orders": 0}, meta=("target", "int8")
            )
        )
    )

    tracker_ddf = tracker_ddf.drop_duplicates(
        subset=["user_id", "item_id", "action_type", "timestamp"]
    ).loc[lambda df: df["action_type"].isin(["page_view", "favorite", "to_cart"])]

    items_ddf = items_ddf.drop_duplicates(subset=["item_id"])

    log_message("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return orders_ddf, tracker_ddf, items_ddf


# -------------------- Train/Test split –ø–æ –≤—Ä–µ–º–µ–Ω–∏ --------------------
def train_test_split_by_time(orders_df, test_size=0.2):
    """
    –î–µ–ª–µ–Ω–∏–µ –ø–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –¥–∞—Ç–µ: train = –ø–µ—Ä–≤—ã–µ (1 - test_size) –ø–æ –≤—Ä–µ–º–µ–Ω–∏,
    test = –ø–æ—Å–ª–µ–¥–Ω–∏–µ test_size –ø–æ –≤—Ä–µ–º–µ–Ω–∏.
    """
    orders_df = orders_df.copy()
    orders_df["created_timestamp"] = pd.to_datetime(orders_df["created_timestamp"])
    orders_df = orders_df.sort_values("created_timestamp")

    cutoff_idx = int(len(orders_df) * (1 - test_size))
    cutoff_ts = orders_df.iloc[cutoff_idx]["created_timestamp"]

    train_df = orders_df[orders_df["created_timestamp"] <= cutoff_ts]
    test_df = orders_df[orders_df["created_timestamp"] > cutoff_ts]

    return (
        train_df.reset_index(drop=True),
        test_df.reset_index(drop=True),
        cutoff_ts,
    )


# -------------------- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π --------------------
def prepare_interactions(
    train_orders_df,
    tracker_ddf,
    cutoff_ts_per_user,
    batch_size=1_000_000,  # –£–º–µ–Ω—å—à–µ–Ω —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    action_weights=None,
    scale_days=5,
    output_dir="/home/root6/python/e_cup/rec_system/data/processed/prepare_interactions_batches",
    force_recreate=False,
    max_tracker_files=10,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ tracker
):
    log_message("–§–æ—Ä–º–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø–æ –±–∞—Ç—á–∞–º...")

    if action_weights is None:
        action_weights = {"page_view": 2, "favorite": 5, "to_cart": 10}

    os.makedirs(output_dir, exist_ok=True)
    batch_files = []
    ref_time = train_orders_df["created_timestamp"].max()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ñ–∞–π–ª—ã
    existing_files = glob_module.glob(os.path.join(output_dir, "*.parquet"))
    if existing_files and not force_recreate:
        log_message(
            f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(existing_files)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ"
        )
        return existing_files

    # ====== Orders ======
    log_message("... –æ–±—Ä–∞–±–æ—Ç–∫–∞ orders")
    n_rows = len(train_orders_df)
    orders_files = []

    for start in range(0, n_rows, batch_size):
        batch_path = os.path.join(output_dir, f"orders_batch_{start}.parquet")
        orders_files.append(batch_path)

        if os.path.exists(batch_path) and not force_recreate:
            log_message(f"‚úÖ Orders –±–∞—Ç—á {start} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            batch_files.append(batch_path)
            continue

        end_idx = min(start + batch_size, n_rows)
        batch = train_orders_df.iloc[start:end_idx].copy()

        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
        days_ago = (ref_time - batch["created_timestamp"]).dt.days.clip(lower=1)
        time_factor = np.log1p(days_ago / scale_days)

        result_batch = pd.DataFrame(
            {
                "user_id": batch["user_id"],
                "item_id": batch["item_id"],
                "weight": 5.0 * time_factor,
                "timestamp": batch["created_timestamp"],
                "action_type": "order",
            }
        )

        result_batch.to_parquet(batch_path, index=False, engine="pyarrow")
        batch_files.append(batch_path)

        log_message(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω orders-–±–∞—Ç—á {start}-{end_idx}")

        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        del batch, result_batch
        gc.collect()

    # ====== Tracker ======
    log_message("... –æ–±—Ä–∞–±–æ—Ç–∫–∞ tracker")
    tracker_files = []

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ tracker –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    tracker_path = "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_tracker_data/"
    all_tracker_files = find_parquet_files(tracker_path)

    if max_tracker_files > 0:
        all_tracker_files = all_tracker_files[:max_tracker_files]

    log_message(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(all_tracker_files)} —Ñ–∞–π–ª–æ–≤ tracker")

    for file_idx, file_path in enumerate(all_tracker_files):
        batch_path = os.path.join(output_dir, f"tracker_file_{file_idx}.parquet")
        tracker_files.append(batch_path)

        if os.path.exists(batch_path) and not force_recreate:
            log_message(f"‚úÖ Tracker —Ñ–∞–π–ª {file_idx} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            batch_files.append(batch_path)
            continue

        try:
            # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            part = pd.read_parquet(
                file_path, columns=["user_id", "item_id", "timestamp", "action_type"]
            )

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º
            part = part[part["action_type"].isin(action_weights.keys())]

            if part.empty:
                # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª
                pd.DataFrame(
                    columns=["user_id", "item_id", "weight", "timestamp", "action_type"]
                ).to_parquet(batch_path, index=False)
                batch_files.append(batch_path)
                continue

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
            part["timestamp"] = pd.to_datetime(part["timestamp"])
            mask = part["timestamp"] < cutoff_ts_per_user
            part = part.loc[mask]

            if part.empty:
                pd.DataFrame(
                    columns=["user_id", "item_id", "weight", "timestamp", "action_type"]
                ).to_parquet(batch_path, index=False)
                batch_files.append(batch_path)
                continue

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
            aw = part["action_type"].map(action_weights)
            days_ago = (ref_time - part["timestamp"]).dt.days.clip(lower=1)
            time_factor = np.log1p(days_ago / scale_days)

            result_part = pd.DataFrame(
                {
                    "user_id": part["user_id"],
                    "item_id": part["item_id"],
                    "weight": aw * time_factor,
                    "timestamp": part["timestamp"],
                    "action_type": part["action_type"],
                }
            )

            result_part.to_parquet(batch_path, index=False, engine="pyarrow")
            batch_files.append(batch_path)

            log_message(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω tracker —Ñ–∞–π–ª {file_idx}: {len(part)} —Å—Ç—Ä–æ–∫")

            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            del part, result_part
            gc.collect()

        except Exception as e:
            log_message(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ tracker —Ñ–∞–π–ª–∞ {file_path}: {e}")
            continue

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å
    expected_files = orders_files + tracker_files
    missing_files = [f for f in expected_files if not os.path.exists(f)]

    if missing_files:
        log_message(f"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç {len(missing_files)} —Ñ–∞–π–ª–æ–≤")
    else:
        log_message("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã")

    log_message(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {len(batch_files)}")
    return batch_files


def check_interactions_files_complete(output_dir):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤—Å–µ –ª–∏ —Ñ–∞–π–ª—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –Ω–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã"""
    import pyarrow.parquet as pq

    files = glob.glob(os.path.join(output_dir, "*.parquet"))
    if not files:
        return False, "–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"

    valid_files = []
    corrupted_files = []

    for file_path in files:
        try:
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –º–æ–∂–Ω–æ –ª–∏ –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª
            table = pq.read_table(file_path)
            if table.num_rows > 0:
                valid_files.append(file_path)
            else:
                corrupted_files.append(file_path)
        except Exception as e:
            corrupted_files.append(file_path)
            log_message(f"‚ùå –§–∞–π–ª {file_path} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω: {e}")

    return (
        len(corrupted_files) == 0,
        f"–í–∞–ª–∏–¥–Ω—ã—Ö: {len(valid_files)}, –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö: {len(corrupted_files)}",
    )


# -------------------- –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å --------------------
def compute_global_popularity(orders_df, cutoff_ts_info):
    """
    –°—á–∏—Ç–∞–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–õ–¨–ö–û —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤.

    Args:
        orders_df: –í—Å–µ –∑–∞–∫–∞–∑—ã (–¥–æ split)
        cutoff_ts_info: –ª–∏–±–æ —Å–ª–æ–≤–∞—Ä—å {user_id: cutoff_ts}, –ª–∏–±–æ –æ–¥–∏–Ω –≥–ª–æ–±–∞–ª—å–Ω—ã–π pd.Timestamp
    """
    log_message("–°—á–∏—Ç–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

    orders_df = orders_df.copy()
    orders_df["created_timestamp"] = pd.to_datetime(orders_df["created_timestamp"])

    if isinstance(cutoff_ts_info, dict):
        # –ü–æ –∫–∞–∂–¥–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å–≤–æ–π cutoff
        train_orders = []
        for user_id, cutoff_ts in cutoff_ts_info.items():
            user_orders = orders_df[
                (orders_df["user_id"] == user_id)
                & (orders_df["created_timestamp"] < cutoff_ts)
            ]
            train_orders.append(user_orders)
        train_orders_df = (
            pd.concat(train_orders, ignore_index=True)
            if train_orders
            else pd.DataFrame(columns=orders_df.columns)
        )

    else:
        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π cutoff (–æ–¥–Ω–∞ –¥–∞—Ç–∞ –¥–ª—è –≤—Å–µ—Ö)
        cutoff_ts = cutoff_ts_info
        train_orders_df = orders_df[orders_df["created_timestamp"] < cutoff_ts]

    # –°—á–∏—Ç–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if train_orders_df.empty:
        log_message("–ù–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏.")
        return pd.Series(dtype=float)

    pop = (
        train_orders_df.groupby("item_id")["item_id"]
        .count()
        .sort_values(ascending=False)
    )
    popularity = pop / pop.max()
    log_message(
        f"–ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ {len(train_orders_df)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–∫–∞–∑–∞—Ö"
    )
    return popularity


# -------------------- –û–±—É—á–µ–Ω–∏–µ ALS --------------------
def train_als(interactions_files, n_factors=64, reg=1e-3, device="cuda"):
    """
    –í–µ—Ä—Å–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –±–∞—Ç—á–µ–π –Ω–∞ –¥–∏—Å–∫ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ item_map.pkl
    """
    # 1. –ü–†–û–•–û–î: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤
    user_set = set()
    item_set = set()
    log_message("–ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤...")

    for f in tqdm(interactions_files):
        df = pl.scan_parquet(f).select(["user_id", "item_id"]).collect()
        user_set.update(df["user_id"].unique().to_list())
        item_set.update(df["item_id"].unique().to_list())

    user_map = {u: i for i, u in enumerate(sorted(user_set))}
    item_map = {i: j for j, i in enumerate(sorted(item_set))}
    log_message(
        f"–ú–∞–ø–ø–∏–Ω–≥–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã. –£–Ω–∏–∫–æ–≤: users={len(user_map)}, items={len(item_map)}"
    )

    # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º item_map.pkl
    map_dir = "/home/root6/python/e_cup/rec_system/data/processed/"
    os.makedirs(map_dir, exist_ok=True)
    item_map_path = os.path.join(map_dir, "item_map.pkl")
    with open(item_map_path, "wb") as f:
        pickle.dump(item_map, f)
    log_message(f"item_map —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {item_map_path}")

    # 2. –ü–†–û–•–û–î: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞—Ç—á–µ–π –Ω–∞ –¥–∏—Å–∫
    log_message("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞—Ç—á–µ–π –Ω–∞ –¥–∏—Å–∫...")

    batch_dir = "/home/root6/python/e_cup/rec_system/data/processed/als_batches/"
    os.makedirs(batch_dir, exist_ok=True)

    user_map_df = pl.DataFrame(
        {"user_id": list(user_map.keys()), "user_idx": list(user_map.values())}
    )
    item_map_df = pl.DataFrame(
        {"item_id": list(item_map.keys()), "item_idx": list(item_map.values())}
    )

    batch_files = []
    for i, f in enumerate(tqdm(interactions_files)):
        df = pl.read_parquet(f, columns=["user_id", "item_id", "weight"])

        df = df.join(user_map_df, on="user_id", how="inner")
        df = df.join(item_map_df, on="item_id", how="inner")

        if len(df) > 0:
            batch_path = os.path.join(batch_dir, f"batch_{i:04d}.npz")
            np.savez(
                batch_path,
                rows=df["user_idx"].to_numpy().astype(np.int32),
                cols=df["item_idx"].to_numpy().astype(np.int32),
                vals=df["weight"].to_numpy().astype(np.float32),
            )
            batch_files.append(batch_path)

    # 3. –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ
    log_message("–û–±—É—á–µ–Ω–∏–µ als_model...")

    als_model = TorchALS(
        len(user_map), len(item_map), n_factors=n_factors, device=device
    )

    for batch_path in tqdm(batch_files):
        try:
            data = np.load(batch_path)
            rows, cols, vals = data["rows"], data["cols"], data["vals"]

            indices_np = np.empty((2, len(rows)), dtype=np.int32)
            indices_np[0] = rows
            indices_np[1] = cols
            indices = torch.tensor(indices_np, dtype=torch.long, device=device)
            values = torch.tensor(vals, dtype=torch.float32, device=device)

            sparse_batch = torch.sparse_coo_tensor(
                indices, values, size=(len(user_map), len(item_map)), device=device
            )

            als_model.partial_fit(sparse_batch, iterations=5, lr=0.005)

            del sparse_batch, indices, values
            if device == "cuda":
                torch.cuda.empty_cache()

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {batch_path}: {e}")
            continue

    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    log_message("–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
    for batch_path in batch_files:
        try:
            os.remove(batch_path)
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {batch_path}: {e}")

    try:
        if not os.listdir(batch_dir):
            os.rmdir(batch_dir)
            log_message("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –±–∞—Ç—á–µ–π —É–¥–∞–ª–µ–Ω–∞")
        else:
            log_message("–í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –æ—Å—Ç–∞–ª–∏—Å—å —Ñ–∞–π–ª—ã, –Ω–µ —É–¥–∞–ª—è–µ–º")
    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {e}")

    log_message("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    return als_model, user_map, item_map


def build_copurchase_map(
    train_orders_df, min_co_items=2, top_n=10, device="cuda", max_items=500
):
    """
    —Å—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ–∫—É–ø–æ–∫ –¥–ª—è —Ç–æ–ø-N —Ç–æ–≤–∞—Ä–æ–≤
    –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –≤ JSON
    """
    log_message("–°—Ç—Ä–æ–∏–º co-purchase –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è —Ç–æ–ø-N —Ç–æ–≤–∞—Ä–æ–≤...")

    # 1. –¢–æ–ø –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
    item_popularity = train_orders_df["item_id"].value_counts()
    top_items = item_popularity.head(max_items).index.tolist()
    popular_items_set = set(top_items)

    log_message(f"–¢–æ–ø-{len(top_items)} –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã")

    # 2. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∫–æ—Ä–∑–∏–Ω
    baskets = []
    for items in train_orders_df.groupby(["user_id", "created_timestamp"])[
        "item_id"
    ].apply(list):
        filtered_items = [item for item in items if item in popular_items_set]
        if len(filtered_items) >= min_co_items:
            baskets.append(filtered_items)

    if not baskets:
        log_message("–ù–µ—Ç –∫–æ—Ä–∑–∏–Ω —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏")
        return {}

    log_message(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(baskets)} –∫–æ—Ä–∑–∏–Ω —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏")

    # 3. –°–ª–æ–≤–∞—Ä–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
    item2idx = {it: i for i, it in enumerate(top_items)}
    idx2item = {i: it for it, i in item2idx.items()}
    n_items = len(top_items)

    log_message(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {n_items}")

    # 4. Sparse –º–∞—Ç—Ä–∏—Ü–∞
    rows, cols, values = [], [], []
    for items in tqdm(baskets, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–∑–∏–Ω"):
        idxs = [item2idx[it] for it in items if it in item2idx]
        if len(idxs) < 2:
            continue

        weight = 1.0 / len(idxs)
        for i in range(len(idxs)):
            for j in range(len(idxs)):
                if i != j:
                    rows.append(idxs[i])
                    cols.append(idxs[j])
                    values.append(weight)

    if not rows:
        log_message("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã")
        return {}

    log_message(f"–°–æ–∑–¥–∞–µ–º sparse –º–∞—Ç—Ä–∏—Ü—É –∏–∑ {len(rows)} –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π...")

    rows_tensor = torch.tensor(rows, dtype=torch.long, device=device)
    cols_tensor = torch.tensor(cols, dtype=torch.long, device=device)
    values_tensor = torch.tensor(values, dtype=torch.float32, device=device)

    co_matrix = torch.sparse_coo_tensor(
        torch.stack([rows_tensor, cols_tensor]),
        values_tensor,
        size=(n_items, n_items),
        device=device,
    ).coalesce()

    log_message(
        f"Sparse –º–∞—Ç—Ä–∏—Ü–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞: {co_matrix.shape}, –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {co_matrix._nnz()}"
    )

    # 6. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    row_sums = torch.sparse.sum(co_matrix, dim=1).to_dense().clamp(min=1e-9)

    # 7. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
    final_copurchase = {}
    indices = co_matrix.indices()
    values = co_matrix.values()

    log_message("–§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏...")
    for i in tqdm(range(n_items), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤"):
        mask = indices[0] == i
        if mask.any():
            col_indices = indices[1][mask]
            row_values = values[mask] / row_sums[i]

            if len(row_values) > 0:
                topk_vals, topk_idx = torch.topk(
                    row_values, k=min(top_n, len(row_values))
                )
                final_copurchase[idx2item[i]] = [
                    (idx2item[col_indices[j].item()], topk_vals[j].item())
                    for j in range(len(topk_vals))
                    if topk_vals[j].item() > 0
                ]

    log_message(f"Co-purchase —Å–ª–æ–≤–∞—Ä—å –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {len(final_copurchase)} —Ç–æ–≤–∞—Ä–æ–≤")

    avg_recommendations = sum(len(v) for v in final_copurchase.values()) / max(
        1, len(final_copurchase)
    )
    log_message(f"–í —Å—Ä–µ–¥–Ω–µ–º {avg_recommendations:.1f} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ —Ç–æ–≤–∞—Ä")

    return final_copurchase


def build_category_maps(items_df, categories_df):
    """
    –£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: —Å—Ç—Ä–æ–∏–º –º–∞–ø–ø–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª—ã.
    """
    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤...")

    # –¢–æ–≤–∞—Ä -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    item_to_cat = dict(zip(items_df["item_id"], items_df["catalogid"]))

    # –ö–∞—Ç–µ–≥–æ—Ä–∏—è -> —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤
    cat_to_items = (
        items_df.groupby("catalogid")["item_id"].apply(lambda x: x.to_numpy()).to_dict()
    )

    # –ò–µ—Ä–∞—Ä—Ö–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    cat_tree = dict(zip(categories_df["catalogid"], categories_df["ids"]))

    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—é (–≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ)
    extended_cat_to_items = {}
    for cat_id, items_list in cat_to_items.items():
        all_items = set(items_list)
        parents = cat_tree.get(cat_id, [])
        for parent in parents:
            if parent in cat_to_items:
                all_items.update(cat_to_items[parent])
        extended_cat_to_items[cat_id] = np.array(list(all_items))
    return item_to_cat, extended_cat_to_items


# -------------------- –ú–µ—Ç—Ä–∏–∫–∏ --------------------
def ndcg_at_k(recommended, ground_truth, k=100, device="cuda"):
    """
    NDCG@K: —Å—á–∏—Ç–∞–µ–º —á–µ—Ä–µ–∑ torch –Ω–∞ GPU
    recommended: —Å–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö item_id
    ground_truth: –º–Ω–æ–∂–µ—Å—Ç–≤–æ/—Å–ø–∏—Å–æ–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö item_id
    """
    if not ground_truth:
        return 0.0

    # –ë–µ—Ä—ë–º —Ç–æ–ø-k —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    rec_k = torch.tensor(recommended[:k], device=device)

    # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
    gt_set = set(ground_truth)
    gt_mask = torch.tensor(
        [1 if x.item() in gt_set else 0 for x in rec_k],
        dtype=torch.float32,
        device=device,
    )

    # –ü–æ–∑–∏—Ü–∏–∏ (1..k)
    positions = torch.arange(1, len(rec_k) + 1, device=device, dtype=torch.float32)

    # DCG: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å / log2(–ø–æ–∑–∏—Ü–∏—è+1)
    dcg = torch.sum(gt_mask / torch.log2(positions + 1))

    # IDCG: –∏–¥–µ–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
    ideal_len = min(len(ground_truth), k)
    idcg = torch.sum(
        1.0
        / torch.log2(
            torch.arange(1, ideal_len + 1, device=device, dtype=torch.float32) + 1
        )
    )

    return (dcg / idcg).item() if idcg > 0 else 0.0


def build_recent_items_map_from_batches(
    batch_dir,
    recent_n=5,
    save_path="/home/root6/python/e_cup/rec_system/data/processed/recent_items_map.pkl",
):
    """–í–µ—Ä—Å–∏—è –≥–¥–µ weight –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ items.
    save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ None ‚Äî –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º).
    """
    batch_files = sorted(Path(batch_dir).glob("*.parquet"))
    recent_items_map = {}

    for f in tqdm(batch_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π"):
        try:
            df = pl.read_parquet(
                f, columns=["user_id", "item_id", "timestamp", "weight"]
            )

            df = df.with_columns(
                [
                    pl.col("user_id").cast(pl.Int64),
                    pl.col("item_id").cast(pl.Int64),
                    pl.col("timestamp").dt.epoch("s").alias("ts_epoch"),
                    pl.col("weight").cast(pl.Float64),
                ]
            )

            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
            df = df.with_columns(
                (
                    pl.col("weight") * 0.8
                    + pl.col("ts_epoch") / pl.col("ts_epoch").max() * 0.2
                ).alias("score")
            )

            df_sorted = df.sort(["user_id", "score"], descending=[False, True])

            grouped = df_sorted.group_by("user_id").agg(
                pl.col("item_id").head(recent_n).alias("items")
            )

            for row in grouped.iter_rows():
                user_id, items = row[0], row[1]
                if user_id not in recent_items_map:
                    recent_items_map[user_id] = items
                else:
                    combined = (recent_items_map[user_id] + items)[:recent_n]
                    recent_items_map[user_id] = combined

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {f}: {e}")
            continue

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    if save_path is not None:
        try:
            with open(save_path, "wb") as f:
                pickle.dump(recent_items_map, f)
            log_message(f"–°–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {save_path}: {e}")

    return recent_items_map


# -------------------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ --------------------
def save_model(model, user_map, item_map, path="src/models/model_als.pkl"):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–∞–ø–ø–∏–Ω–≥–∏ –≤ pickle.
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)

    data = {
        "model": model,
        "user_map": user_map,
        "item_map": item_map,
    }

    with open(path, "wb") as f:
        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)

    log_message(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")


# -------------------- –ú–µ—Ç—Ä–∏–∫–∏ --------------------
def ndcg_at_k_grouped(predictions, targets, groups, k=100, device="cpu"):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ NDCG@k –¥–ª—è —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch.

    predictions: 1D –º–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö score (list, np.array –∏–ª–∏ torch.tensor)
    targets:     1D –º–∞—Å—Å–∏–≤ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (0/1) —Ç–æ–π –∂–µ –¥–ª–∏–Ω—ã
    groups:      —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –≥—Ä—É–ø–ø (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∫–æ–ª—å–∫–æ –∞–π—Ç–µ–º–æ–≤ —É –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
    k:           —Ç–æ–ø-K –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏
    device:      "cpu" –∏–ª–∏ "cuda"
    """
    preds = torch.as_tensor(predictions, dtype=torch.float32, device=device)
    targs = torch.as_tensor(targets, dtype=torch.float32, device=device)

    ndcg_scores = []
    start_idx = 0

    for group_size in groups:
        if group_size == 0:
            continue

        end_idx = start_idx + group_size
        group_preds = preds[start_idx:end_idx]
        group_targs = targs[start_idx:end_idx]

        # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        sorted_idx = torch.argsort(group_preds, descending=True)
        sorted_targs = group_targs[sorted_idx]

        # DCG
        denom = torch.log2(
            torch.arange(2, 2 + min(k, group_size), device=device, dtype=torch.float32)
        )
        dcg = (sorted_targs[:k] / denom).sum()

        # IDCG
        ideal_sorted = torch.sort(group_targs, descending=True).values
        idcg = (ideal_sorted[:k] / denom).sum()

        ndcg = (dcg / idcg) if idcg > 0 else torch.tensor(0.0, device=device)
        ndcg_scores.append(ndcg)

        start_idx = end_idx

    if not ndcg_scores:
        return 0.0

    return float(torch.stack(ndcg_scores).mean().cpu())


class TorchALS(nn.Module):
    def __init__(
        self,
        n_users,
        n_items,
        n_factors=64,
        reg=1e-3,
        dtype=torch.float32,
        device="cuda",
    ):
        super().__init__()
        self.user_factors = nn.Parameter(
            nn.init.xavier_normal_(
                torch.empty(n_users, n_factors, dtype=dtype, device=device)
            )
        )
        self.item_factors = nn.Parameter(
            nn.init.xavier_normal_(
                torch.empty(n_items, n_factors, dtype=dtype, device=device)
            )
        )
        self.reg = reg
        self.device = device
        self.partial_optimizer = None  # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è partial_fit
        self.to(device)

    def forward(self, user, item):
        return (self.user_factors[user] * self.item_factors[item]).sum(1)

    def partial_fit(self, sparse_batch, iterations=5, lr=0.005, show_progress=False):
        """
        –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞
        """
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º sparse tensor
        if not sparse_batch.is_coalesced():
            sparse_batch = sparse_batch.coalesce()

        users_coo, items_coo = sparse_batch.indices()
        values = sparse_batch.values()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
        if self.partial_optimizer is None:
            self.partial_optimizer = torch.optim.Adam(
                self.parameters(), lr=lr, weight_decay=self.reg
            )
        else:
            # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
            for param_group in self.partial_optimizer.param_groups:
                param_group["lr"] = lr

        # –û–±—É—á–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞ (–±–µ–∑ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è!)
        for epoch in range(iterations):
            self.partial_optimizer.zero_grad()

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –≤ –±–∞—Ç—á–µ
            pred = self.forward(users_coo, items_coo)
            loss = F.mse_loss(pred, values)

            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            user_reg = self.reg * self.user_factors[users_coo].pow(2).mean()
            item_reg = self.reg * self.item_factors[items_coo].pow(2).mean()
            total_loss = loss + user_reg + item_reg

            total_loss.backward()
            self.partial_optimizer.step()

            if show_progress and (epoch % 10 == 0 or epoch == iterations - 1):
                log_message(f"Partial fit epoch {epoch}, Loss: {total_loss.item():.6f}")


class LightGBMRecommender:
    def __init__(self):
        self.model = None
        self.feature_columns = []
        self.user_embeddings = None
        self.item_embeddings = None
        self.external_embeddings_dict = None
        self.copurchase_map = None
        self.item_to_cat = None
        self.cat_to_items = None
        self.user_map = None
        self.item_map = None
        self.covisitation_matrix = None

    def load_training_data_from_parquet(self, train_dir, val_dir):
        """–ó–∞–≥—Ä—É–∑–∫–∞ train/val –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–∞—Ä–∫–µ—Ç-—Ñ–∞–π–ª–æ–≤"""
        train_files = sorted(Path(train_dir).glob("*.parquet"))
        val_files = sorted(Path(val_dir).glob("*.parquet"))

        train_dfs = [pd.read_parquet(f) for f in train_files]
        val_dfs = [pd.read_parquet(f) for f in val_files]

        train_data = pd.concat(train_dfs, ignore_index=True)
        val_data = pd.concat(val_dfs, ignore_index=True)

        return train_data, val_data

    def set_als_embeddings(self, als_model):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º ALS —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"""
        self.user_embeddings = als_model.user_factors
        self.item_embeddings = als_model.item_factors

    def set_external_embeddings(self, embeddings_dict):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤"""
        self.external_embeddings_dict = embeddings_dict

    def set_additional_data(
        self, copurchase_map, item_to_cat, cat_to_items, user_map, item_map
    ):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        self.copurchase_map = copurchase_map
        self.item_to_cat = item_to_cat
        self.cat_to_items = cat_to_items
        self.user_map = user_map
        self.item_map = item_map

    def _train_covisitation_matrix(
        self, train_data: pd.DataFrame, min_cooccurrence: int = 5
    ):
        """–û–±—É—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        log_message("–û–±—É—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏–∏ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö...")

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å: user_id -> —Å–ø–∏—Å–æ–∫ item_id —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞–ª
        user_items = {}
        for user_id, group in train_data.groupby("user_id"):
            user_items[user_id] = set(group["item_id"].unique())

        # –°—á–∏—Ç–∞–µ–º –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏—é (—Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –ø–æ—è–≤–ª–µ–Ω–∏—è)
        cooccurrence = defaultdict(int)

        for user_id, items in user_items.items():
            items_list = list(items)
            for i in range(len(items_list)):
                for j in range(i + 1, len(items_list)):
                    item1, item2 = items_list[i], items_list[j]
                    # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –ø–∞—Ä—É –¥–ª—è consistency
                    pair = (min(item1, item2), max(item1, item2))
                    cooccurrence[pair] += 1

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏–π
        self.covisitation_matrix = {}
        for (item1, item2), count in cooccurrence.items():
            if count >= min_cooccurrence:
                self.covisitation_matrix[item1] = (
                    self.covisitation_matrix.get(item1, 0) + count
                )
                self.covisitation_matrix[item2] = (
                    self.covisitation_matrix.get(item2, 0) + count
                )

        log_message(
            f"–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏–∏: {len(self.covisitation_matrix)} —Ç–æ–≤–∞—Ä–æ–≤"
        )

    def _load_ui_features_for_pairs(self, pairs_df, ui_features_path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç UI-–ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞—Ä user-item
        """
        try:
            if pairs_df.empty:
                return None

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–º–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            temp_pairs_path = "/tmp/filter_pairs.parquet"
            pairs_df[["user_id", "item_id"]].to_parquet(temp_pairs_path, index=False)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Polars –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            result = (
                pl.scan_parquet(ui_features_path)
                .join(
                    pl.scan_parquet(temp_pairs_path),
                    on=["user_id", "item_id"],
                    how="inner",
                )
                .collect()
                .to_pandas()
            )

            # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if os.path.exists(temp_pairs_path):
                os.remove(temp_pairs_path)

            return result

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–∞—Ä: {e}")
            return None

    def _add_rich_features(
        self, data: pd.DataFrame, train_only_data: pd.DataFrame = None
    ) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏—á —Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ–º —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""
        log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–æ–≥–∞—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
        if "timestamp" in data.columns:
            try:
                data["is_weekend"] = (data["timestamp"].dt.dayofweek >= 5).astype(int)
                data["hour"] = data["timestamp"].dt.hour.fillna(0)
            except Exception as e:
                log_message(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å timestamp: {e}")
                data["is_weekend"] = 0
                data["hour"] = 0
        else:
            data["is_weekend"] = 0
            data["hour"] = 0

        # –ï—Å–ª–∏ train_only_data –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –±–µ—Ä—ë–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ data
        if train_only_data is None or train_only_data.empty:
            train_only_data = data[data.get("target", 0) == 0]

        # --- –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–∞ ---
        if not train_only_data.empty:
            item_pop = (
                train_only_data.groupby("item_id")["user_id"]
                .count()
                .rename("item_popularity")
                .reset_index()
            )
            if not item_pop.empty:
                data = data.merge(item_pop, on="item_id", how="left")

        data["item_popularity"] = data.get("item_popularity", 0).fillna(0)

        # --- –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ---
        if "category_id" in data.columns:
            if not train_only_data.empty and "category_id" in train_only_data.columns:
                cat_pop = (
                    train_only_data.groupby("category_id")["user_id"]
                    .count()
                    .rename("category_popularity")
                    .reset_index()
                )
                if not cat_pop.empty:
                    data = data.merge(cat_pop, on="category_id", how="left")
            data["category_popularity"] = data.get("category_popularity", 0).fillna(0)
        else:
            data["category_popularity"] = 0

        # --- –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
        if not train_only_data.empty:
            user_activity = (
                train_only_data.groupby("user_id")["item_id"]
                .count()
                .rename("user_activity")
                .reset_index()
            )
            if not user_activity.empty:
                data = data.merge(user_activity, on="user_id", how="left")
        data["user_activity"] = data.get("user_activity", 0).fillna(0)

        # --- –°—Ä–µ–¥–Ω—è—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–æ–≤ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
        if not train_only_data.empty and "item_popularity" in data.columns:
            train_with_pop = (
                train_only_data.merge(item_pop, on="item_id", how="left")
                if not item_pop.empty
                else train_only_data.copy()
            )
            if "item_popularity" in train_with_pop.columns:
                train_with_pop["item_popularity"] = train_with_pop[
                    "item_popularity"
                ].fillna(0)
                user_avg_pop = (
                    train_with_pop.groupby("user_id")["item_popularity"]
                    .mean()
                    .rename("user_avg_item_popularity")
                    .reset_index()
                )
                if not user_avg_pop.empty:
                    data = data.merge(user_avg_pop, on="user_id", how="left")
        data["user_avg_item_popularity"] = data.get(
            "user_avg_item_popularity", 0
        ).fillna(0)

        # --- –ö–æ–≤–∏–∑–∏—Ç–∞—Ü–∏—è ---
        if (
            hasattr(self, "covisitation_matrix")
            and self.covisitation_matrix is not None
        ):
            data["covisitation_score"] = (
                data["item_id"].map(self.covisitation_matrix.get).fillna(0)
            )
        else:
            data["covisitation_score"] = 0

        # --- FCLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ---
        if getattr(self, "external_embeddings_dict", None):
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            all_item_ids = list(self.external_embeddings_dict.keys())
            embedding_dim = len(next(iter(self.external_embeddings_dict.values())))
            n_fclip_dims = min(EMD_LENGHT, embedding_dim)
            embeddings_tensor = torch.tensor(
                [self.external_embeddings_dict[i] for i in all_item_ids],
                dtype=torch.float32,
                device=device,
            )
            item_id_to_idx = {item_id: idx for idx, item_id in enumerate(all_item_ids)}
            batch_size = 100_000
            total_rows = len(data)
            for i in range(n_fclip_dims):
                data[f"fclip_embed_{i}"] = 0.0
                for start_idx in range(0, total_rows, batch_size):
                    end_idx = min(start_idx + batch_size, total_rows)
                    batch_item_ids = data.iloc[start_idx:end_idx]["item_id"].values
                    valid_mask = np.array(
                        [item in item_id_to_idx for item in batch_item_ids]
                    )
                    valid_indices = np.where(valid_mask)[0]
                    valid_item_ids = batch_item_ids[valid_mask]
                    if len(valid_item_ids):
                        tensor_indices = torch.tensor(
                            [item_id_to_idx[item] for item in valid_item_ids],
                            device=device,
                        )
                        batch_emb = embeddings_tensor[tensor_indices, i].cpu().numpy()
                        data.iloc[
                            start_idx + valid_indices,
                            data.columns.get_loc(f"fclip_embed_{i}"),
                        ] = batch_emb
            del embeddings_tensor, item_id_to_idx
            torch.cuda.empty_cache()

        # --- –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ---
        new_features = [
            "is_weekend",
            "hour",
            "item_popularity",
            "category_popularity",
            "user_activity",
            "user_avg_item_popularity",
            "covisitation_score",
        ]
        if getattr(self, "external_embeddings_dict", None):
            new_features += [f"fclip_embed_{i}" for i in range(n_fclip_dims)]

        existing_features = set(getattr(self, "feature_columns", []))
        for feature in new_features:
            if feature in data.columns and feature not in existing_features:
                self.feature_columns.append(feature)
                existing_features.add(feature)

        log_message(f"–î–æ–±–∞–≤–ª–µ–Ω—ã —Ñ–∏—á–∏: {[f for f in new_features if f in data.columns]}")
        log_message(f"–í—Å–µ–≥–æ —Ñ–∏—á –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {len(self.feature_columns)}")

        return data

    def prepare_training_data(
        self,
        interactions_files,
        orders_ddf,
        user_map,
        item_map,
        popularity_s,
        recent_items_map=None,
        copurchase_map=None,
        item_to_cat=None,
        cat_to_items=None,
        user_features_dict=None,
        item_features_dict=None,
        embeddings_dict=None,
        sample_fraction=0.1,
        negatives_per_positive=0,
        split: float = 0.2,
    ):
        log_message(
            "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LightGBM (streaming, polars lazy, –≤–µ–∫—Ç–æ—Ä–Ω–æ)..."
        )

        base_dir = Path("/home/root6/python/e_cup/rec_system/data/processed/")
        interactions_out_dir = base_dir / "interactions_streaming"
        train_out_dir = base_dir / "train_streaming"
        val_out_dir = base_dir / "val_streaming"
        for p in [interactions_out_dir, train_out_dir, val_out_dir]:
            p.mkdir(parents=True, exist_ok=True)

        # --- 1. –û–±—ä–µ–¥–∏–Ω—è–µ–º interactions –ª–µ–Ω–∏–≤–æ ---
        interactions_lazy = pl.concat(
            [
                pl.scan_parquet(str(f)).select(
                    ["user_id", "item_id", "timestamp", "weight"]
                )
                for f in interactions_files
            ]
        )
        interactions_all_path = interactions_out_dir / "interactions_all.parquet"
        interactions_lazy.sink_parquet(str(interactions_all_path))

        # --- 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ orders (sample + lazy) ---
        if sample_fraction < 1.0:
            sampled_orders = orders_ddf.sample(frac=sample_fraction, random_state=42)
        else:
            sampled_orders = orders_ddf

        orders_tmp_dir = base_dir / "orders_sample_tmp"
        orders_tmp_dir.mkdir(exist_ok=True)
        sampled_orders.to_parquet(str(orders_tmp_dir), write_index=False)
        orders_pl = pl.scan_parquet(str(orders_tmp_dir / "*.parquet"))

        # --- 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º split_time –Ω–∞ –æ—Å–Ω–æ–≤–µ ORDERS ---
        orders_ts_bounds = orders_pl.select(
            [
                pl.col("created_timestamp").min().alias("min_ts"),
                pl.col("created_timestamp").max().alias("max_ts"),
            ]
        ).collect()

        min_ts, max_ts = orders_ts_bounds[0, "min_ts"], orders_ts_bounds[0, "max_ts"]
        duration = max_ts - min_ts
        split_by_time = duration > timedelta(0)
        split_time = (
            max_ts - timedelta(seconds=duration.total_seconds() * split)
            if split_by_time
            else max_ts
        )
        train_timestamp_fill = split_time - timedelta(seconds=1)

        # --- 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π ---
        if user_features_dict:
            user_feats_lazy = pl.LazyFrame(
                [{"user_id": k, **v} for k, v in user_features_dict.items()]
            )
            user_feat_cols = [
                c for c in user_feats_lazy.collect_schema().names() if c != "user_id"
            ]
        else:
            user_feats_lazy = None
            user_feat_cols = []

        if item_features_dict:
            items_data = []
            for item_id, feats in item_features_dict.items():
                if isinstance(feats, dict):
                    feats_copy = feats.copy()
                    feats_copy["item_id"] = item_id
                    items_data.append(feats_copy)
                elif isinstance(feats, np.ndarray):
                    emb = {
                        f"emb_{i}": float(feats[i])
                        for i in range(min(EMD_LENGHT, feats.shape[0]))
                    }
                    emb["item_id"] = item_id
                    items_data.append(emb)
            item_feats_lazy = pl.LazyFrame(items_data)
            item_feat_cols = [
                c for c in item_feats_lazy.collect_schema().names() if c != "item_id"
            ]
        else:
            item_feats_lazy = None
            item_feat_cols = []

        self.feature_columns = ["copurchase_count"] + user_feat_cols + item_feat_cols

        # --- 5. –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ä–¥–∂ ---
        interactions_lazy = pl.scan_parquet(str(interactions_all_path))

        merged = orders_pl.join(
            interactions_lazy, on=["user_id", "item_id"], how="left"
        ).with_columns(
            [
                pl.col("timestamp").fill_null(train_timestamp_fill),
                pl.col("weight").fill_null(0.0),
                pl.when(pl.col("target") == 1).then(1).otherwise(0).alias("target"),
            ]
        )

        if user_feats_lazy is not None:
            merged = merged.join(user_feats_lazy, on="user_id", how="left")

        if item_feats_lazy is not None:
            merged = merged.join(item_feats_lazy, on="item_id", how="left")

        # --- 6. Copurchase count ---
        if copurchase_map:
            copurchase_data = []
            for k, v in copurchase_map.items():
                count = sum(c[1] if isinstance(c, tuple) else c for c in v)
                copurchase_data.append({"item_id": k, "copurchase_count": float(count)})

            copurchase_lazy = pl.LazyFrame(copurchase_data)
            merged = merged.join(copurchase_lazy, on="item_id", how="left")
            merged = merged.with_columns(pl.col("copurchase_count").fill_null(0.0))
        else:
            merged = merged.with_columns(pl.lit(0.0).alias("copurchase_count"))

        # --- 7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–æ–≤ ---
        if negatives_per_positive > 0:
            log_message("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–æ–≤ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ...")

            all_item_ids = list(item_map.keys())

            def generate_negatives_for_user(user_data):
                user_pos_items = user_data.filter(pl.col("target") == 1)[
                    "item_id"
                ].to_list()
                user_all_items = user_data["item_id"].to_list()
                if not user_pos_items:
                    return None
                n_neg = len(user_pos_items) * negatives_per_positive
                available_items = [i for i in all_item_ids if i not in user_all_items]
                if not available_items:
                    return None
                sampled = np.random.choice(
                    available_items, min(n_neg, len(available_items)), replace=False
                )
                neg_rows = []
                for item_id in sampled:
                    neg_rows.append(
                        {
                            "user_id": user_data["user_id"][0],
                            "item_id": item_id,
                            "timestamp": train_timestamp_fill,
                            "target": 0,
                            "weight": 0.0,
                            "copurchase_count": 0.0,
                        }
                    )
                return pl.DataFrame(neg_rows)

            negatives_list = []
            user_chunks = (
                merged.select("user_id")
                .unique()
                .collect()
                .partition_by(1000, "user_id")
            )
            for i, user_chunk in enumerate(user_chunks):
                user_ids = user_chunk["user_id"].to_list()
                user_data_chunk = merged.filter(
                    pl.col("user_id").is_in(user_ids)
                ).collect()
                for user_id in user_ids:
                    user_data = user_data_chunk.filter(pl.col("user_id") == user_id)
                    neg_df = generate_negatives_for_user(user_data)
                    if neg_df is not None:
                        negatives_list.append(neg_df)
                if i % 10 == 0:
                    log_message(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i * 1000} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –Ω–µ–≥–∞—Ç–∏–≤–æ–≤")

            if negatives_list:
                negatives_df = pl.concat(negatives_list)
                for col in merged.columns:
                    if col not in negatives_df.columns:
                        negatives_df = negatives_df.with_columns(
                            pl.lit(None).alias(col)
                        )
                negatives_lazy = negatives_df.lazy()
                merged = pl.concat([merged, negatives_lazy])

        # --- 8. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val ---
        if split_by_time:
            train_lazy = merged.filter(pl.col("timestamp") <= split_time)
            val_lazy = merged.filter(pl.col("timestamp") > split_time)
        else:
            train_lazy = merged.filter(
                pl.col("user_id").hash() % 100 < int(100 * (1 - split))
            )
            val_lazy = merged.filter(
                pl.col("user_id").hash() % 100 >= int(100 * (1 - split))
            )

        train_lazy = train_lazy.drop("timestamp")
        val_lazy = val_lazy.drop("timestamp")

        train_lazy.sink_parquet(
            str(train_out_dir / "train.parquet"), row_group_size=100000
        )
        val_lazy.sink_parquet(str(val_out_dir / "val.parquet"), row_group_size=100000)

        if orders_tmp_dir.exists():
            import shutil

            shutil.rmtree(orders_tmp_dir)

        log_message("‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –±–µ–∑ —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏")
        return train_out_dir, val_out_dir

    def _get_copurchase_strength(self, item_id):
        """–ü–æ–ª—É—á–∞–µ–º —Å–∏–ª—É co-purchase —Å–≤—è–∑–∏"""
        if not self.copurchase_map or item_id not in self.copurchase_map:
            return 0.0

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∏–ª–∞ —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º
        strengths = [strength for _, strength in self.copurchase_map[item_id]]
        return max(strengths) if strengths else 0.0

    def _get_user_copurchase_affinity(self, user_id, item_id):
        """Affinity –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫ co-purchase —Å–≤—è–∑—è–º"""
        if not self.copurchase_map or not hasattr(self, "user_items_history"):
            return 0.0

        # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Ä–∞—Å—á–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—â—É—é —Å–∏–ª—É
        return self._get_copurchase_strength(item_id)

    def train(self, train_data, val_data=None, params=None):
        """
        –û–±—É—á–µ–Ω–∏–µ LightGBM —Å –±–∏–Ω–∞—Ä–Ω–æ–π —Ü–µ–ª—å—é, —Å –æ—Ü–µ–Ω–∫–æ–π NDCG@100 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
        –ö–∞—Å—Ç–æ–º–Ω—ã–π callback: —Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ NDCG@100.
        """
        if params is None:
            params = {
                "objective": "binary",
                "metric": "None",
                "learning_rate": 0.05,
                "num_leaves": 31,
                "max_depth": 6,
                "min_data_in_leaf": 50,
                "feature_fraction": 0.8,
                "bagging_fraction": 0.8,
                "bagging_freq": 10,
                "verbosity": -1,
                "force_row_wise": True,
                "num_threads": 4,
                "max_bin": 255,
                "boosting": "gbdt",
                "bin_construct_sample_cnt": 100000,
                "device": "cpu",
                "seed": 42,
            }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if not hasattr(self, "feature_columns") or not self.feature_columns:
            log_message("‚ùå –û–®–ò–ë–ö–ê: –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
            return None

        missing_features = [
            f for f in self.feature_columns if f not in train_data.columns
        ]
        if missing_features:
            log_message(f"‚ùå –í train_data –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
            return None

        X_train = train_data[self.feature_columns]
        y_train = train_data["target"]

        for df in [train_data, val_data]:
            if df is not None:
                for col in df.select_dtypes(include=["float64"]).columns:
                    df[col] = df[col].astype("float32")
                for col in df.select_dtypes(include=["int64"]).columns:
                    df[col] = df[col].astype("int32")

        train_users = train_data["user_id"]

        log_message(f"–†–∞–∑–º–µ—Ä train: {len(X_train)}")
        log_message(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ train: {train_users.nunique()}")

        categorical_features = [
            col
            for col in X_train.columns
            if col in ["user_id", "item_id"] or col.startswith(("user_", "item_"))
        ]

        train_dataset = lgb.Dataset(
            X_train,
            label=y_train,
            categorical_feature=categorical_features,
            feature_name=list(X_train.columns),
        )

        X_val, y_val, val_users, val_dataset = None, None, None, None
        if val_data is not None:
            missing_val_features = [
                f for f in self.feature_columns if f not in val_data.columns
            ]
            if missing_val_features:
                log_message(
                    f"‚ùå –í val_data –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_val_features}"
                )
                return None

            X_val = val_data[self.feature_columns]
            y_val = val_data["target"]
            val_users = val_data["user_id"]

            log_message(f"–†–∞–∑–º–µ—Ä val: {len(X_val)}")
            log_message(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ val: {val_users.nunique()}")

            val_dataset = lgb.Dataset(
                X_val,
                label=y_val,
                categorical_feature=categorical_features,
                feature_name=list(X_val.columns),
            )

        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        best_ndcg = -float("inf")
        best_iteration = 0
        best_eval_result_list = None
        no_improvement_count = 0
        early_stopping_rounds = EARLY_STOP
        eval_history = {"train": [], "valid": []}

        # –ö–∞—Å—Ç–æ–º–Ω—ã–π callback
        def ndcg_callback(env):
            nonlocal best_ndcg, best_iteration, best_eval_result_list, no_improvement_count

            iteration = env.iteration

            # –í—ã—á–∏—Å–ª—è–µ–º NDCG —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 5 –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏
            if iteration % 5 != 0 and iteration > 10:
                return

            # –ö—ç—à–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            if not hasattr(env, "last_preds") or env.last_preds_iter != iteration:
                train_preds = env.model.predict(X_train, num_iteration=iteration)
                env.last_preds = train_preds
                env.last_preds_iter = iteration
            else:
                train_preds = env.last_preds

            train_ndcg = self._calculate_ndcg(train_data, train_preds, train_users)
            eval_history["train"].append((iteration, train_ndcg))

            eval_list = [("train", "ndcg@100", float(train_ndcg), True)]

            if val_data is not None:
                val_preds = env.model.predict(X_val, num_iteration=iteration)
                valid_ndcg = self._calculate_ndcg(val_data, val_preds, val_users)
                eval_history["valid"].append((iteration, valid_ndcg))
                eval_list.append(("valid", "ndcg@100", float(valid_ndcg), True))

                # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 5 –∏—Ç–µ—Ä–∞—Ü–∏–π
                if iteration % 5 == 0:
                    log_message(
                        f"[Iter {iteration}] train_ndcg@100: {train_ndcg:.6f}, valid_ndcg@100: {valid_ndcg:.6f}"
                    )

                if valid_ndcg > best_ndcg + 1e-6:
                    best_ndcg = valid_ndcg
                    best_iteration = iteration
                    best_eval_result_list = list(eval_list)
                    no_improvement_count = 0
                    log_message(
                        f"‚úÖ [Iter {iteration}] –ù–æ–≤—ã–π –ª—É—á—à–∏–π valid_ndcg@100: {valid_ndcg:.6f}"
                    )
                else:
                    no_improvement_count += 1

                env.evaluation_result_list = eval_list

                if no_improvement_count >= early_stopping_rounds:
                    log_message(
                        f"‚èπÔ∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}. "
                        f"–õ—É—á—à–∏–π NDCG@100={best_ndcg:.6f} (iter {best_iteration})"
                    )
                    raise lgb.callback.EarlyStopException(
                        best_iteration, best_eval_result_list
                    )
            else:
                if iteration % 5 == 0:
                    log_message(f"[Iter {iteration}] train_ndcg@100: {train_ndcg:.6f}")
                env.evaluation_result_list = eval_list

        try:
            valid_sets = [train_dataset] + (
                [val_dataset] if val_dataset is not None else []
            )
            valid_names = ["train"] + (["valid"] if val_dataset is not None else [])

            self.model = lgb.train(
                params,
                train_dataset,
                num_boost_round=ITER_N,
                valid_sets=valid_sets,
                valid_names=valid_names,
                callbacks=[ndcg_callback],
            )

            self.eval_history = eval_history

            if val_data is not None:
                final_val_ndcg = self.evaluate(val_data)
                log_message(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π NDCG@100 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_val_ndcg:.6f}")
                log_message(
                    f"üìà –õ—É—á—à–∏–π NDCG@100: {best_ndcg:.6f} –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {best_iteration}"
                )

            return self.model

        except Exception as e:
            log_message(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LightGBM: {e}")
            import traceback

            log_message(f"–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
            return None

    def _calculate_ndcg(self, data, predictions, user_ids):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç NDCG@100"""
        if len(data) == 0:
            return 0.0

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        temp_df = pd.DataFrame(
            {
                "user_id": data["user_id"].values,
                "target": data["target"].values,
                "score": predictions,
            }
        )

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø –±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ø–∏–π
        def calculate_group_ndcg(group):
            if len(group) <= 1:
                return 0.0
            # –ë–µ—Ä–µ–º —Ç–æ–ø-100 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            top_k = min(100, len(group))
            sorted_group = group.nlargest(top_k, "score")
            return ndcg_score(
                [sorted_group["target"].values],
                [sorted_group["target"].values],
                k=top_k,
            )

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        if len(temp_df) > 100000:
            from concurrent.futures import ThreadPoolExecutor

            with ThreadPoolExecutor(max_workers=4) as executor:
                results = list(
                    executor.map(
                        calculate_group_ndcg,
                        [group for _, group in temp_df.groupby("user_id")],
                    )
                )
            return np.mean(results)
        else:
            return temp_df.groupby("user_id").apply(calculate_group_ndcg).mean()

    def evaluate(self, data, k=100):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ NDCG@k"""
        if self.model is None or len(data) == 0:
            return 0.0
        data = data.copy()
        data["score"] = self.model.predict(data[self.feature_columns])
        groups = data.groupby("user_id").size().values
        return ndcg_at_k_grouped(
            data["score"].values, data["target"].values, groups, k=k
        )

    def recommend(self, user_items_data, top_k=100):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —Ç–æ–ø-K"""
        data = user_items_data.copy()
        data["score"] = self.model.predict(data[self.feature_columns])

        recommendations = {
            user_id: group.nlargest(top_k, "score")["item_id"].tolist()
            for user_id, group in data.groupby("user_id")
        }
        return recommendations

    def debug_data_info(self, data, name="data"):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö"""
        if data is None or data.empty:
            log_message(f"{name}: –ü—É—Å—Ç–æ")
            return

        log_message(f"=== –î–ï–ë–ê–ì –ò–ù–§–û–†–ú–ê–¶–ò–Ø: {name} ===")
        log_message(f"–†–∞–∑–º–µ—Ä: {len(data)} —Å—Ç—Ä–æ–∫")
        log_message(f"–ö–æ–ª–æ–Ω–∫–∏: {list(data.columns)}")

        if hasattr(self, "feature_columns"):
            missing = [f for f in self.feature_columns if f not in data.columns]
            if missing:
                log_message(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing}")
            else:
                log_message(f"‚úÖ –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        if "target" in data.columns:
            target_counts = data["target"].value_counts()
            log_message(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {dict(target_counts)}")

        # –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–æ–∫
        if len(data) > 0:
            log_message("–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏:")
            for col in data.columns:
                if col in data.columns:
                    log_message(f"  {col}: {data[col].iloc[0]}")


def build_user_features_dict(interactions_files, orders_df, device="cuda"):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Polars
    """
    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    # 1. –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –¢–†–ï–ö–ï–†–£ (–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è)
    user_stats_list = []
    for f in tqdm(interactions_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–µ–∫–µ—Ä–∞"):
        df = pl.read_parquet(f)

        chunk_stats = df.group_by("user_id").agg(
            [
                pl.col("weight").count().alias("count"),
                pl.col("weight").sum().alias("sum"),
                pl.col("weight").max().alias("max"),
                pl.col("weight").min().alias("min"),
            ]
        )
        user_stats_list.append(chunk_stats)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if user_stats_list:
        all_stats = pl.concat(user_stats_list)
        final_stats = all_stats.group_by("user_id").agg(
            [
                pl.col("count").sum().alias("user_count"),
                pl.col("sum").sum().alias("user_sum"),
                pl.col("max").max().alias("user_max"),
                pl.col("min").min().alias("user_min"),
            ]
        )
    else:
        final_stats = pl.DataFrame()

    # 2. –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –ó–ê–ö–ê–ó–ê–ú
    log_message("–ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∑–∞–∫–∞–∑–∞–º...")
    if isinstance(orders_df, pl.DataFrame):
        orders_pl = orders_df
    else:
        orders_pl = pl.from_pandas(
            orders_df.compute() if hasattr(orders_df, "compute") else orders_df
        )

    order_stats = orders_pl.group_by("user_id").agg(
        [
            pl.col("item_id").count().alias("user_orders_count"),
        ]
    )

    # 3. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –î–ê–ù–ù–´–•
    if len(final_stats) > 0 and len(order_stats) > 0:
        user_stats = final_stats.join(order_stats, on="user_id", how="full")
    elif len(final_stats) > 0:
        user_stats = final_stats
    else:
        user_stats = order_stats

    # 4. –í–´–ß–ò–°–õ–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
    current_time = pl.lit(datetime.now())

    user_stats = user_stats.with_columns(
        [
            pl.col("user_count").fill_null(0),
            pl.col("user_sum").fill_null(0),
            pl.col("user_orders_count").fill_null(0),
            (pl.col("user_sum") / pl.col("user_count")).alias("user_mean"),
        ]
    ).fill_nan(0)

    # 5. –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –°–õ–û–í–ê–†–¨
    user_stats_dict = {}
    for row in user_stats.iter_rows(named=True):
        user_stats_dict[row["user_id"]] = {
            "user_count": row["user_count"],
            "user_mean": row["user_mean"],
            "user_sum": row["user_sum"],
            "user_max": row["user_max"],
            "user_min": row["user_min"],
            "user_orders_count": row["user_orders_count"],
        }

    log_message(
        f"–°–ª–æ–≤–∞—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {save_path}. –ó–∞–ø–∏—Å–µ–π: {len(user_stats_dict)}"
    )
    return user_stats_dict


def load_ui_features_for_user_item(user_id, item_id, ui_features_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç UI-–ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–∞—Ä—ã user-item
    """
    if not ui_features_path or not os.path.exists(ui_features_path):
        return None

    query = pl.scan_parquet(ui_features_path).filter(
        (pl.col("user_id") == user_id) & (pl.col("item_id") == item_id)
    )
    result = query.collect()

    if len(result) == 0:
        return None

    return result[0].to_dict()


def build_item_features_dict(
    interactions_files,
    items_df,
    orders_df,
    embeddings_dict,
    device="cuda",
    batch_size=1_000_000,
    temp_dir="/tmp/item_features",
):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –Ω–∞ –¥–∏—Å–∫
    """
    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–≤–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π...")

    os.makedirs(temp_dir, exist_ok=True)

    # 1. –ë–ê–¢–ß–ï–í–ê–Ø –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø–ú
    log_message("–ë–∞—Ç—á–µ–≤–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π...")

    temp_stats_files = []

    for i, f in enumerate(tqdm(interactions_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π")):
        try:
            # –ß–∏—Ç–∞–µ–º –≤–µ—Å—å —Ñ–∞–π–ª (–±–µ–∑ batch_size)
            df = pl.read_parquet(f)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –≤—Ä—É—á–Ω—É—é
            n_rows = len(df)
            for start in range(0, n_rows, batch_size):
                end = min(start + batch_size, n_rows)
                df_batch = df[start:end]

                if df_batch.is_empty():
                    continue

                chunk_stats = df_batch.group_by("item_id").agg(
                    [
                        pl.col("weight").count().alias("item_count"),
                        pl.col("weight").sum().alias("item_sum"),
                        pl.col("weight").max().alias("item_max"),
                        pl.col("weight").min().alias("item_min"),
                    ]
                )

                stats_path = os.path.join(temp_dir, f"stats_{i}_{start}.parquet")
                chunk_stats.write_parquet(stats_path)
                temp_stats_files.append(stats_path)

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {f}: {e}")
            continue

    # 2. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –°–¢–ê–¢–ò–°–¢–ò–ö –° –î–ò–°–ö–ê
    log_message("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫...")

    final_stats = None
    for stats_file in tqdm(temp_stats_files, desc="–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫"):
        try:
            stats_df = pl.read_parquet(stats_file)
            if final_stats is None:
                final_stats = stats_df
            else:
                final_stats = pl.concat([final_stats, stats_df])
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {stats_file}: {e}")
            continue

    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if final_stats is not None and not final_stats.is_empty():
        final_stats = final_stats.group_by("item_id").agg(
            [
                pl.col("item_count").sum().alias("item_count"),
                pl.col("item_sum").sum().alias("item_sum"),
                pl.col("item_max").max().alias("item_max"),
                pl.col("item_min").min().alias("item_min"),
            ]
        )
    else:
        final_stats = pl.DataFrame(
            schema={
                "item_id": pl.Int32(),
                "item_count": pl.Int64(),
                "item_sum": pl.Float64(),
                "item_max": pl.Float64(),
                "item_min": pl.Float64(),
            }
        )

    # 3. –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–ö–ê–ó–û–í –ü–û –ë–ê–¢–ß–ê–ú
    log_message("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–∫–∞–∑–æ–≤...")

    order_stats_list = []
    if hasattr(orders_df, "compute"):
        n_partitions = orders_df.npartitions
        for i in tqdm(range(n_partitions), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ä—Ç–∏—Ü–∏–π –∑–∞–∫–∞–∑–æ–≤"):
            try:
                partition = orders_df.get_partition(i).compute()
                order_stats = (
                    pl.from_pandas(partition)
                    .group_by("item_id")
                    .agg([pl.col("user_id").count().alias("item_orders_count")])
                )
                order_stats_list.append(order_stats)
            except Exception as e:
                log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—Ç–∏—Ü–∏–∏ {i}: {e}")
    else:
        try:
            order_stats = (
                pl.from_pandas(orders_df)
                .group_by("item_id")
                .agg([pl.col("user_id").count().alias("item_orders_count")])
            )
            order_stats_list.append(order_stats)
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–∫–∞–∑–æ–≤: {e}")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–∫–∞–∑–æ–≤
    if order_stats_list:
        order_stats_final = pl.concat(order_stats_list)
        order_stats_final = order_stats_final.group_by("item_id").agg(
            [pl.col("item_orders_count").sum().alias("item_orders_count")]
        )
    else:
        order_stats_final = pl.DataFrame(
            schema={"item_id": pl.Int32(), "item_orders_count": pl.Int64()}
        )

    # 4. –û–ë–†–ê–ë–û–¢–ö–ê ITEMS –ü–û –ë–ê–¢–ß–ê–ú
    log_message("–û–±—Ä–∞–±–æ—Ç–∫–∞ items...")

    items_catalog_list = []
    if hasattr(items_df, "compute"):
        n_partitions = items_df.npartitions
        for i in tqdm(range(n_partitions), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ä—Ç–∏—Ü–∏–π items"):
            try:
                partition = items_df.get_partition(i).compute()
                items_chunk = (
                    pl.from_pandas(partition).select(["item_id", "catalogid"]).unique()
                )
                items_catalog_list.append(items_chunk)
            except Exception as e:
                log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—Ç–∏—Ü–∏–∏ items {i}: {e}")
    else:
        try:
            items_chunk = (
                pl.from_pandas(items_df).select(["item_id", "catalogid"]).unique()
            )
            items_catalog_list.append(items_chunk)
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ items: {e}")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º items
    if items_catalog_list:
        items_catalog = pl.concat(items_catalog_list).unique()
    else:
        items_catalog = pl.DataFrame(
            schema={"item_id": pl.Int32(), "catalogid": pl.Int32()}
        )

    # 5. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í–°–ï–• –î–ê–ù–ù–´–• –° –ü–†–û–í–ï–†–ö–û–ô –ö–û–õ–û–ù–û–ö
    log_message("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")

    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π DataFrame —Å item_id
    all_item_ids = set()

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ item_id –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    if not final_stats.is_empty():
        all_item_ids.update(final_stats["item_id"].to_list())
    if not order_stats_final.is_empty():
        all_item_ids.update(order_stats_final["item_id"].to_list())
    if not items_catalog.is_empty():
        all_item_ids.update(items_catalog["item_id"].to_list())

    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π DataFrame —Å–æ –≤—Å–µ–º–∏ item_id
    base_df = pl.DataFrame({"item_id": list(all_item_ids)})

    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–æ–∫
    if not final_stats.is_empty():
        base_df = base_df.join(final_stats, on="item_id", how="left")
    else:
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ final_stats –ø—É—Å—Ç–æ–π
        base_df = base_df.with_columns(
            [
                pl.lit(0).alias("item_count"),
                pl.lit(0.0).alias("item_sum"),
                pl.lit(0.0).alias("item_max"),
                pl.lit(0.0).alias("item_min"),
            ]
        )

    if not order_stats_final.is_empty():
        base_df = base_df.join(order_stats_final, on="item_id", how="left")
    else:
        base_df = base_df.with_columns([pl.lit(0).alias("item_orders_count")])

    if not items_catalog.is_empty():
        base_df = base_df.join(items_catalog, on="item_id", how="left")
    else:
        base_df = base_df.with_columns(
            [pl.lit(None).cast(pl.Int32()).alias("catalogid")]
        )

    # 6. –í–´–ß–ò–°–õ–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ò –°–û–ó–î–ê–ù–ò–ï –°–õ–û–í–ê–†–Ø
    log_message("–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è...")

    item_stats_dict = {}

    if not base_df.is_empty():
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        base_df = base_df.with_columns(
            [
                pl.col("item_count").fill_null(0),
                pl.col("item_sum").fill_null(0),
                pl.col("item_orders_count").fill_null(0),
                pl.col("item_max").fill_null(0),
                pl.col("item_min").fill_null(0),
            ]
        )

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        base_df = base_df.with_columns(
            [(pl.col("item_sum") / pl.col("item_count")).fill_nan(0).alias("item_mean")]
        )

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å –ø–æ—Ä—Ü–∏—è–º–∏
        for i in range(0, len(base_df), batch_size):
            batch = base_df[i : i + batch_size]
            for row in batch.iter_rows(named=True):
                item_stats_dict[row["item_id"]] = {
                    "item_count": row.get("item_count", 0),
                    "item_mean": row.get("item_mean", 0),
                    "item_sum": row.get("item_sum", 0),
                    "item_max": row.get("item_max", 0),
                    "item_min": row.get("item_min", 0),
                    "item_orders_count": row.get("item_orders_count", 0),
                    "item_category": row.get("catalogid", None),
                }

    # 7. –î–û–ë–ê–í–õ–ï–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –ü–û–†–¶–ò–Ø–ú–ò
    log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

    embedding_items = list(embeddings_dict.keys())
    for i in tqdm(
        range(0, len(embedding_items), batch_size), desc="–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
    ):
        batch_items = embedding_items[i : i + batch_size]
        for item_id in batch_items:
            if item_id in item_stats_dict:
                embedding = embeddings_dict[item_id]
                for j in range(min(EMD_LENGHT, len(embedding))):
                    item_stats_dict[item_id][f"fclip_embed_{j}"] = float(embedding[j])

    # 8. –û–ß–ò–°–¢–ö–ê –í–†–ï–ú–ï–ù–ù–´–• –§–ê–ô–õ–û–í
    log_message("–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
    try:
        for temp_file in temp_stats_files:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        if os.path.exists(temp_dir) and not os.listdir(temp_dir):
            os.rmdir(temp_dir)
    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")

    log_message(f"–°–ª–æ–≤–∞—Ä—å —Ç–æ–≤–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω. –ó–∞–ø–∏—Å–µ–π: {len(item_stats_dict)}")
    return item_stats_dict


def build_category_features_dict(category_df, items_df):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Polars
    """
    import polars as pl

    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    if not isinstance(category_df, pl.DataFrame):
        category_pl = pl.from_pandas(
            category_df.compute() if hasattr(category_df, "compute") else category_df
        )
    else:
        category_pl = category_df

    if not isinstance(items_df, pl.DataFrame):
        items_pl = pl.from_pandas(
            items_df.compute() if hasattr(items_df, "compute") else items_df
        )
    else:
        items_pl = items_df

    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏—è -> —É—Ä–æ–≤–µ–Ω—å –≤ –∏–µ—Ä–∞—Ä—Ö–∏–∏
    cat_levels = category_pl.with_columns(
        [(pl.col("ids").list.lengths() - 1).alias("category_level")]
    ).select(["catalogid", "category_level"])

    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ —Ç–æ–≤–∞—Ä -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    item_categories = items_pl.select(["item_id", "catalogid"]).unique()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º
    category_features = item_categories.join(cat_levels, on="catalogid", how="left")
    category_features = category_features.with_columns(
        [pl.col("category_level").fill_null(0)]
    )

    # –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –°–õ–û–í–ê–†–¨
    category_features_dict = {}
    for row in category_features.iter_rows(named=True):
        category_features_dict[row["item_id"]] = {
            "item_category": row["catalogid"],
            "category_level": row["category_level"],
        }

    log_message(
        f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã. –ó–∞–ø–∏—Å–µ–π: {len(category_features_dict)}"
    )
    return category_features_dict


def prepare_lgbm_training_data(
    user_features_dict,
    item_features_dict,
    user_item_features_dict,
    category_features_dict,
    test_orders_df,
    all_items,  # —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö item_id
    sample_fraction=0.1,
):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LightGBM:
    1 –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä -> 1 –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–∏–º–µ—Ä.
    """
    log_message("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LightGBM...")

    # –ë–µ—Ä–µ–º sample –æ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–∫–∞–∑–æ–≤
    test_sample = test_orders_df.sample(frac=sample_fraction, random_state=42)

    train_examples = []

    for _, row in test_sample.iterrows():
        user_id = row["user_id"]
        pos_item_id = row["item_id"]

        # ---------- –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π ----------
        features = {}
        if user_id in user_features_dict:
            features.update(user_features_dict[user_id])
        if pos_item_id in item_features_dict:
            features.update(item_features_dict[pos_item_id])
        if (user_id, pos_item_id) in user_item_features_dict:
            features.update(user_item_features_dict[(user_id, pos_item_id)])
        if pos_item_id in category_features_dict:
            features.update(category_features_dict[pos_item_id])

        features["target"] = 1
        features["user_id"] = user_id
        features["item_id"] = pos_item_id
        train_examples.append(features)

        # ---------- –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π ----------
        neg_item_id = random.choice(all_items)
        while neg_item_id == pos_item_id:
            neg_item_id = random.choice(all_items)

        neg_features = {}
        if user_id in user_features_dict:
            neg_features.update(user_features_dict[user_id])
        if neg_item_id in item_features_dict:
            neg_features.update(item_features_dict[neg_item_id])
        if (user_id, neg_item_id) in user_item_features_dict:
            neg_features.update(user_item_features_dict[(user_id, neg_item_id)])
        if neg_item_id in category_features_dict:
            neg_features.update(category_features_dict[neg_item_id])

        neg_features["target"] = 0
        neg_features["user_id"] = user_id
        neg_features["item_id"] = neg_item_id
        train_examples.append(neg_features)

    # –°–æ–∑–¥–∞–µ–º DataFrame
    train_df = pd.DataFrame(train_examples)

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns
    train_df[numeric_cols] = train_df[numeric_cols].fillna(0)

    log_message(
        f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã. –†–∞–∑–º–µ—Ä: {len(train_df)} (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {len(test_sample)}, –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {len(test_sample)})"
    )
    return train_df


def load_and_process_embeddings(
    items_ddf, embedding_column="fclip_embed", device="cuda", max_items=0
):
    """
    –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å item_id -> np.array
    """
    log_message("–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

    if max_items > 0:
        items_sample = items_ddf[["item_id", embedding_column]].head(
            max_items, compute=True
        )
    else:
        items_sample = items_ddf[["item_id", embedding_column]].compute()

    embeddings_dict = {}
    for row in tqdm(
        items_sample.itertuples(index=False),
        total=len(items_sample),
        desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
    ):
        item_id = row.item_id
        embedding_data = getattr(row, embedding_column, None)
        if embedding_data is None:
            continue
        try:
            if isinstance(embedding_data, str):
                embedding = np.fromstring(
                    embedding_data.strip("[]"), sep=",", dtype=np.float32
                )
            elif isinstance(embedding_data, list):
                embedding = np.array(embedding_data, dtype=np.float32)
            elif isinstance(embedding_data, np.ndarray):
                embedding = embedding_data.astype(np.float32)
            else:
                continue
            if embedding.size > 0:
                embeddings_dict[item_id] = embedding
        except Exception:
            continue

    log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(embeddings_dict)} —Ç–æ–≤–∞—Ä–æ–≤")
    return embeddings_dict


def load_streaming_data(path_pattern: str) -> pd.DataFrame:
    files = sorted(glob(path_pattern))
    if not files:
        raise FileNotFoundError(f"–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω—É: {path_pattern}")
    dfs = [pd.read_parquet(f) for f in files]
    return pd.concat(dfs, ignore_index=True)


def normalize_item_features_dict(item_features_dict, embed_prefix="fclip_embed_"):
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞—Ä—å item_features_dict –∫ —Ñ–æ—Ä–º–∞—Ç—É {item_id: np.array([...])},
    –≥–¥–µ –≤–µ–∫—Ç–æ—Ä = [–æ–±—ã—á–Ω—ã–µ —Ñ–∏—á–∏ + —ç–º–±–µ–¥–¥–∏–Ω–≥–∏].
    """
    normalized = {}

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–ª—é—á–∏ –ø–æ –≤—Å–µ–º item
    all_keys = set()
    for feats in item_features_dict.values():
        all_keys.update(feats.keys())

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—ã—á–Ω—ã–µ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤—ã–µ
    base_keys = [k for k in all_keys if not k.startswith(embed_prefix)]
    embed_keys = sorted(
        [k for k in all_keys if k.startswith(embed_prefix)],
        key=lambda x: int(x.replace(embed_prefix, "")),
    )

    feature_order = base_keys + embed_keys

    # –§–æ—Ä–º–∏—Ä—É–µ–º numpy-–≤–µ–∫—Ç–æ—Ä–∞
    for item_id, feats in item_features_dict.items():
        row = []
        for key in feature_order:
            val = feats.get(key, 0.0)
            if val is None:
                val = 0.0
            row.append(val)
        normalized[item_id] = np.array(row, dtype=np.float32)

    return normalized, feature_order


def normalize_item_feats(item_feats, max_emb_dim=30):
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Ñ–∏—á–∏ –∞–π—Ç–µ–º–∞ –∫ —Å–ª–æ–≤–∞—Ä—é (dict).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç dict –∏ np.ndarray.
    """
    norm_feats = {}

    if isinstance(item_feats, dict):
        for k, v in item_feats.items():
            if isinstance(v, (int, float, np.number)):
                norm_feats[k] = float(v)
    elif isinstance(item_feats, np.ndarray):
        for i, v in enumerate(item_feats):
            if i < max_emb_dim:
                norm_feats[f"emb_{i}"] = float(v)

    return norm_feats


# -------------------- –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ --------------------
if __name__ == "__main__":
    start_time = time.time()

    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_file = "/home/root6/python/e_cup/rec_system/training_log.txt"

    def log_message(message):
        """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–∞–π–ª –∏ –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        full_message = f"[{timestamp}] {message}"
        print(full_message)
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(full_message + "\n")

    # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –ª–æ–≥–∞ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ
    with open(log_file, "w", encoding="utf-8") as f:
        f.write(
            f"=== –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n\n"
        )

    try:
        K = 100
        RECENT_N = 5
        TEST_SIZE = 0.2

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        SCALING_STAGE = "full"  # small, medium, large, full

        scaling_config = {
            "small": {"sample_users": 500, "sample_fraction": 0.1},
            "medium": {"sample_users": 5000, "sample_fraction": 0.3},
            "large": {"sample_users": 20000, "sample_fraction": 0.7},
            "full": {"sample_users": None, "sample_fraction": 1.0},
        }

        config = scaling_config[SCALING_STAGE]

        log_message(f"=== –†–ï–ñ–ò–ú –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–Ø: {SCALING_STAGE.upper()} ===")
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {config['sample_users'] or '–≤—Å–µ'}")
        log_message(f"–î–∞–Ω–Ω—ã—Ö: {config['sample_fraction']*100}%")

        # === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===")
        orders_ddf, tracker_ddf, items_ddf, categories_ddf, test_users_ddf = (
            load_train_data()
        )

        orders_ddf, tracker_ddf, items_ddf = filter_data(
            orders_ddf, tracker_ddf, items_ddf
        )

        stage_time = time.time() - stage_start
        log_message(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

        # === –ó–ê–ì–†–£–ó–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ===
        stage_start = time.time()
        log_message("=== –ó–ê–ì–†–£–ó–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ===")
        embeddings_dict = load_and_process_embeddings(items_ddf)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/embeddings_dict.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(embeddings_dict, f)
        log_message(
            f"–ü—Ä–∏–º–µ—Ä –∫–ª—é—á–µ–π –≤ embeddings_dict: {list(embeddings_dict.keys())[:5]}"
        )
        log_message(f"–ü—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏—è: {next(iter(embeddings_dict.values()))[:5]}")
        stage_time = time.time() - stage_start
        log_message(
            f"–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}"
        )
        log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings_dict)}")

        # === SPLIT –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== SPLIT –î–ê–ù–ù–´–• ===")
        orders_df_full = orders_ddf.compute()
        train_orders_df, test_orders_df, cutoff_ts_per_user = train_test_split_by_time(
            orders_df_full, TEST_SIZE
        )
        stage_time = time.time() - stage_start
        log_message(f"Split –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {timedelta(seconds=stage_time)}")
        log_message(
            f"Train orders: {len(train_orders_df)}, Test orders: {len(test_orders_df)}"
        )

        # === –ü–û–î–ì–û–¢–û–í–ö–ê –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–ô ===
        stage_start = time.time()
        log_message("=== –ü–û–î–ì–û–¢–û–í–ö–ê –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–ô ===")
        interactions_files = prepare_interactions(
            train_orders_df,
            tracker_ddf,
            cutoff_ts_per_user,
            scale_days=5,
            force_recreate=False,
        )
        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}"
        )
        log_message(f"–°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {len(interactions_files)}")

        # === –ü–û–°–õ–ï–î–ù–ò–ï –¢–û–í–ê–†–´ ===
        stage_start = time.time()
        log_message("=== –ü–û–°–õ–ï–î–ù–ò–ï –¢–û–í–ê–†–´ ===")
        batch_dir = "/home/root6/python/e_cup/rec_system/data/processed/prepare_interactions_batches"
        recent_items_map = build_recent_items_map_from_batches(
            batch_dir, recent_n=RECENT_N
        )
        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ recent items map –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}"
        )
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å recent items: {len(recent_items_map)}")

        # === –û–ë–£–ß–ï–ù–ò–ï ALS –î–õ–Ø –ü–†–ò–ó–ù–ê–ö–û–í ===
        stage_start = time.time()
        log_message("=== –û–ë–£–ß–ï–ù–ò–ï ALS –î–õ–Ø –ü–†–ò–ó–ù–ê–ö–û–í ===")
        model, user_map, item_map = train_als(
            interactions_files, n_factors=64, reg=1e-3, device="cuda"
        )
        inv_item_map = {v: k for k, v in item_map.items()}
        popularity_s = compute_global_popularity(
            orders_df_full, cutoff_ts_per_user
        )  # —Ç–µ–ø–µ—Ä—å —ç—Ç–æ pd.Timestamp
        popular_items = popularity_s.index.tolist()
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/popular_items.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(popular_items, f)
        stage_time = time.time() - stage_start
        log_message(f"–û–±—É—á–µ–Ω–∏–µ ALS –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}")
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_map)}, –¢–æ–≤–∞—Ä–æ–≤: {len(item_map)}")

        # === –ü–û–°–¢–†–û–ï–ù–ò–ï –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== –ü–û–°–¢–†–û–ï–ù–ò–ï –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –î–ê–ù–ù–´–• ===")

        # –°—Ç—Ä–æ–∏–º co-purchase map
        copurchase_map = build_copurchase_map(train_orders_df)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/copurchase_map.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(copurchase_map, f)
        log_message(f"Co-purchase map –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(copurchase_map)} —Ç–æ–≤–∞—Ä–æ–≤")

        # –°—Ç—Ä–æ–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
        items_df = items_ddf.compute()
        categories_df = categories_ddf.compute()
        item_to_cat, cat_to_items = build_category_maps(items_df, categories_df)
        save_path = "/home/root6/python/e_cup/rec_system/data/processed/item_to_cat.pkl"
        with open(save_path, "wb") as f:
            pickle.dump(item_to_cat, f)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/cat_to_items.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(cat_to_items, f)
        log_message(
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã: {len(item_to_cat)} —Ç–æ–≤–∞—Ä–æ–≤, {len(cat_to_items)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π"
        )

        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø LGBM ===
        stage_start = time.time()
        log_message("=== –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø LGBM ===")

        # User features
        user_start = time.time()
        user_features_dict = build_user_features_dict(interactions_files, orders_ddf)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/user_features_dict.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(user_features_dict, f)
        user_time = time.time() - user_start
        log_message(
            f"User features –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∑–∞ {timedelta(seconds=user_time)}: {len(user_features_dict)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"
        )

        # Item features
        item_start = time.time()
        raw_item_features_dict = build_item_features_dict(
            interactions_files, items_df, orders_ddf, embeddings_dict
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –≤ np.array
        item_features_dict, feature_order = normalize_item_features_dict(
            raw_item_features_dict
        )

        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä
        log_message(
            f"Item features: {len(item_features_dict)} —Ç–æ–≤–∞—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞: {len(feature_order)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
        )
        log_message(
            f"–ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {feature_order[:10]}{'...' if len(feature_order)>10 else ''}"
        )

        sample_items = list(item_features_dict.items())[:5]
        for k, v in sample_items:
            log_message(
                f"item_features_dict[{k}] type={type(v)}, shape={v.shape}, example={v[:5]}"
            )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/item_features_dict.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(item_features_dict, f)

        # –û—Ç–¥–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤–∞–∂–Ω–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/item_feature_order.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(feature_order, f)

        item_time = time.time() - item_start
        log_message(f"Item features –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∑–∞ {timedelta(seconds=item_time)}")

        # === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø LightGBM ===
        stage_start = time.time()
        log_message("=== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø LightGBM ===")
        recommender = LightGBMRecommender()
        recommender.set_als_embeddings(model)
        recommender.set_additional_data(
            copurchase_map, item_to_cat, cat_to_items, user_map, item_map
        )

        if embeddings_dict:
            recommender.set_external_embeddings(embeddings_dict)

        # –ú–ê–°–®–¢–ê–ë–ò–†–£–ï–ú –¥–∞–Ω–Ω—ã–µ
        if config["sample_users"]:
            sample_test_orders = test_orders_df.sample(
                min(config["sample_users"], len(test_orders_df)), random_state=42
            )
        else:
            sample_test_orders = test_orders_df

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å UI-–ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        train_data, val_data = recommender.prepare_training_data(
            interactions_files=interactions_files,
            orders_ddf=orders_ddf,
            user_map=user_map,
            item_map=item_map,
            popularity_s=popularity_s,
            recent_items_map=recent_items_map,
            copurchase_map=copurchase_map,
            item_to_cat=item_to_cat,
            cat_to_items=cat_to_items,
            user_features_dict=user_features_dict,
            item_features_dict=item_features_dict,
            embeddings_dict=embeddings_dict,
            sample_fraction=config["sample_fraction"],
            negatives_per_positive=0,
            split=0.2,
        )

        # --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ —Å—Ç—Ä–æ–∫ –≤ parquet ---
        def parquet_len(parquet_pattern: str) -> int:
            """–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ parquet —Ñ–∞–π–ª–∞—Ö –ø–æ —à–∞–±–ª–æ–Ω—É –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ –≤ –ø–∞–º—è—Ç—å"""
            return pl.scan_parquet(parquet_pattern).select(pl.count()).collect()[0, 0]

        # --- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ train/val ---
        train_parquet_pattern = "rec_system/data/processed/train_streaming/*.parquet"
        val_parquet_pattern = "rec_system/data/processed/val_streaming/*.parquet"

        train_len = parquet_len(train_parquet_pattern)
        val_len = parquet_len(val_parquet_pattern)

        log_message(f"–†–∞–∑–º–µ—Ä train: {train_len}, validation: {val_len}")
        log_message(f"–ü—Ä–∏–∑–Ω–∞–∫–∏: {len(recommender.feature_columns)}")

        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LightGBM –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í ===
        stage_start = time.time()
        log_message("=== –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê FEATURE GENERATION ===")

        # --- –ü—Ä–æ–≤–µ—Ä–∫–∏ user features ---
        log_message("--- –ü–†–û–í–ï–†–ö–ê USER FEATURES ---")
        if user_features_dict:
            sample_user = next(iter(user_features_dict))
            user_feats = user_features_dict[sample_user]
            log_message(f"–ü—Ä–∏–º–µ—Ä user features –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {sample_user}:")
            for feat, value in user_feats.items():
                log_message(f"  {feat}: {value}")

            users_with_features = len(user_features_dict)
            users_with_real_features = sum(
                1
                for feats in user_features_dict.values()
                if any(v != 0 for v in feats.values())
            )
            log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å features: {users_with_features}")
            log_message(
                f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –ù–ï–Ω—É–ª–µ–≤—ã–º–∏ features: {users_with_real_features}"
            )
        else:
            log_message("‚ö†Ô∏è user_features_dict –ü–£–°–¢–û–ô!")

        # --- –ü—Ä–æ–≤–µ—Ä–∫–∏ item features ---
        log_message("--- –ü–†–û–í–ï–†–ö–ê ITEM FEATURES ---")
        if item_features_dict:
            sample_item = next(iter(item_features_dict))
            raw_item_feats = item_features_dict[sample_item]
            item_feats = normalize_item_feats(raw_item_feats, max_emb_dim=EMD_LENGHT)

            log_message(f"–ü—Ä–∏–º–µ—Ä item features –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {sample_item}:")
            for feat, value in item_feats.items():
                log_message(f"  {feat}: {value}")

            items_with_features = len(item_features_dict)
            items_with_real_features = 0
            for feats in item_features_dict.values():
                norm_feats = normalize_item_feats(feats, max_emb_dim=EMD_LENGHT)
                if any(v != 0 for v in norm_feats.values()):
                    items_with_real_features += 1

            log_message(f"–¢–æ–≤–∞—Ä–æ–≤ —Å features: {items_with_features}")
            log_message(f"–¢–æ–≤–∞—Ä–æ–≤ —Å –ù–ï–Ω—É–ª–µ–≤—ã–º–∏ features: {items_with_real_features}")
        else:
            log_message("‚ö†Ô∏è item_features_dict –ü–£–°–¢–û–ô!")

        # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ---
        log_message("--- –ü–†–û–í–ï–†–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ---")
        if embeddings_dict:
            sample_item = next(iter(embeddings_dict))
            embedding = embeddings_dict[sample_item]
            log_message(
                f"–ü—Ä–∏–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {sample_item}: shape {embedding.shape}"
            )
            log_message(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(embeddings_dict)}")
            log_message(f"–ü—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏–π: {embedding[:5]}")
        else:
            log_message("‚ö†Ô∏è embeddings_dict –ü–£–°–¢–û–ô!")

        # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ co-purchase map ---
        log_message("--- –ü–†–û–í–ï–†–ö–ê CO-PURCHASE MAP ---")
        if copurchase_map:
            sample_item = next(iter(copurchase_map))
            co_items = copurchase_map[sample_item]
            log_message(
                f"–ü—Ä–∏–º–µ—Ä co-purchase –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {sample_item}: {len(co_items)} —Ç–æ–≤–∞—Ä–æ–≤"
            )
            log_message(f"Co-purchase –∑–∞–ø–∏—Å–µ–π: {len(copurchase_map)}")
        else:
            log_message("‚ö†Ô∏è copurchase_map –ü–£–°–¢–û–ô!")

        # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤ ---
        log_message("--- –ü–†–û–í–ï–†–ö–ê –ö–ê–¢–ï–ì–û–†–ò–ô–ù–´–• –ú–ê–ü–ü–ò–ù–ì–û–í ---")
        if item_to_cat and cat_to_items:
            sample_item = next(iter(item_to_cat))
            cat_id = item_to_cat[sample_item]
            cat_items = cat_to_items.get(cat_id, [])
            log_message(f"–¢–æ–≤–∞—Ä {sample_item} -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è {cat_id}")
            log_message(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {cat_id} -> {len(cat_items)} —Ç–æ–≤–∞—Ä–æ–≤")
            log_message(f"–¢–æ–≤–∞—Ä–æ–≤ –≤ –º–∞–ø–ø–∏–Ω–≥–µ: {len(item_to_cat)}")
            log_message(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –º–∞–ø–ø–∏–Ω–≥–µ: {len(cat_to_items)}")
        else:
            log_message("‚ö†Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ –ü–£–°–¢–´–ï!")

        stage_time = time.time() - stage_start
        log_message(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

        # === –û–ë–£–ß–ï–ù–ò–ï LightGBM ===
        stage_start = time.time()
        log_message("=== –û–ë–£–ß–ï–ù–ò–ï LightGBM ===")

        train_df = load_streaming_data(train_parquet_pattern)
        val_df = load_streaming_data(val_parquet_pattern)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
        recommender.debug_data_info(train_df, "TRAIN")
        recommender.debug_data_info(val_df, "VAL")

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if hasattr(recommender, "feature_columns"):
            log_message(f"Feature columns: {recommender.feature_columns}")
        else:
            log_message("‚ùå Feature columns –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã!")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
        if not train_df.empty and getattr(recommender, "feature_columns", None):
            model = recommender.train(train_df, val_df)
        else:
            log_message("‚ùå –ù–µ–ª—å–∑—è –æ–±—É—á–∞—Ç—å: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # model = recommender.train(train_df, val_df)

        stage_time = time.time() - stage_start
        log_message(f"–û–±—É—á–µ–Ω–∏–µ LightGBM –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}")

        # === –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò ===
        # stage_start = time.time()
        # log_message("=== –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò ===")
        # train_ndcg = recommender.evaluate(train_df)
        # val_ndcg = recommender.evaluate(val_df)

        # log_message(f"NDCG@100 train: {train_ndcg:.4f}")
        # log_message(f"NDCG@100 val: {val_ndcg:.4f}")
        # stage_time = time.time() - stage_start
        # log_message(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        stage_start = time.time()
        log_message("=== –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í ===")
        feature_importance = pd.DataFrame(
            {
                "feature": recommender.feature_columns,
                "importance": recommender.model.feature_importance(),
            }
        )
        feature_importance = feature_importance.sort_values(
            "importance", ascending=False
        )
        top_features = feature_importance.head(20)
        log_message("–¢–æ–ø-20 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for i, row in top_features.iterrows():
            log_message(f"  {row['feature']}: {row['importance']}")
        stage_time = time.time() - stage_start
        log_message(
            f"–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –í–ê–ñ–ù–´–• –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –ü–†–ò–ó–ù–ê–ö–û–í ===")
        save_data = {
            "lgbm_model": recommender.model,
            "feature_columns": recommender.feature_columns,
            "als_model": model,
            "user_map": user_map,
            "item_map": item_map,
            "inv_item_map": inv_item_map,
            "popular_items": popular_items,
            "user_features_dict": user_features_dict,
            "item_features_dict": item_features_dict,
            "recent_items_map": recent_items_map,
            "copurchase_map": copurchase_map,
            "item_to_cat": item_to_cat,
        }

        model_path = (
            "/home/root6/python/e_cup/rec_system/src/models/lgbm_model_full.pkl"
        )
        with open(model_path, "wb") as f:
            pickle.dump(save_data, f, protocol=pickle.HIGHEST_PROTOCOL)

        stage_time = time.time() - stage_start
        log_message(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}")
        log_message(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_path}")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        all_items = set()

        # === –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===
        total_time = time.time() - start_time
        log_message("=== –û–ë–£–ß–ï–ù–ò–ï –ò –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ó–ê–í–ï–†–®–ï–ù–´ –£–°–ü–ï–®–ù–û ===")
        log_message(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {timedelta(seconds=total_time)}")
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_map)}")
        log_message(f"–¢–æ–≤–∞—Ä–æ–≤: {len(item_map)}")
        log_message(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(recommender.feature_columns)}")
        # log_message(f"NDCG@100 train: {train_ndcg:.4f}")
        # log_message(f"NDCG@100 val: {val_ndcg:.4f}")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
        log_message("=== –°–ò–°–¢–ï–ú–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø ===")
        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
            import torch

            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                log_message(f"–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: {gpu_name}")
                log_message(f"–ü–∞–º—è—Ç—å GPU: {gpu_memory:.1f} GB")
            else:
                log_message("–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
        except Exception:
            log_message("–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CPU
            import multiprocessing

            import psutil

            cpu_freq = psutil.cpu_freq()
            cpu_cores = multiprocessing.cpu_count()
            log_message(f"–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä: {psutil.cpu_percent()}% –∑–∞–≥—Ä—É–∑–∫–∏")
            log_message(f"–Ø–¥—Ä–∞ CPU: {cpu_cores}")
            if cpu_freq:
                log_message(f"–ß–∞—Å—Ç–æ—Ç–∞ CPU: {cpu_freq.current:.1f} MHz")
        except Exception:
            log_message("–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ RAM
            import psutil

            ram = psutil.virtual_memory()
            ram_total = ram.total / 1024**3
            ram_used = ram.used / 1024**3
            log_message(
                f"–û–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å: {ram_total:.1f} GB –≤—Å–µ–≥–æ, {ram_used:.1f} GB –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ"
            )
        except Exception:
            log_message("–û–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        log_message("==========================================")
        log_message("–í–°–ï –≠–¢–ê–ü–´ –í–´–ü–û–õ–ù–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        log_message("==========================================")

    except Exception as e:
        error_time = time.time() - start_time
        log_message(f"!!! –û–®–ò–ë–ö–ê –í–´–ü–û–õ–ù–ï–ù–ò–Ø !!!")
        log_message(f"–û—à–∏–±–∫–∞: {str(e)}")
        log_message(f"–í—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {timedelta(seconds=error_time)}")
        log_message("–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
        import traceback

        traceback_str = traceback.format_exc()
        log_message(traceback_str)

    finally:
        # –í—Å–µ–≥–¥–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –≤—Ä–µ–º—è
        total_time = time.time() - start_time
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(
                f"\n=== –û–ë–©–ï–ï –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø: {timedelta(seconds=total_time)} ===\n"
            )
