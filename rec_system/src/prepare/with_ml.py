import gc
import glob
import json
import os
import pickle
import random
import shutil
import tempfile
import time
import traceback
import uuid
import warnings
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from glob import glob
from pathlib import Path

import dask.dataframe as dd
import lightgbm as lgb
import numpy as np
import pandas as pd
import polars as pl
import psutil
import pyarrow as pa
import pyarrow.parquet as pq
import torch
import torch.nn.functional as F
import torch.sparse
from dask.diagnostics import ProgressBar
from implicit.als import AlternatingLeastSquares
from scipy.sparse import coo_matrix, csr_matrix, vstack
from sklearn.metrics import ndcg_score
from sklearn.model_selection import train_test_split
from torch import nn
from tqdm import tqdm
from tqdm.auto import tqdm

warnings.filterwarnings(
    "ignore", category=FutureWarning, message=".*Downcasting object dtype arrays.*"
)
# tqdm –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å pandas
tqdm.pandas()


# -------------------- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö --------------------
def load_train_data(max_parts=0, max_rows=10000):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º parquet-—Ñ–∞–π–ª—ã orders, tracker, items, categories_tree, test_users.
    –ò—â–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ –ø–∞–ø–∫–∞–º –≤—Å–µ .parquet —Ñ–∞–π–ª—ã. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫.
    """

    paths = {
        "orders": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_orders_data/",
        "tracker": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_tracker_data/",
        "items": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_items_data/",
        "categories": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_categories_tree/",
        "test_users": "/home/root6/python/e_cup/rec_system/data/raw/test_users/",
    }

    columns_map = {
        "orders": ["item_id", "user_id", "created_timestamp", "last_status"],
        "tracker": ["item_id", "user_id", "timestamp", "action_type"],
        "items": ["item_id", "itemname", "fclip_embed", "catalogid"],
        "categories": ["catalogid", "catalogpath", "ids"],
        "test_users": ["user_id"],
    }

    dtype_profiles = {
        "orders": {
            "user_id": "int32",
            "item_id": "int32",
            "created_timestamp": "datetime64[ns]",
            "last_status": "category",
        },
        "tracker": {
            "user_id": "int32",
            "item_id": "int32",
            "timestamp": "datetime64[ns]",
            "action_type": "category",
        },
        "items": {
            "item_id": "int32",
            "catalogid": "int32",
            "itemname": "string",
            "fclip_embed": "object",
        },
        "categories": {
            "catalogid": "int32",
            "catalogpath": "string",
            "ids": "string",
        },
        "test_users": {"user_id": "int32"},
    }

    def find_parquet_files(folder):
        files = glob(os.path.join(folder, "**", "*.parquet"), recursive=True)
        files.sort()
        return files

    def read_sample(
        folder, columns=None, name="", max_parts=max_parts, max_rows=max_rows
    ):
        files = find_parquet_files(folder)
        if not files:
            log_message(f"{name}: parquet —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {folder}")
            return dd.from_pandas(pd.DataFrame(), npartitions=1)

        current_dtypes = dtype_profiles.get(name, {})

        try:
            # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
            ddf = dd.read_parquet(
                files,
                engine="pyarrow",
                dtype=current_dtypes,
                gather_statistics=False,
                split_row_groups=True,
            )
        except Exception as e:
            log_message(f"{name}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ parquet ({e}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return dd.from_pandas(pd.DataFrame(), npartitions=1)

        if columns is not None:
            available_cols = [c for c in columns if c in ddf.columns]
            if not available_cols:
                log_message(
                    f"{name}: –Ω–∏ –æ–¥–Ω–∞ –∏–∑ –∫–æ–ª–æ–Ω–æ–∫ {columns} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º"
                )
                return dd.from_pandas(pd.DataFrame(), npartitions=1)
            ddf = ddf[available_cols]

        total_parts = ddf.npartitions

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if max_parts > 0 and max_parts < total_parts:
            ddf = ddf.partitions[:max_parts]
            used_parts = max_parts
        else:
            used_parts = total_parts

        # –ï—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if max_rows == 0:
            count = ddf.shape[0].compute()
            mem_estimate = ddf.memory_usage(deep=True).sum().compute() / (1024**2)
            log_message(
                f"{name}: {count:,} —Å—Ç—Ä–æ–∫ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_parts} –∏–∑ {total_parts} –ø–∞—Ä—Ç–∏—Ü–∏–π), ~{mem_estimate:.1f} MB"
            )
            return ddf

        # –ë—ã—Å—Ç—Ä—ã–π —Å–ø–æ—Å–æ–± –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ max_rows
        # –î–ª—è —ç—Ç–æ–≥–æ —Å–Ω–∞—á–∞–ª–∞ –≤—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
        total_rows = ddf.shape[0].compute()

        if total_rows <= max_rows:
            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫ –º–µ–Ω—å—à–µ –ª–∏–º–∏—Ç–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å—ë
            count = total_rows
            mem_estimate = ddf.memory_usage(deep=True).sum().compute() / (1024**2)
            log_message(
                f"{name}: {count:,} —Å—Ç—Ä–æ–∫ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_parts} –∏–∑ {total_parts} –ø–∞—Ä—Ç–∏—Ü–∏–π), ~{mem_estimate:.1f} MB"
            )
            return ddf
        else:
            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫ –±–æ–ª—å—à–µ - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π ddf —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
            # –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º head —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º
            limited_ddf = ddf.head(max_rows, compute=False)
            count = max_rows
            mem_estimate = limited_ddf.memory_usage(deep=True).sum().compute() / (
                1024**2
            )
            log_message(
                f"{name}: {count:,} —Å—Ç—Ä–æ–∫ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_parts} –∏–∑ {total_parts} –ø–∞—Ä—Ç–∏—Ü–∏–π), ~{mem_estimate:.1f} MB"
            )
            return limited_ddf

    log_message("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    orders_ddf = read_sample(
        paths["orders"], columns=columns_map["orders"], name="orders"
    )
    tracker_ddf = read_sample(
        paths["tracker"], columns=columns_map["tracker"], name="tracker"
    )
    items_ddf = read_sample(paths["items"], columns=columns_map["items"], name="items")
    categories_ddf = read_sample(
        paths["categories"], columns=columns_map["categories"], name="categories"
    )
    test_users_ddf = read_sample(
        paths["test_users"], columns=columns_map["test_users"], name="test_users"
    )
    log_message("–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

    return orders_ddf, tracker_ddf, items_ddf, categories_ddf, test_users_ddf


# -------------------- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö --------------------
def filter_data(orders_ddf, tracker_ddf, items_ddf):
    """
    –§–∏–ª—å—Ç—Ä—É–µ–º: –æ—Å—Ç–∞–≤–ª—è–µ–º delivered_orders (–ø–æ–∑–∏—Ç–∏–≤) –∏ canceled_orders (–Ω–µ–≥–∞—Ç–∏–≤),
    –∞ —Ç–∞–∫–∂–µ –¥–µ–π—Å—Ç–≤–∏—è page_view, favorite, to_cart.
    """
    log_message("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")

    # –ó–∞–∫–∞–∑—ã: –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ delivered –∏ canceled
    orders_ddf = orders_ddf[
        orders_ddf["last_status"].isin(["delivered_orders", "canceled_orders"])
    ].copy()

    # delivered_orders = 1, canceled_orders = 0
    orders_ddf["target"] = orders_ddf["last_status"].apply(
        lambda x: 1 if x == "delivered_orders" else 0, meta=("target", "int8")
    )

    # –î–µ–π—Å—Ç–≤–∏—è
    tracker_ddf = tracker_ddf[
        tracker_ddf["action_type"].isin(["page_view", "favorite", "to_cart"])
    ]

    log_message("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return orders_ddf, tracker_ddf, items_ddf


# -------------------- Train/Test split –ø–æ –≤—Ä–µ–º–µ–Ω–∏ --------------------
def train_test_split_by_time(orders_df, test_size=0.2):
    """
    –î–µ–ª–µ–Ω–∏–µ –ø–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –¥–∞—Ç–µ: train = –ø–µ—Ä–≤—ã–µ (1 - test_size) –ø–æ –≤—Ä–µ–º–µ–Ω–∏,
    test = –ø–æ—Å–ª–µ–¥–Ω–∏–µ test_size –ø–æ –≤—Ä–µ–º–µ–Ω–∏.
    """
    orders_df = orders_df.copy()
    orders_df["created_timestamp"] = pd.to_datetime(orders_df["created_timestamp"])
    orders_df = orders_df.sort_values("created_timestamp")

    cutoff_idx = int(len(orders_df) * (1 - test_size))
    cutoff_ts = orders_df.iloc[cutoff_idx]["created_timestamp"]

    train_df = orders_df[orders_df["created_timestamp"] <= cutoff_ts]
    test_df = orders_df[orders_df["created_timestamp"] > cutoff_ts]

    return (
        train_df.reset_index(drop=True),
        test_df.reset_index(drop=True),
        cutoff_ts,
    )


# -------------------- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π --------------------
def prepare_interactions(
    train_orders_df,
    tracker_ddf,
    cutoff_ts_per_user,
    batch_size=300_000_000,
    action_weights=None,
    scale_days=5,
    output_dir="/home/root6/python/e_cup/rec_system/data/processed/prepare_interactions_batches",
):
    log_message("–§–æ—Ä–º–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø–æ –±–∞—Ç—á–∞–º...")

    if action_weights is None:
        action_weights = {"page_view": 2, "favorite": 5, "to_cart": 10}

    os.makedirs(output_dir, exist_ok=True)
    batch_files = []
    ref_time = train_orders_df["created_timestamp"].max()

    # ====== Orders ======
    log_message("... –¥–ª—è orders")
    n_rows = len(train_orders_df)
    for start in range(0, n_rows, batch_size):
        batch = train_orders_df.iloc[start : start + batch_size].copy()
        days_ago = (ref_time - batch["created_timestamp"]).dt.days.clip(lower=1)
        time_factor = np.log1p(days_ago / scale_days)
        batch = batch.assign(
            timestamp=batch["created_timestamp"],
            weight=5.0 * time_factor,
            action_type="order",
        )[["user_id", "item_id", "weight", "timestamp", "action_type"]]

        path = os.path.join(output_dir, f"orders_batch_{start}.parquet")
        batch.to_parquet(path, index=False, engine="pyarrow")
        batch_files.append(path)
        del batch
        gc.collect()
        log_message(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω orders-–±–∞—Ç—á {start}-{min(start+batch_size, n_rows)}")

    # ====== Tracker ======
    log_message("... –¥–ª—è tracker")
    tracker_ddf = tracker_ddf[["user_id", "item_id", "timestamp", "action_type"]]

    # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º Dask DataFrame
    n_partitions = tracker_ddf.npartitions
    for partition_id in range(n_partitions):
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–¥–Ω—É –ø–∞—Ä—Ç–∏—Ü–∏—é
        part = tracker_ddf.get_partition(partition_id).compute()
        part["timestamp"] = pd.to_datetime(part["timestamp"])

        # cutoff_ts_per_user –∑–¥–µ—Å—å –æ–¥–∏–Ω –≥–ª–æ–±–∞–ª—å–Ω—ã–π timestamp
        cutoff_ts = cutoff_ts_per_user
        mask = part["timestamp"] < cutoff_ts
        part = part.loc[mask]

        if part.empty:
            continue

        aw = part["action_type"].map(action_weights).fillna(0)
        days_ago = (ref_time - part["timestamp"]).dt.days.clip(lower=1)
        time_factor = np.log1p(days_ago / scale_days)
        part = part.assign(weight=aw * time_factor)[
            ["user_id", "item_id", "weight", "timestamp", "action_type"]
        ]

        path = os.path.join(output_dir, f"tracker_part_{partition_id}.parquet")
        part.to_parquet(path, index=False, engine="pyarrow")
        batch_files.append(path)
        del part
        gc.collect()
        log_message(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω tracker-–ø–∞—Ä—Ç–∏—Ü–∏—è {partition_id}")

    log_message("–í—Å–µ –±–∞—Ç—á–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –Ω–∞ –¥–∏—Å–∫.")
    return batch_files


# -------------------- –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å --------------------
def compute_global_popularity(orders_df, cutoff_ts_info):
    """
    –°—á–∏—Ç–∞–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–õ–¨–ö–û —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤.

    Args:
        orders_df: –í—Å–µ –∑–∞–∫–∞–∑—ã (–¥–æ split)
        cutoff_ts_info: –ª–∏–±–æ —Å–ª–æ–≤–∞—Ä—å {user_id: cutoff_ts}, –ª–∏–±–æ –æ–¥–∏–Ω –≥–ª–æ–±–∞–ª—å–Ω—ã–π pd.Timestamp
    """
    log_message("–°—á–∏—Ç–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

    orders_df = orders_df.copy()
    orders_df["created_timestamp"] = pd.to_datetime(orders_df["created_timestamp"])

    if isinstance(cutoff_ts_info, dict):
        # –ü–æ –∫–∞–∂–¥–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å–≤–æ–π cutoff
        train_orders = []
        for user_id, cutoff_ts in cutoff_ts_info.items():
            user_orders = orders_df[
                (orders_df["user_id"] == user_id)
                & (orders_df["created_timestamp"] < cutoff_ts)
            ]
            train_orders.append(user_orders)
        train_orders_df = (
            pd.concat(train_orders, ignore_index=True)
            if train_orders
            else pd.DataFrame(columns=orders_df.columns)
        )

    else:
        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π cutoff (–æ–¥–Ω–∞ –¥–∞—Ç–∞ –¥–ª—è –≤—Å–µ—Ö)
        cutoff_ts = cutoff_ts_info
        train_orders_df = orders_df[orders_df["created_timestamp"] < cutoff_ts]

    # –°—á–∏—Ç–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if train_orders_df.empty:
        log_message("–ù–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏.")
        return pd.Series(dtype=float)

    pop = (
        train_orders_df.groupby("item_id")["item_id"]
        .count()
        .sort_values(ascending=False)
    )
    popularity = pop / pop.max()
    log_message(
        f"–ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ {len(train_orders_df)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–∫–∞–∑–∞—Ö"
    )
    return popularity


# -------------------- –û–±—É—á–µ–Ω–∏–µ ALS --------------------
def train_als(interactions_files, n_factors=64, reg=1e-3, device="cuda"):
    """
    –í–µ—Ä—Å–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –±–∞—Ç—á–µ–π –Ω–∞ –¥–∏—Å–∫ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ item_map.pkl
    """
    # 1. –ü–†–û–•–û–î: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤
    user_set = set()
    item_set = set()
    log_message("–ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤...")

    for f in tqdm(interactions_files):
        df = pl.read_parquet(f, columns=["user_id", "item_id"])
        user_set.update(df["user_id"].unique().to_list())
        item_set.update(df["item_id"].unique().to_list())

    user_map = {u: i for i, u in enumerate(sorted(user_set))}
    item_map = {i: j for j, i in enumerate(sorted(item_set))}
    log_message(
        f"–ú–∞–ø–ø–∏–Ω–≥–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã. –£–Ω–∏–∫–æ–≤: users={len(user_map)}, items={len(item_map)}"
    )

    # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º item_map.pkl
    map_dir = "/home/root6/python/e_cup/rec_system/data/processed/"
    os.makedirs(map_dir, exist_ok=True)
    item_map_path = os.path.join(map_dir, "item_map.pkl")
    with open(item_map_path, "wb") as f:
        pickle.dump(item_map, f)
    log_message(f"item_map —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {item_map_path}")

    # 2. –ü–†–û–•–û–î: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞—Ç—á–µ–π –Ω–∞ –¥–∏—Å–∫
    log_message("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞—Ç—á–µ–π –Ω–∞ –¥–∏—Å–∫...")

    batch_dir = "/home/root6/python/e_cup/rec_system/data/processed/als_batches/"
    os.makedirs(batch_dir, exist_ok=True)

    user_map_df = pl.DataFrame(
        {"user_id": list(user_map.keys()), "user_idx": list(user_map.values())}
    )
    item_map_df = pl.DataFrame(
        {"item_id": list(item_map.keys()), "item_idx": list(item_map.values())}
    )

    batch_files = []
    for i, f in enumerate(tqdm(interactions_files)):
        df = pl.read_parquet(f, columns=["user_id", "item_id", "weight"])

        df = df.join(user_map_df, on="user_id", how="inner")
        df = df.join(item_map_df, on="item_id", how="inner")

        if len(df) > 0:
            batch_path = os.path.join(batch_dir, f"batch_{i:04d}.npz")
            np.savez(
                batch_path,
                rows=df["user_idx"].to_numpy().astype(np.int32),
                cols=df["item_idx"].to_numpy().astype(np.int32),
                vals=df["weight"].to_numpy().astype(np.float32),
            )
            batch_files.append(batch_path)

    # 3. –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ
    log_message("–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")

    als_model = TorchALS(
        len(user_map), len(item_map), n_factors=n_factors, device=device
    )

    for batch_path in tqdm(batch_files):
        try:
            data = np.load(batch_path)
            rows, cols, vals = data["rows"], data["cols"], data["vals"]

            indices_np = np.empty((2, len(rows)), dtype=np.int32)
            indices_np[0] = rows
            indices_np[1] = cols
            indices = torch.tensor(indices_np, dtype=torch.long, device=device)
            values = torch.tensor(vals, dtype=torch.float32, device=device)

            sparse_batch = torch.sparse_coo_tensor(
                indices, values, size=(len(user_map), len(item_map)), device=device
            )

            als_model.partial_fit(sparse_batch, iterations=5, lr=0.005)

            del sparse_batch, indices, values
            if device == "cuda":
                torch.cuda.empty_cache()

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {batch_path}: {e}")
            continue

    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    log_message("–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
    for batch_path in batch_files:
        try:
            os.remove(batch_path)
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {batch_path}: {e}")

    try:
        if not os.listdir(batch_dir):
            os.rmdir(batch_dir)
            log_message("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –±–∞—Ç—á–µ–π —É–¥–∞–ª–µ–Ω–∞")
        else:
            log_message("–í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –æ—Å—Ç–∞–ª–∏—Å—å —Ñ–∞–π–ª—ã, –Ω–µ —É–¥–∞–ª—è–µ–º")
    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {e}")

    log_message("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    return als_model, user_map, item_map


def build_copurchase_map(
    train_orders_df, min_co_items=2, top_n=10, device="cuda", max_items=500
):
    """
    —Å—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ–∫—É–ø–æ–∫ –¥–ª—è —Ç–æ–ø-N —Ç–æ–≤–∞—Ä–æ–≤
    –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –≤ JSON
    """
    log_message("–°—Ç—Ä–æ–∏–º co-purchase –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è —Ç–æ–ø-N —Ç–æ–≤–∞—Ä–æ–≤...")

    # 1. –¢–æ–ø –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
    item_popularity = train_orders_df["item_id"].value_counts()
    top_items = item_popularity.head(max_items).index.tolist()
    popular_items_set = set(top_items)

    log_message(f"–¢–æ–ø-{len(top_items)} –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã")

    # 2. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∫–æ—Ä–∑–∏–Ω
    baskets = []
    for items in train_orders_df.groupby(["user_id", "created_timestamp"])[
        "item_id"
    ].apply(list):
        filtered_items = [item for item in items if item in popular_items_set]
        if len(filtered_items) >= min_co_items:
            baskets.append(filtered_items)

    if not baskets:
        log_message("–ù–µ—Ç –∫–æ—Ä–∑–∏–Ω —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏")
        return {}

    log_message(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(baskets)} –∫–æ—Ä–∑–∏–Ω —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏")

    # 3. –°–ª–æ–≤–∞—Ä–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
    item2idx = {it: i for i, it in enumerate(top_items)}
    idx2item = {i: it for it, i in item2idx.items()}
    n_items = len(top_items)

    log_message(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {n_items}")

    # 4. Sparse –º–∞—Ç—Ä–∏—Ü–∞
    rows, cols, values = [], [], []
    for items in tqdm(baskets, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–∑–∏–Ω"):
        idxs = [item2idx[it] for it in items if it in item2idx]
        if len(idxs) < 2:
            continue

        weight = 1.0 / len(idxs)
        for i in range(len(idxs)):
            for j in range(len(idxs)):
                if i != j:
                    rows.append(idxs[i])
                    cols.append(idxs[j])
                    values.append(weight)

    if not rows:
        log_message("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã")
        return {}

    log_message(f"–°–æ–∑–¥–∞–µ–º sparse –º–∞—Ç—Ä–∏—Ü—É –∏–∑ {len(rows)} –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π...")

    rows_tensor = torch.tensor(rows, dtype=torch.long, device=device)
    cols_tensor = torch.tensor(cols, dtype=torch.long, device=device)
    values_tensor = torch.tensor(values, dtype=torch.float32, device=device)

    co_matrix = torch.sparse_coo_tensor(
        torch.stack([rows_tensor, cols_tensor]),
        values_tensor,
        size=(n_items, n_items),
        device=device,
    ).coalesce()

    log_message(
        f"Sparse –º–∞—Ç—Ä–∏—Ü–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞: {co_matrix.shape}, –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {co_matrix._nnz()}"
    )

    # 6. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    row_sums = torch.sparse.sum(co_matrix, dim=1).to_dense().clamp(min=1e-9)

    # 7. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
    final_copurchase = {}
    indices = co_matrix.indices()
    values = co_matrix.values()

    log_message("–§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏...")
    for i in tqdm(range(n_items), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤"):
        mask = indices[0] == i
        if mask.any():
            col_indices = indices[1][mask]
            row_values = values[mask] / row_sums[i]

            if len(row_values) > 0:
                topk_vals, topk_idx = torch.topk(
                    row_values, k=min(top_n, len(row_values))
                )
                final_copurchase[idx2item[i]] = [
                    (idx2item[col_indices[j].item()], topk_vals[j].item())
                    for j in range(len(topk_vals))
                    if topk_vals[j].item() > 0
                ]

    log_message(f"Co-purchase —Å–ª–æ–≤–∞—Ä—å –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {len(final_copurchase)} —Ç–æ–≤–∞—Ä–æ–≤")

    avg_recommendations = sum(len(v) for v in final_copurchase.values()) / max(
        1, len(final_copurchase)
    )
    log_message(f"–í —Å—Ä–µ–¥–Ω–µ–º {avg_recommendations:.1f} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ —Ç–æ–≤–∞—Ä")

    return final_copurchase


def build_category_maps(items_df, categories_df):
    """
    –£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: —Å—Ç—Ä–æ–∏–º –º–∞–ø–ø–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª—ã.
    """
    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤...")

    # –¢–æ–≤–∞—Ä -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    item_to_cat = dict(zip(items_df["item_id"], items_df["catalogid"]))

    # –ö–∞—Ç–µ–≥–æ—Ä–∏—è -> —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤
    cat_to_items = (
        items_df.groupby("catalogid")["item_id"].apply(lambda x: x.to_numpy()).to_dict()
    )

    # –ò–µ—Ä–∞—Ä—Ö–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    cat_tree = dict(zip(categories_df["catalogid"], categories_df["ids"]))

    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—é (–≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ)
    extended_cat_to_items = {}
    for cat_id, items_list in cat_to_items.items():
        all_items = set(items_list)
        parents = cat_tree.get(cat_id, [])
        for parent in parents:
            if parent in cat_to_items:
                all_items.update(cat_to_items[parent])
        extended_cat_to_items[cat_id] = np.array(list(all_items))
    return item_to_cat, extended_cat_to_items


# -------------------- –ú–µ—Ç—Ä–∏–∫–∏ --------------------
def ndcg_at_k(recommended, ground_truth, k=100, device="cuda"):
    """
    NDCG@K: —Å—á–∏—Ç–∞–µ–º —á–µ—Ä–µ–∑ torch –Ω–∞ GPU
    recommended: —Å–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö item_id
    ground_truth: –º–Ω–æ–∂–µ—Å—Ç–≤–æ/—Å–ø–∏—Å–æ–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö item_id
    """
    if not ground_truth:
        return 0.0

    # –ë–µ—Ä—ë–º —Ç–æ–ø-k —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    rec_k = torch.tensor(recommended[:k], device=device)

    # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
    gt_set = set(ground_truth)
    gt_mask = torch.tensor(
        [1 if x.item() in gt_set else 0 for x in rec_k],
        dtype=torch.float32,
        device=device,
    )

    # –ü–æ–∑–∏—Ü–∏–∏ (1..k)
    positions = torch.arange(1, len(rec_k) + 1, device=device, dtype=torch.float32)

    # DCG: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å / log2(–ø–æ–∑–∏—Ü–∏—è+1)
    dcg = torch.sum(gt_mask / torch.log2(positions + 1))

    # IDCG: –∏–¥–µ–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
    ideal_len = min(len(ground_truth), k)
    idcg = torch.sum(
        1.0
        / torch.log2(
            torch.arange(1, ideal_len + 1, device=device, dtype=torch.float32) + 1
        )
    )

    return (dcg / idcg).item() if idcg > 0 else 0.0


def build_recent_items_map_from_batches(
    batch_dir,
    recent_n=5,
    save_path="/home/root6/python/e_cup/rec_system/data/processed/recent_items_map.pkl",
):
    """–í–µ—Ä—Å–∏—è –≥–¥–µ weight –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ items.
    save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ None ‚Äî –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º).
    """
    batch_files = sorted(Path(batch_dir).glob("*.parquet"))
    recent_items_map = {}

    for f in tqdm(batch_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π"):
        try:
            df = pl.read_parquet(
                f, columns=["user_id", "item_id", "timestamp", "weight"]
            )

            df = df.with_columns(
                [
                    pl.col("user_id").cast(pl.Int64),
                    pl.col("item_id").cast(pl.Int64),
                    pl.col("timestamp").dt.epoch("s").alias("ts_epoch"),
                    pl.col("weight").cast(pl.Float64),
                ]
            )

            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
            df = df.with_columns(
                (
                    pl.col("weight") * 0.8
                    + pl.col("ts_epoch") / pl.col("ts_epoch").max() * 0.2
                ).alias("score")
            )

            df_sorted = df.sort(["user_id", "score"], descending=[False, True])

            grouped = df_sorted.group_by("user_id").agg(
                pl.col("item_id").head(recent_n).alias("items")
            )

            for row in grouped.iter_rows():
                user_id, items = row[0], row[1]
                if user_id not in recent_items_map:
                    recent_items_map[user_id] = items
                else:
                    combined = (recent_items_map[user_id] + items)[:recent_n]
                    recent_items_map[user_id] = combined

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {f}: {e}")
            continue

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    if save_path is not None:
        try:
            with open(save_path, "wb") as f:
                pickle.dump(recent_items_map, f)
            log_message(f"–°–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {save_path}: {e}")

    return recent_items_map


# -------------------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ --------------------
def save_model(model, user_map, item_map, path="src/models/model_als.pkl"):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–∞–ø–ø–∏–Ω–≥–∏ –≤ pickle.
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)

    data = {
        "model": model,
        "user_map": user_map,
        "item_map": item_map,
    }

    with open(path, "wb") as f:
        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)

    log_message(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")


# -------------------- –ú–µ—Ç—Ä–∏–∫–∏ --------------------
def ndcg_at_k_grouped(predictions, targets, groups, k=100, device="cpu"):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ NDCG@k –¥–ª—è —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch.

    predictions: 1D –º–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö score (list, np.array –∏–ª–∏ torch.tensor)
    targets:     1D –º–∞—Å—Å–∏–≤ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (0/1) —Ç–æ–π –∂–µ –¥–ª–∏–Ω—ã
    groups:      —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –≥—Ä—É–ø–ø (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∫–æ–ª—å–∫–æ –∞–π—Ç–µ–º–æ–≤ —É –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
    k:           —Ç–æ–ø-K –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏
    device:      "cpu" –∏–ª–∏ "cuda"
    """
    preds = torch.as_tensor(predictions, dtype=torch.float32, device=device)
    targs = torch.as_tensor(targets, dtype=torch.float32, device=device)

    ndcg_scores = []
    start_idx = 0

    for group_size in groups:
        if group_size == 0:
            continue

        end_idx = start_idx + group_size
        group_preds = preds[start_idx:end_idx]
        group_targs = targs[start_idx:end_idx]

        # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        sorted_idx = torch.argsort(group_preds, descending=True)
        sorted_targs = group_targs[sorted_idx]

        # DCG
        denom = torch.log2(
            torch.arange(2, 2 + min(k, group_size), device=device, dtype=torch.float32)
        )
        dcg = (sorted_targs[:k] / denom).sum()

        # IDCG
        ideal_sorted = torch.sort(group_targs, descending=True).values
        idcg = (ideal_sorted[:k] / denom).sum()

        ndcg = (dcg / idcg) if idcg > 0 else torch.tensor(0.0, device=device)
        ndcg_scores.append(ndcg)

        start_idx = end_idx

    if not ndcg_scores:
        return 0.0

    return float(torch.stack(ndcg_scores).mean().cpu())


class TorchALS(nn.Module):
    def __init__(
        self,
        n_users,
        n_items,
        n_factors=64,
        reg=1e-3,
        dtype=torch.float32,
        device="cuda",
    ):
        super().__init__()
        self.user_factors = nn.Parameter(
            nn.init.xavier_normal_(
                torch.empty(n_users, n_factors, dtype=dtype, device=device)
            )
        )
        self.item_factors = nn.Parameter(
            nn.init.xavier_normal_(
                torch.empty(n_items, n_factors, dtype=dtype, device=device)
            )
        )
        self.reg = reg
        self.device = device
        self.partial_optimizer = None  # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è partial_fit
        self.to(device)

    def forward(self, user, item):
        return (self.user_factors[user] * self.item_factors[item]).sum(1)

    def partial_fit(self, sparse_batch, iterations=5, lr=0.005, show_progress=False):
        """
        –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞
        """
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º sparse tensor
        if not sparse_batch.is_coalesced():
            sparse_batch = sparse_batch.coalesce()

        users_coo, items_coo = sparse_batch.indices()
        values = sparse_batch.values()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
        if self.partial_optimizer is None:
            self.partial_optimizer = torch.optim.Adam(
                self.parameters(), lr=lr, weight_decay=self.reg
            )
        else:
            # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
            for param_group in self.partial_optimizer.param_groups:
                param_group["lr"] = lr

        # –û–±—É—á–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞ (–±–µ–∑ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è!)
        for epoch in range(iterations):
            self.partial_optimizer.zero_grad()

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –≤ –±–∞—Ç—á–µ
            pred = self.forward(users_coo, items_coo)
            loss = F.mse_loss(pred, values)

            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            user_reg = self.reg * self.user_factors[users_coo].pow(2).mean()
            item_reg = self.reg * self.item_factors[items_coo].pow(2).mean()
            total_loss = loss + user_reg + item_reg

            total_loss.backward()
            self.partial_optimizer.step()

            if show_progress and (epoch % 10 == 0 or epoch == iterations - 1):
                log_message(f"Partial fit epoch {epoch}, Loss: {total_loss.item():.6f}")


class LightGBMRecommender:
    def __init__(self):
        self.model = None
        self.feature_columns = []
        self.user_embeddings = None
        self.item_embeddings = None
        self.external_embeddings_dict = None
        self.copurchase_map = None
        self.item_to_cat = None
        self.cat_to_items = None
        self.user_map = None
        self.item_map = None
        self.covisitation_matrix = None

    def load_training_data_from_parquet(self, train_dir, val_dir):
        """–ó–∞–≥—Ä—É–∑–∫–∞ train/val –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–∞—Ä–∫–µ—Ç-—Ñ–∞–π–ª–æ–≤"""
        train_files = sorted(Path(train_dir).glob("*.parquet"))
        val_files = sorted(Path(val_dir).glob("*.parquet"))

        train_dfs = [pd.read_parquet(f) for f in train_files]
        val_dfs = [pd.read_parquet(f) for f in val_files]

        train_data = pd.concat(train_dfs, ignore_index=True)
        val_data = pd.concat(val_dfs, ignore_index=True)

        return train_data, val_data

    def set_als_embeddings(self, als_model):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º ALS —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"""
        self.user_embeddings = als_model.user_factors
        self.item_embeddings = als_model.item_factors

    def set_external_embeddings(self, embeddings_dict):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤"""
        self.external_embeddings_dict = embeddings_dict

    def set_additional_data(
        self, copurchase_map, item_to_cat, cat_to_items, user_map, item_map
    ):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        self.copurchase_map = copurchase_map
        self.item_to_cat = item_to_cat
        self.cat_to_items = cat_to_items
        self.user_map = user_map
        self.item_map = item_map

    def _train_covisitation_matrix(
        self, train_data: pd.DataFrame, min_cooccurrence: int = 5
    ):
        """–û–±—É—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        log_message("–û–±—É—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏–∏ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö...")

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å: user_id -> —Å–ø–∏—Å–æ–∫ item_id —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞–ª
        user_items = {}
        for user_id, group in train_data.groupby("user_id"):
            user_items[user_id] = set(group["item_id"].unique())

        # –°—á–∏—Ç–∞–µ–º –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏—é (—Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –ø–æ—è–≤–ª–µ–Ω–∏—è)
        cooccurrence = defaultdict(int)

        for user_id, items in user_items.items():
            items_list = list(items)
            for i in range(len(items_list)):
                for j in range(i + 1, len(items_list)):
                    item1, item2 = items_list[i], items_list[j]
                    # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –ø–∞—Ä—É –¥–ª—è consistency
                    pair = (min(item1, item2), max(item1, item2))
                    cooccurrence[pair] += 1

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏–π
        self.covisitation_matrix = {}
        for (item1, item2), count in cooccurrence.items():
            if count >= min_cooccurrence:
                self.covisitation_matrix[item1] = (
                    self.covisitation_matrix.get(item1, 0) + count
                )
                self.covisitation_matrix[item2] = (
                    self.covisitation_matrix.get(item2, 0) + count
                )

        log_message(
            f"–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏–∏: {len(self.covisitation_matrix)} —Ç–æ–≤–∞—Ä–æ–≤"
        )

    def _load_ui_features_for_pairs(self, pairs_df, ui_features_path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç UI-–ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞—Ä user-item
        """
        try:
            if pairs_df.empty:
                return None

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–º–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            temp_pairs_path = "/tmp/filter_pairs.parquet"
            pairs_df[["user_id", "item_id"]].to_parquet(temp_pairs_path, index=False)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Polars –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            result = (
                pl.scan_parquet(ui_features_path)
                .join(
                    pl.scan_parquet(temp_pairs_path),
                    on=["user_id", "item_id"],
                    how="inner",
                )
                .collect()
                .to_pandas()
            )

            # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if os.path.exists(temp_pairs_path):
                os.remove(temp_pairs_path)

            return result

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–∞—Ä: {e}")
            return None

    def _add_rich_features(
        self, data: pd.DataFrame, train_only_data: pd.DataFrame = None
    ) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏—á —Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ–º —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""
        log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–æ–≥–∞—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
        if "timestamp" in data.columns:
            try:
                data["is_weekend"] = (data["timestamp"].dt.dayofweek >= 5).astype(int)
                data["hour"] = data["timestamp"].dt.hour.fillna(0)
            except Exception as e:
                log_message(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å timestamp: {e}")
                data["is_weekend"] = 0
                data["hour"] = 0
        else:
            data["is_weekend"] = 0
            data["hour"] = 0

        # –ï—Å–ª–∏ train_only_data –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –±–µ—Ä—ë–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ data
        if train_only_data is None or train_only_data.empty:
            train_only_data = data[data.get("target", 0) == 0]

        # --- –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–∞ ---
        if not train_only_data.empty:
            item_pop = (
                train_only_data.groupby("item_id")["user_id"]
                .count()
                .rename("item_popularity")
                .reset_index()
            )
            if not item_pop.empty:
                data = data.merge(item_pop, on="item_id", how="left")

        data["item_popularity"] = data.get("item_popularity", 0).fillna(0)

        # --- –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ---
        if "category_id" in data.columns:
            if not train_only_data.empty and "category_id" in train_only_data.columns:
                cat_pop = (
                    train_only_data.groupby("category_id")["user_id"]
                    .count()
                    .rename("category_popularity")
                    .reset_index()
                )
                if not cat_pop.empty:
                    data = data.merge(cat_pop, on="category_id", how="left")
            data["category_popularity"] = data.get("category_popularity", 0).fillna(0)
        else:
            data["category_popularity"] = 0

        # --- –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
        if not train_only_data.empty:
            user_activity = (
                train_only_data.groupby("user_id")["item_id"]
                .count()
                .rename("user_activity")
                .reset_index()
            )
            if not user_activity.empty:
                data = data.merge(user_activity, on="user_id", how="left")
        data["user_activity"] = data.get("user_activity", 0).fillna(0)

        # --- –°—Ä–µ–¥–Ω—è—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–æ–≤ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
        if not train_only_data.empty and "item_popularity" in data.columns:
            train_with_pop = (
                train_only_data.merge(item_pop, on="item_id", how="left")
                if not item_pop.empty
                else train_only_data.copy()
            )
            if "item_popularity" in train_with_pop.columns:
                train_with_pop["item_popularity"] = train_with_pop[
                    "item_popularity"
                ].fillna(0)
                user_avg_pop = (
                    train_with_pop.groupby("user_id")["item_popularity"]
                    .mean()
                    .rename("user_avg_item_popularity")
                    .reset_index()
                )
                if not user_avg_pop.empty:
                    data = data.merge(user_avg_pop, on="user_id", how="left")
        data["user_avg_item_popularity"] = data.get(
            "user_avg_item_popularity", 0
        ).fillna(0)

        # --- –ö–æ–≤–∏–∑–∏—Ç–∞—Ü–∏—è ---
        if (
            hasattr(self, "covisitation_matrix")
            and self.covisitation_matrix is not None
        ):
            data["covisitation_score"] = (
                data["item_id"].map(self.covisitation_matrix.get).fillna(0)
            )
        else:
            data["covisitation_score"] = 0

        # --- FCLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ---
        if getattr(self, "external_embeddings_dict", None):
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            all_item_ids = list(self.external_embeddings_dict.keys())
            embedding_dim = len(next(iter(self.external_embeddings_dict.values())))
            n_fclip_dims = min(10, embedding_dim)
            embeddings_tensor = torch.tensor(
                [self.external_embeddings_dict[i] for i in all_item_ids],
                dtype=torch.float32,
                device=device,
            )
            item_id_to_idx = {item_id: idx for idx, item_id in enumerate(all_item_ids)}
            batch_size = 100_000
            total_rows = len(data)
            for i in range(n_fclip_dims):
                data[f"fclip_embed_{i}"] = 0.0
                for start_idx in range(0, total_rows, batch_size):
                    end_idx = min(start_idx + batch_size, total_rows)
                    batch_item_ids = data.iloc[start_idx:end_idx]["item_id"].values
                    valid_mask = np.array(
                        [item in item_id_to_idx for item in batch_item_ids]
                    )
                    valid_indices = np.where(valid_mask)[0]
                    valid_item_ids = batch_item_ids[valid_mask]
                    if len(valid_item_ids):
                        tensor_indices = torch.tensor(
                            [item_id_to_idx[item] for item in valid_item_ids],
                            device=device,
                        )
                        batch_emb = embeddings_tensor[tensor_indices, i].cpu().numpy()
                        data.iloc[
                            start_idx + valid_indices,
                            data.columns.get_loc(f"fclip_embed_{i}"),
                        ] = batch_emb
            del embeddings_tensor, item_id_to_idx
            torch.cuda.empty_cache()

        # --- –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ---
        new_features = [
            "is_weekend",
            "hour",
            "item_popularity",
            "category_popularity",
            "user_activity",
            "user_avg_item_popularity",
            "covisitation_score",
        ]
        if getattr(self, "external_embeddings_dict", None):
            new_features += [f"fclip_embed_{i}" for i in range(n_fclip_dims)]

        existing_features = set(getattr(self, "feature_columns", []))
        for feature in new_features:
            if feature in data.columns and feature not in existing_features:
                self.feature_columns.append(feature)
                existing_features.add(feature)

        log_message(f"–î–æ–±–∞–≤–ª–µ–Ω—ã —Ñ–∏—á–∏: {[f for f in new_features if f in data.columns]}")
        log_message(f"–í—Å–µ–≥–æ —Ñ–∏—á –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {len(self.feature_columns)}")

        return data

    def prepare_training_data(
        self,
        interactions_files,
        orders_ddf,
        user_map,
        item_map,
        popularity_s,
        recent_items_map,
        copurchase_map=None,
        item_to_cat=None,
        cat_to_items=None,
        user_features_dict=None,
        item_features_dict=None,
        embeddings_dict=None,
        sample_fraction=0.1,
        negatives_per_positive=3,
        split: float = 0.2,  # 20% —Ö–≤–æ—Å—Ç –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏—é
    ):
        log_message("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LightGBM (streaming)...")

        # --- sample fraction –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º ---
        orders_ddf = orders_ddf.map_partitions(
            lambda df: df.sample(frac=sample_fraction, random_state=42)
        )

        base_dir = Path("/home/root6/python/e_cup/rec_system/data/processed/")
        base_dir.mkdir(parents=True, exist_ok=True)

        interactions_out_dir = base_dir / "interactions_streaming"
        train_out_dir = base_dir / "train_streaming"
        val_out_dir = base_dir / "val_streaming"

        for p in [interactions_out_dir, train_out_dir, val_out_dir]:
            p.mkdir(parents=True, exist_ok=True)

        batch_size = 500_000
        file_idx = 0

        # --- streaming –∑–∞–ø–∏—Å—å interactions ---
        def write_interactions_batch(batch_list):
            nonlocal file_idx
            if not batch_list:
                return
            batch_df = pd.DataFrame(
                batch_list, columns=["user_id", "item_id", "timestamp", "weight"]
            )
            out_path = interactions_out_dir / f"part_{file_idx:05d}.parquet"
            batch_df.to_parquet(out_path, index=False, engine="pyarrow")
            file_idx += 1
            batch_list.clear()

        interactions_batch = []
        for f in tqdm(interactions_files, desc="–ó–∞–≥—Ä—É–∑–∫–∞ interactions"):
            df = pd.read_parquet(
                f, columns=["user_id", "item_id", "timestamp", "weight"]
            )
            df["user_id"] = df["user_id"].astype("int64")
            df["item_id"] = df["item_id"].astype("int64")
            df["timestamp"] = pd.to_datetime(df["timestamp"])

            for start in range(0, len(df), batch_size):
                batch = df.iloc[start : start + batch_size]
                interactions_batch.extend(batch.values.tolist())
                if len(interactions_batch) >= batch_size:
                    write_interactions_batch(interactions_batch)
        if interactions_batch:
            write_interactions_batch(interactions_batch)

        # --- split_time ---
        parquet_files = sorted(interactions_out_dir.glob("*.parquet"))
        if not parquet_files:
            raise ValueError("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ interactions –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è split_time")

        mins, maxs = [], []
        for f in parquet_files:
            ts = pd.read_parquet(f, columns=["timestamp"])["timestamp"]
            if ts.empty:
                continue
            mins.append(ts.min())
            maxs.append(ts.max())

        if not maxs:
            raise ValueError("–í–æ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö interactions –Ω–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫")

        min_timestamp = min(mins)
        max_timestamp = max(maxs)
        duration = max_timestamp - min_timestamp

        if duration <= pd.Timedelta(0):
            split_by_time = False
            split_time = max_timestamp
        else:
            split_by_time = True
            split_time = max_timestamp - duration * split

        train_timestamp_fill = split_time - pd.Timedelta(seconds=1)

        log_message(
            f"min_timestamp={min_timestamp}; max_timestamp={max_timestamp}; duration={duration}"
        )
        log_message(
            f"split_time ({int(split*100)}% tail) = {split_time}; train_timestamp_fill = {train_timestamp_fill}"
        )

        # --- user_interacted_items_train ---
        user_interacted_items_train = {}
        for f in tqdm(
            sorted(interactions_out_dir.glob("*.parquet")), desc="User history build"
        ):
            df = pd.read_parquet(f, columns=["user_id", "item_id", "timestamp"])
            train_df = df[df["timestamp"] <= split_time]
            for uid, gr in train_df.groupby("user_id"):
                user_interacted_items_train.setdefault(uid, set()).update(
                    gr["item_id"].unique()
                )
            del df, train_df
            gc.collect()

        # --- –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –Ω–µ–≥–∞—Ç–∏–≤–æ–≤ ---
        all_items = np.array(list(item_map.keys()), dtype=np.int64)
        popular_items_set = set(popularity_s.nlargest(10000).index.astype(np.int64))
        all_items_tensor = torch.tensor(all_items, device="cuda")
        popular_items_tensor = torch.tensor(list(popular_items_set), device="cuda")

        train_file_idx = 0
        val_file_idx = 0
        train_data_batches = []
        val_data_batches = []

        # --- —Ñ–∏–∫—Å–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ ---
        fixed_feature_keys = [
            "copurchase_count",
        ]

        user_feature_keys, item_feature_keys = [], []
        if user_features_dict:
            all_user_keys = set()
            for feats in user_features_dict.values():
                all_user_keys.update(feats.keys())
            user_feature_keys = [f"user_{k}" for k in sorted(all_user_keys)]
        if item_features_dict:
            all_item_keys = set()
            for feats in item_features_dict.values():
                if isinstance(feats, dict):
                    all_item_keys.update(feats.keys())
                elif isinstance(feats, np.ndarray):
                    all_item_keys.update(
                        [f"emb_{i}" for i in range(min(30, feats.shape[0]))]
                    )
                else:
                    raise TypeError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {type(feats)}")
            item_feature_keys = [f"item_{k}" for k in sorted(all_item_keys)]

        feature_cols = fixed_feature_keys + user_feature_keys + item_feature_keys

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º feature_columns –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏
        self.feature_columns = feature_cols

        log_message(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏: {feature_cols}")
        log_message(
            f"User features: {len(user_feature_keys)}, Item features: {len(item_feature_keys)}"
        )

        # --- enrich_features ---
        def enrich_features(uid, item_id):
            features = {}
            features["copurchase_count"] = (
                sum(x[1] for x in copurchase_map[item_id])
                if copurchase_map and item_id in copurchase_map
                else 0.0
            )

            # user features
            user_feats = user_features_dict.get(uid, {}) if user_features_dict else {}
            for k, v in user_feats.items():
                features[f"user_{k}"] = v

            # item features (—É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ)
            raw_item_feats = (
                item_features_dict.get(item_id, {}) if item_features_dict else {}
            )
            item_feats = normalize_item_feats(raw_item_feats, max_emb_dim=30)

            for k, v in item_feats.items():
                if k.startswith("emb_"):
                    idx = int(k.split("_")[1])
                    if idx < 30:
                        features[f"item_{k}"] = v
                elif k.startswith("item_fclip_embed_"):
                    features[f"item_{k}"] = v
                elif k in [
                    "item_item_count",
                    "item_item_orders_count",
                    "item_item_mean",
                ]:
                    features[f"item_{k}"] = np.log1p(v)
                else:
                    features[f"item_{k}"] = v

            return features

        # --- –≥–µ–Ω–µ—Ä–∞—Ü–∏—è train/val ---
        small_batch = []

        train_file_idx = 0
        val_file_idx = 0

        # —Å—É–º–º–∞—Ä–Ω–æ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–º–æ–∂–Ω–æ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º, –µ—Å–ª–∏ —É–¥–æ–±–Ω–µ–µ)
        total_users = orders_ddf["user_id"].nunique().compute()

        for part in orders_ddf.to_delayed():
            pdf = part.compute()
            for uid, gr in tqdm(
                pdf.groupby("user_id"),
                desc="Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π",
                total=pdf["user_id"].nunique(),
                leave=False,
            ):
                pos_items = set(gr[gr["target"] == 1]["item_id"])
                neg_items_existing = set(gr[gr["target"] == 0]["item_id"])
                interacted = user_interacted_items_train.get(uid, set())
                excluded = pos_items | neg_items_existing | interacted

                # --- —Ñ–æ—Ä–º–∏—Ä—É–µ–º –º–∞—Å–∫—É –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ GPU ---
                excluded_tensor = (
                    torch.tensor(list(excluded), device="cuda", dtype=torch.int32)
                    if excluded
                    else torch.tensor([], device="cuda", dtype=torch.int32)
                )
                mask = ~torch.isin(all_items_tensor, excluded_tensor)
                available_items_tensor = all_items_tensor[mask]

                # --- –ø–æ–¥–≥—Ä—É–∂–∞–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤) ---
                inter_user_list = []
                for f in interactions_out_dir.glob("*.parquet"):
                    d = pd.read_parquet(
                        f, columns=["user_id", "item_id", "timestamp", "weight"]
                    )
                    inter_user_list.append(d[d["user_id"] == uid])
                if inter_user_list:
                    inter_user = pd.concat(inter_user_list)
                else:
                    inter_user = pd.DataFrame(
                        columns=["item_id", "timestamp", "weight"]
                    )

                candidate_items = list(pos_items | neg_items_existing)
                for it in candidate_items:
                    target = 1 if it in pos_items else 0
                    if it in inter_user["item_id"].values:
                        ts = inter_user.loc[
                            inter_user["item_id"] == it, "timestamp"
                        ].max()
                        weight = inter_user.loc[
                            inter_user["item_id"] == it, "weight"
                        ].max()
                    else:
                        ts = train_timestamp_fill
                        weight = 0.0
                    feats = enrich_features(uid, it)
                    feature_values = [feats.get(c, 0) for c in feature_cols]
                    small_batch.append([uid, it, ts, target, weight] + feature_values)

                # --- –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–æ–≤ ---
                n_needed = max(
                    0, len(pos_items) * negatives_per_positive - len(neg_items_existing)
                )
                if n_needed > 0 and len(available_items_tensor) > 0:
                    popular_mask = torch.isin(
                        available_items_tensor, popular_items_tensor
                    )
                    popular_candidates = available_items_tensor[popular_mask]
                    random_candidates = available_items_tensor[~popular_mask]

                    n_popular = min(n_needed // 2, len(popular_candidates))
                    n_random = min(n_needed - n_popular, len(random_candidates))
                    sampled_items = []

                    if n_popular > 0:
                        perm = torch.randperm(len(popular_candidates), device="cuda")
                        sampled_items.extend(
                            popular_candidates[perm[:n_popular]].tolist()
                        )
                    if n_random > 0:
                        perm = torch.randperm(len(random_candidates), device="cuda")
                        sampled_items.extend(
                            random_candidates[perm[:n_random]].tolist()
                        )

                    for it in sampled_items:
                        feats = enrich_features(uid, it)
                        feature_values = [feats.get(c, 0) for c in feature_cols]
                        small_batch.append(
                            [uid, it, train_timestamp_fill, 0, 0.0] + feature_values
                        )

                # --- –∑–∞–ø–∏—Å—å –±–∞—Ç—á–∞ ---
                if len(small_batch) >= batch_size:
                    cols_order = [
                        "user_id",
                        "item_id",
                        "timestamp",
                        "target",
                        "weight",
                    ] + feature_cols
                    batch_df = pd.DataFrame(small_batch, columns=cols_order)

                    # —Å–∂–∞—Ç–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    batch_df["user_id"] = batch_df["user_id"].astype("int32")
                    batch_df["item_id"] = batch_df["item_id"].astype("int32")
                    batch_df["target"] = batch_df["target"].astype("int8")
                    batch_df["weight"] = batch_df["weight"].astype("float32")
                    for c in feature_cols:
                        batch_df[c] = batch_df[c].astype("float32")

                    # --- —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val ---
                    if split_by_time:
                        train_df = batch_df[batch_df["timestamp"] <= split_time]
                        val_df = batch_df[batch_df["timestamp"] > split_time]
                        if val_df.empty:
                            idx_split = int(len(batch_df) * (1 - split))
                            train_df = batch_df.iloc[:idx_split]
                            val_df = batch_df.iloc[idx_split:]
                    else:
                        idx_split = int(len(batch_df) * (1 - split))
                        train_df = batch_df.iloc[:idx_split]
                        val_df = batch_df.iloc[idx_split:]

                    # --- —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
                    if not train_df.empty:
                        train_df = train_df.drop(columns=["timestamp"])
                        train_df.to_parquet(
                            train_out_dir / f"part_{train_file_idx:05d}.parquet",
                            index=False,
                            engine="pyarrow",
                        )
                        train_data_batches.append(train_df)
                        train_file_idx += 1

                    if not val_df.empty:
                        val_df = val_df.drop(columns=["timestamp"])
                        val_df.to_parquet(
                            val_out_dir / f"part_{val_file_idx:05d}.parquet",
                            index=False,
                            engine="pyarrow",
                        )
                        val_data_batches.append(val_df)
                        val_file_idx += 1

                    small_batch.clear()

        # --- —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å—Ç–∞—Ç–∫–∞ ---
        if small_batch:
            cols_order = [
                "user_id",
                "item_id",
                "timestamp",
                "target",
                "weight",
            ] + feature_cols
            batch_df = pd.DataFrame(small_batch, columns=cols_order)

            batch_df["user_id"] = batch_df["user_id"].astype("int32")
            batch_df["item_id"] = batch_df["item_id"].astype("int32")
            batch_df["target"] = batch_df["target"].astype("int8")
            batch_df["weight"] = batch_df["weight"].astype("float32")
            for c in feature_cols:
                batch_df[c] = batch_df[c].astype("float32")

            if split_by_time:
                train_df = batch_df[batch_df["timestamp"] <= split_time]
                val_df = batch_df[batch_df["timestamp"] > split_time]
                if val_df.empty:
                    idx_split = int(len(batch_df) * (1 - split))
                    train_df = batch_df.iloc[:idx_split]
                    val_df = batch_df.iloc[idx_split:]
            else:
                idx_split = int(len(batch_df) * (1 - split))
                train_df = batch_df.iloc[:idx_split]
                val_df = batch_df.iloc[idx_split:]

            if not train_df.empty:
                train_df = train_df.drop(columns=["timestamp"])
                train_df.to_parquet(
                    train_out_dir / f"part_{train_file_idx:05d}.parquet",
                    index=False,
                    engine="pyarrow",
                )
                train_data_batches.append(train_df)
            if not val_df.empty:
                val_df = val_df.drop(columns=["timestamp"])
                val_df.to_parquet(
                    val_out_dir / f"part_{val_file_idx:05d}.parquet",
                    index=False,
                    engine="pyarrow",
                )
                val_data_batches.append(val_df)

        # --- –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ---
        def log_message_dist(df, name):
            if "target" in df.columns:
                counts = df["target"].value_counts(dropna=False).to_dict()
            else:
                counts = {}
            log_message(f"{name}: rows={len(df)}; target_counts={counts}")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if hasattr(self, "feature_columns"):
                missing_features = [
                    f for f in self.feature_columns if f not in df.columns
                ]
                if missing_features:
                    log_message(f"‚ö†Ô∏è –í {name} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
                else:
                    log_message(
                        f"‚úÖ –í {name} –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –º–µ—Å—Ç–µ: {len(self.feature_columns)} —à—Ç"
                    )

        train_data = (
            pd.concat(train_data_batches, ignore_index=True)
            if train_data_batches
            else pd.DataFrame()
        )
        val_data = (
            pd.concat(val_data_batches, ignore_index=True)
            if val_data_batches
            else pd.DataFrame()
        )

        log_message_dist(train_data, "TRAIN")
        log_message_dist(val_data, "VAL")
        log_message(f"split_time={split_time} split_by_time={split_by_time}")
        log_message(
            f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: train={len(train_data)}, val={len(val_data)}"
        )

        return train_data, val_data

    def _get_copurchase_strength(self, item_id):
        """–ü–æ–ª—É—á–∞–µ–º —Å–∏–ª—É co-purchase —Å–≤—è–∑–∏"""
        if not self.copurchase_map or item_id not in self.copurchase_map:
            return 0.0

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∏–ª–∞ —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º
        strengths = [strength for _, strength in self.copurchase_map[item_id]]
        return max(strengths) if strengths else 0.0

    def _get_user_copurchase_affinity(self, user_id, item_id):
        """Affinity –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫ co-purchase —Å–≤—è–∑—è–º"""
        if not self.copurchase_map or not hasattr(self, "user_items_history"):
            return 0.0

        # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Ä–∞—Å—á–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—â—É—é —Å–∏–ª—É
        return self._get_copurchase_strength(item_id)

    def train(self, train_data, val_data=None, params=None):
        """
        –û–±—É—á–µ–Ω–∏–µ LightGBM —Å –±–∏–Ω–∞—Ä–Ω–æ–π —Ü–µ–ª—å—é, —Å –æ—Ü–µ–Ω–∫–æ–π NDCG@100 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
        """
        if params is None:
            params = {
                "objective": "binary",
                "metric": "binary_logloss",
                "learning_rate": 0.02,
                "num_leaves": 63,
                "max_depth": 10,
                "min_data_in_leaf": 50,
                "feature_fraction": 0.6,
                "bagging_fraction": 0.8,
                "bagging_freq": 1,
                "verbosity": 1,
                "force_row_wise": True,
                "device": "cpu",
                "num_threads": 8,
                "max_bin": 200,
                "boosting": "gbdt",
            }

        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        if not hasattr(self, "feature_columns") or not self.feature_columns:
            log_message("‚ùå –û–®–ò–ë–ö–ê: –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
            log_message(
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ user_features_dict –∏ item_features_dict –Ω–µ –ø—É—Å—Ç—ã–µ"
            )
            return None

        missing_features = [
            f for f in self.feature_columns if f not in train_data.columns
        ]
        if missing_features:
            log_message(
                f"‚ùå –û–®–ò–ë–ö–ê: –í train_data –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}"
            )
            log_message(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(train_data.columns)}")
            return None

        X_train = train_data[self.feature_columns]
        y_train = train_data["target"]

        log_message(f"–†–∞–∑–º–µ—Ä train: {len(X_train)}")
        log_message(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {list(X_train.columns)}")
        log_message(f"–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X_train.iloc[0].to_dict()}")

        # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        categorical_features = []
        for col in X_train.columns:
            if col in ["user_id", "item_id"] or col.startswith(("user_", "item_")):
                categorical_features.append(col)

        log_message(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {categorical_features}")

        train_dataset = lgb.Dataset(
            X_train,
            label=y_train,
            categorical_feature=categorical_features,
            feature_name=list(X_train.columns),
        )

        valid_sets = [train_dataset]
        valid_names = ["train"]

        if val_data is not None:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ validation
            missing_val_features = [
                f for f in self.feature_columns if f not in val_data.columns
            ]
            if missing_val_features:
                log_message(
                    f"‚ùå –û–®–ò–ë–ö–ê: –í val_data –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_val_features}"
                )
                return None

            X_val = val_data[self.feature_columns]
            y_val = val_data["target"]
            val_dataset = lgb.Dataset(
                X_val,
                label=y_val,
                categorical_feature=categorical_features,
                reference=train_dataset,
            )
            valid_sets.append(val_dataset)
            valid_names.append("valid")
            log_message(f"–†–∞–∑–º–µ—Ä val: {len(X_val)}")

        # Callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        def log_every_N_iter(env):
            if env.iteration % 10 == 0:
                metrics = ", ".join(
                    [
                        f"{name}_{metric}:{val:.4f}"
                        for name, metric, val, _ in env.evaluation_result_list
                    ]
                )
                log_message(f"[Iter {env.iteration}] {metrics}")

        try:
            self.model = lgb.train(
                params,
                train_dataset,
                num_boost_round=1000,
                valid_sets=valid_sets,
                valid_names=valid_names,
                callbacks=[lgb.early_stopping(50), log_every_N_iter],
            )

            # –í—ã—á–∏—Å–ª—è–µ–º NDCG@100 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if val_data is not None:
                ndcg_val = self.evaluate(val_data)
                log_message(f"NDCG@100 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {ndcg_val:.4f}")

            return self.model

        except Exception as e:
            log_message(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LightGBM: {e}")
            log_message("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –¥–∞–Ω–Ω—ã–µ")
            return None

    def evaluate(self, data, k=100):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ NDCG@k"""
        if self.model is None or len(data) == 0:
            return 0.0

        data = data.copy()
        data["score"] = self.model.predict(data[self.feature_columns])

        groups = data.groupby("user_id").size().values
        ndcg = ndcg_at_k_grouped(
            data["score"].values, data["target"].values, groups, k=k
        )
        return ndcg

    def recommend(self, user_items_data, top_k=100):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —Ç–æ–ø-K"""
        data = user_items_data.copy()
        data["score"] = self.model.predict(data[self.feature_columns])

        recommendations = {
            user_id: group.nlargest(top_k, "score")["item_id"].tolist()
            for user_id, group in data.groupby("user_id")
        }
        return recommendations

    def debug_data_info(self, data, name="data"):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö"""
        if data is None or data.empty:
            log_message(f"{name}: –ü—É—Å—Ç–æ")
            return

        log_message(f"=== –î–ï–ë–ê–ì –ò–ù–§–û–†–ú–ê–¶–ò–Ø: {name} ===")
        log_message(f"–†–∞–∑–º–µ—Ä: {len(data)} —Å—Ç—Ä–æ–∫")
        log_message(f"–ö–æ–ª–æ–Ω–∫–∏: {list(data.columns)}")

        if hasattr(self, "feature_columns"):
            missing = [f for f in self.feature_columns if f not in data.columns]
            if missing:
                log_message(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing}")
            else:
                log_message(f"‚úÖ –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        if "target" in data.columns:
            target_counts = data["target"].value_counts()
            log_message(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {dict(target_counts)}")

        # –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–æ–∫
        if len(data) > 0:
            log_message("–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏:")
            for col in data.columns:
                if col in data.columns:
                    log_message(f"  {col}: {data[col].iloc[0]}")


def build_user_features_dict(interactions_files, orders_df, device="cuda"):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Polars
    """
    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    # 1. –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –¢–†–ï–ö–ï–†–£ (–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è)
    user_stats_list = []
    for f in tqdm(interactions_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–µ–∫–µ—Ä–∞"):
        df = pl.read_parquet(f)

        chunk_stats = df.group_by("user_id").agg(
            [
                pl.col("weight").count().alias("count"),
                pl.col("weight").sum().alias("sum"),
                pl.col("weight").max().alias("max"),
                pl.col("weight").min().alias("min"),
            ]
        )
        user_stats_list.append(chunk_stats)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if user_stats_list:
        all_stats = pl.concat(user_stats_list)
        final_stats = all_stats.group_by("user_id").agg(
            [
                pl.col("count").sum().alias("user_count"),
                pl.col("sum").sum().alias("user_sum"),
                pl.col("max").max().alias("user_max"),
                pl.col("min").min().alias("user_min"),
            ]
        )
    else:
        final_stats = pl.DataFrame()

    # 2. –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –ó–ê–ö–ê–ó–ê–ú
    log_message("–ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∑–∞–∫–∞–∑–∞–º...")
    if isinstance(orders_df, pl.DataFrame):
        orders_pl = orders_df
    else:
        orders_pl = pl.from_pandas(
            orders_df.compute() if hasattr(orders_df, "compute") else orders_df
        )

    order_stats = orders_pl.group_by("user_id").agg(
        [
            pl.col("item_id").count().alias("user_orders_count"),
        ]
    )

    # 3. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –î–ê–ù–ù–´–•
    if len(final_stats) > 0 and len(order_stats) > 0:
        user_stats = final_stats.join(order_stats, on="user_id", how="full")
    elif len(final_stats) > 0:
        user_stats = final_stats
    else:
        user_stats = order_stats

    # 4. –í–´–ß–ò–°–õ–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
    current_time = pl.lit(datetime.now())

    user_stats = user_stats.with_columns(
        [
            pl.col("user_count").fill_null(0),
            pl.col("user_sum").fill_null(0),
            pl.col("user_orders_count").fill_null(0),
            (pl.col("user_sum") / pl.col("user_count")).alias("user_mean"),
        ]
    ).fill_nan(0)

    # 5. –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –°–õ–û–í–ê–†–¨
    user_stats_dict = {}
    for row in user_stats.iter_rows(named=True):
        user_stats_dict[row["user_id"]] = {
            "user_count": row["user_count"],
            "user_mean": row["user_mean"],
            "user_sum": row["user_sum"],
            "user_max": row["user_max"],
            "user_min": row["user_min"],
            "user_orders_count": row["user_orders_count"],
        }

    log_message(
        f"–°–ª–æ–≤–∞—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {save_path}. –ó–∞–ø–∏—Å–µ–π: {len(user_stats_dict)}"
    )
    return user_stats_dict


def load_ui_features_for_user_item(user_id, item_id, ui_features_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç UI-–ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–∞—Ä—ã user-item
    """
    if not ui_features_path or not os.path.exists(ui_features_path):
        return None

    query = pl.scan_parquet(ui_features_path).filter(
        (pl.col("user_id") == user_id) & (pl.col("item_id") == item_id)
    )
    result = query.collect()

    if len(result) == 0:
        return None

    return result[0].to_dict()


def build_item_features_dict(
    interactions_files, items_df, orders_df, embeddings_dict, device="cuda"
):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Polars
    """
    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–≤–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    # 1. –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –¢–†–ï–ö–ï–†–£ –ò –ó–ê–ö–ê–ó–ê–ú
    item_stats_list = []
    for f in tqdm(interactions_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π"):
        df = pl.read_parquet(f)

        chunk_stats = df.group_by("item_id").agg(
            [
                pl.col("weight").count().alias("count"),
                pl.col("weight").sum().alias("sum"),
                pl.col("weight").max().alias("max"),
                pl.col("weight").min().alias("min"),
            ]
        )
        item_stats_list.append(chunk_stats)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if item_stats_list:
        all_stats = pl.concat(item_stats_list)
        final_stats = all_stats.group_by("item_id").agg(
            [
                pl.col("count").sum().alias("item_count"),
                pl.col("sum").sum().alias("item_sum"),
                pl.col("max").max().alias("item_max"),
                pl.col("min").min().alias("item_min"),
            ]
        )
    else:
        final_stats = pl.DataFrame()

    # 2. –î–û–ë–ê–í–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• –ò–ó –ó–ê–ö–ê–ó–û–í
    if isinstance(orders_df, pl.DataFrame):
        orders_pl = orders_df
    else:
        orders_pl = pl.from_pandas(
            orders_df.compute() if hasattr(orders_df, "compute") else orders_df
        )

    order_stats = orders_pl.group_by("item_id").agg(
        [pl.col("user_id").count().alias("item_orders_count")]
    )

    # 3. –î–û–ë–ê–í–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• –ò–ó items_df
    log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ items_df...")
    if isinstance(items_df, pl.DataFrame):
        items_pl = items_df
    else:
        items_pl = pl.from_pandas(
            items_df.compute() if hasattr(items_df, "compute") else items_df
        )

    items_catalog = items_pl.select(["item_id", "catalogid"]).unique()

    # 4. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í–°–ï–• –î–ê–ù–ù–´–•
    item_stats = final_stats.join(order_stats, on="item_id", how="full")
    item_stats = item_stats.join(items_catalog, on="item_id", how="left")

    # 5. –í–´–ß–ò–°–õ–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
    current_time = pl.lit(datetime.now())

    item_stats = item_stats.with_columns(
        [
            pl.col("item_count").fill_null(0),
            pl.col("item_sum").fill_null(0),
            pl.col("item_orders_count").fill_null(0),
            (pl.col("item_sum") / pl.col("item_count")).alias("item_mean"),
        ]
    ).fill_nan(0)

    # 6. –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –°–õ–û–í–ê–†–¨
    item_stats_dict = {}
    for row in item_stats.iter_rows(named=True):
        item_stats_dict[row["item_id"]] = {
            "item_count": row["item_count"],
            "item_mean": row["item_mean"],
            "item_sum": row["item_sum"],
            "item_max": row["item_max"],
            "item_min": row["item_min"],
            "item_orders_count": row["item_orders_count"],
            "item_category": row["catalogid"],
        }

    # 7. –î–û–ë–ê–í–õ–ï–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
    log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    for item_id, embedding in embeddings_dict.items():
        if item_id in item_stats_dict:
            for i in range(min(5, len(embedding))):
                item_stats_dict[item_id][f"fclip_embed_{i}"] = float(embedding[i])

    log_message(f"–°–ª–æ–≤–∞—Ä—å —Ç–æ–≤–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω. –ó–∞–ø–∏—Å–µ–π: {len(item_stats_dict)}")
    return item_stats_dict


def build_category_features_dict(category_df, items_df):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Polars
    """
    import polars as pl

    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    if not isinstance(category_df, pl.DataFrame):
        category_pl = pl.from_pandas(
            category_df.compute() if hasattr(category_df, "compute") else category_df
        )
    else:
        category_pl = category_df

    if not isinstance(items_df, pl.DataFrame):
        items_pl = pl.from_pandas(
            items_df.compute() if hasattr(items_df, "compute") else items_df
        )
    else:
        items_pl = items_df

    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏—è -> —É—Ä–æ–≤–µ–Ω—å –≤ –∏–µ—Ä–∞—Ä—Ö–∏–∏
    cat_levels = category_pl.with_columns(
        [(pl.col("ids").list.lengths() - 1).alias("category_level")]
    ).select(["catalogid", "category_level"])

    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ —Ç–æ–≤–∞—Ä -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    item_categories = items_pl.select(["item_id", "catalogid"]).unique()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º
    category_features = item_categories.join(cat_levels, on="catalogid", how="left")
    category_features = category_features.with_columns(
        [pl.col("category_level").fill_null(0)]
    )

    # –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –°–õ–û–í–ê–†–¨
    category_features_dict = {}
    for row in category_features.iter_rows(named=True):
        category_features_dict[row["item_id"]] = {
            "item_category": row["catalogid"],
            "category_level": row["category_level"],
        }

    log_message(
        f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã. –ó–∞–ø–∏—Å–µ–π: {len(category_features_dict)}"
    )
    return category_features_dict


def prepare_lgbm_training_data(
    user_features_dict,
    item_features_dict,
    user_item_features_dict,
    category_features_dict,
    test_orders_df,
    all_items,  # —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö item_id
    sample_fraction=0.1,
):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LightGBM:
    1 –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä -> 1 –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–∏–º–µ—Ä.
    """
    log_message("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LightGBM...")

    # –ë–µ—Ä–µ–º sample –æ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–∫–∞–∑–æ–≤
    test_sample = test_orders_df.sample(frac=sample_fraction, random_state=42)

    train_examples = []

    for _, row in test_sample.iterrows():
        user_id = row["user_id"]
        pos_item_id = row["item_id"]

        # ---------- –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π ----------
        features = {}
        if user_id in user_features_dict:
            features.update(user_features_dict[user_id])
        if pos_item_id in item_features_dict:
            features.update(item_features_dict[pos_item_id])
        if (user_id, pos_item_id) in user_item_features_dict:
            features.update(user_item_features_dict[(user_id, pos_item_id)])
        if pos_item_id in category_features_dict:
            features.update(category_features_dict[pos_item_id])

        features["target"] = 1
        features["user_id"] = user_id
        features["item_id"] = pos_item_id
        train_examples.append(features)

        # ---------- –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π ----------
        neg_item_id = random.choice(all_items)
        while neg_item_id == pos_item_id:
            neg_item_id = random.choice(all_items)

        neg_features = {}
        if user_id in user_features_dict:
            neg_features.update(user_features_dict[user_id])
        if neg_item_id in item_features_dict:
            neg_features.update(item_features_dict[neg_item_id])
        if (user_id, neg_item_id) in user_item_features_dict:
            neg_features.update(user_item_features_dict[(user_id, neg_item_id)])
        if neg_item_id in category_features_dict:
            neg_features.update(category_features_dict[neg_item_id])

        neg_features["target"] = 0
        neg_features["user_id"] = user_id
        neg_features["item_id"] = neg_item_id
        train_examples.append(neg_features)

    # –°–æ–∑–¥–∞–µ–º DataFrame
    train_df = pd.DataFrame(train_examples)

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns
    train_df[numeric_cols] = train_df[numeric_cols].fillna(0)

    log_message(
        f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã. –†–∞–∑–º–µ—Ä: {len(train_df)} (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {len(test_sample)}, –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {len(test_sample)})"
    )
    return train_df


def load_and_process_embeddings(
    items_ddf, embedding_column="fclip_embed", device="cuda", max_items=0
):
    """
    –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å item_id -> np.array
    """
    log_message("–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

    if max_items > 0:
        items_sample = items_ddf[["item_id", embedding_column]].head(
            max_items, compute=True
        )
    else:
        items_sample = items_ddf[["item_id", embedding_column]].compute()

    embeddings_dict = {}
    for row in tqdm(
        items_sample.itertuples(index=False),
        total=len(items_sample),
        desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
    ):
        item_id = row.item_id
        embedding_data = getattr(row, embedding_column, None)
        if embedding_data is None:
            continue
        try:
            if isinstance(embedding_data, str):
                embedding = np.fromstring(
                    embedding_data.strip("[]"), sep=",", dtype=np.float32
                )
            elif isinstance(embedding_data, list):
                embedding = np.array(embedding_data, dtype=np.float32)
            elif isinstance(embedding_data, np.ndarray):
                embedding = embedding_data.astype(np.float32)
            else:
                continue
            if embedding.size > 0:
                embeddings_dict[item_id] = embedding
        except Exception:
            continue

    log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(embeddings_dict)} —Ç–æ–≤–∞—Ä–æ–≤")
    return embeddings_dict


def load_streaming_data(path_pattern: str) -> pd.DataFrame:
    files = sorted(glob(path_pattern))
    if not files:
        raise FileNotFoundError(f"–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω—É: {path_pattern}")
    dfs = [pd.read_parquet(f) for f in files]
    return pd.concat(dfs, ignore_index=True)


def normalize_item_features_dict(item_features_dict, embed_prefix="fclip_embed_"):
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞—Ä—å item_features_dict –∫ —Ñ–æ—Ä–º–∞—Ç—É {item_id: np.array([...])},
    –≥–¥–µ –≤–µ–∫—Ç–æ—Ä = [–æ–±—ã—á–Ω—ã–µ —Ñ–∏—á–∏ + —ç–º–±–µ–¥–¥–∏–Ω–≥–∏].
    """
    normalized = {}

    # –û–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ—Ä—è–¥–æ–∫ –∫–ª—é—á–µ–π (—Ñ–∏–∫—Å–∏—Ä—É–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
    example_dict = next(iter(item_features_dict.values()))
    base_keys = [k for k in example_dict.keys() if not k.startswith(embed_prefix)]
    embed_keys = sorted(
        [k for k in example_dict.keys() if k.startswith(embed_prefix)],
        key=lambda x: int(x.replace(embed_prefix, "")),
    )

    feature_order = base_keys + embed_keys

    for item_id, feats in item_features_dict.items():
        row = []
        for key in feature_order:
            val = feats.get(key, 0.0)  # –µ—Å–ª–∏ –∫–ª—é—á–∞ –Ω–µ—Ç ‚Äî —Å—Ç–∞–≤–∏–º 0
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –º–æ–∂–Ω–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å, –ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å
            if val is None:
                val = 0.0
            row.append(val)
        normalized[item_id] = np.array(row, dtype=np.float32)

    return normalized, feature_order


def normalize_item_feats(item_feats, max_emb_dim=30):
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Ñ–∏—á–∏ –∞–π—Ç–µ–º–∞ –∫ —Å–ª–æ–≤–∞—Ä—é (dict).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç dict –∏ np.ndarray.
    """
    norm_feats = {}

    if isinstance(item_feats, dict):
        for k, v in item_feats.items():
            if isinstance(v, (int, float, np.number)):
                norm_feats[k] = float(v)
    elif isinstance(item_feats, np.ndarray):
        for i, v in enumerate(item_feats):
            if i < max_emb_dim:
                norm_feats[f"emb_{i}"] = float(v)

    return norm_feats


# -------------------- –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ --------------------
if __name__ == "__main__":
    start_time = time.time()

    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_file = "/home/root6/python/e_cup/rec_system/training_log.txt"

    def log_message(message):
        """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–∞–π–ª –∏ –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        full_message = f"[{timestamp}] {message}"
        print(full_message)
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(full_message + "\n")

    # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –ª–æ–≥–∞ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ
    with open(log_file, "w", encoding="utf-8") as f:
        f.write(
            f"=== –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n\n"
        )

    try:
        K = 100
        RECENT_N = 5
        TEST_SIZE = 0.2

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        SCALING_STAGE = "full"  # small, medium, large, full

        scaling_config = {
            "small": {"sample_users": 500, "sample_fraction": 0.1},
            "medium": {"sample_users": 5000, "sample_fraction": 0.3},
            "large": {"sample_users": 20000, "sample_fraction": 0.7},
            "full": {"sample_users": None, "sample_fraction": 1.0},
        }

        config = scaling_config[SCALING_STAGE]

        log_message(f"=== –†–ï–ñ–ò–ú –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–Ø: {SCALING_STAGE.upper()} ===")
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {config['sample_users'] or '–≤—Å–µ'}")
        log_message(f"–î–∞–Ω–Ω—ã—Ö: {config['sample_fraction']*100}%")

        # === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===")
        orders_ddf, tracker_ddf, items_ddf, categories_ddf, test_users_ddf = (
            load_train_data()
        )
        orders_ddf, tracker_ddf, items_ddf = filter_data(
            orders_ddf, tracker_ddf, items_ddf
        )
        stage_time = time.time() - stage_start
        log_message(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

        # === –ó–ê–ì–†–£–ó–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ===
        stage_start = time.time()
        log_message("=== –ó–ê–ì–†–£–ó–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ===")
        embeddings_dict = load_and_process_embeddings(items_ddf)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/embeddings_dict.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(embeddings_dict, f)
        log_message(
            f"–ü—Ä–∏–º–µ—Ä –∫–ª—é—á–µ–π –≤ embeddings_dict: {list(embeddings_dict.keys())[:5]}"
        )
        log_message(f"–ü—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏—è: {next(iter(embeddings_dict.values()))[:5]}")
        stage_time = time.time() - stage_start
        log_message(
            f"–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}"
        )
        log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings_dict)}")

        # === SPLIT –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== SPLIT –î–ê–ù–ù–´–• ===")
        orders_df_full = orders_ddf.compute()
        train_orders_df, test_orders_df, cutoff_ts_per_user = train_test_split_by_time(
            orders_df_full, TEST_SIZE
        )
        stage_time = time.time() - stage_start
        log_message(f"Split –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {timedelta(seconds=stage_time)}")
        log_message(
            f"Train orders: {len(train_orders_df)}, Test orders: {len(test_orders_df)}"
        )

        # === –ü–û–î–ì–û–¢–û–í–ö–ê –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–ô ===
        stage_start = time.time()
        log_message("=== –ü–û–î–ì–û–¢–û–í–ö–ê –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–ô ===")
        interactions_files = prepare_interactions(
            train_orders_df, tracker_ddf, cutoff_ts_per_user, scale_days=30
        )
        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}"
        )
        log_message(f"–°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {len(interactions_files)}")

        # === –ü–û–°–õ–ï–î–ù–ò–ï –¢–û–í–ê–†–´ ===
        stage_start = time.time()
        log_message("=== –ü–û–°–õ–ï–î–ù–ò–ï –¢–û–í–ê–†–´ ===")
        batch_dir = "/home/root6/python/e_cup/rec_system/data/processed/prepare_interactions_batches"
        recent_items_map = build_recent_items_map_from_batches(
            batch_dir, recent_n=RECENT_N
        )
        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ recent items map –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}"
        )
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å recent items: {len(recent_items_map)}")

        # === –û–ë–£–ß–ï–ù–ò–ï ALS –î–õ–Ø –ü–†–ò–ó–ù–ê–ö–û–í ===
        stage_start = time.time()
        log_message("=== –û–ë–£–ß–ï–ù–ò–ï ALS –î–õ–Ø –ü–†–ò–ó–ù–ê–ö–û–í ===")
        model, user_map, item_map = train_als(
            interactions_files, n_factors=64, reg=1e-3, device="cuda"
        )
        inv_item_map = {v: k for k, v in item_map.items()}
        popularity_s = compute_global_popularity(
            orders_df_full, cutoff_ts_per_user
        )  # —Ç–µ–ø–µ—Ä—å —ç—Ç–æ pd.Timestamp
        popular_items = popularity_s.index.tolist()
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/popular_items.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(popular_items, f)
        stage_time = time.time() - stage_start
        log_message(f"–û–±—É—á–µ–Ω–∏–µ ALS –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}")
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_map)}, –¢–æ–≤–∞—Ä–æ–≤: {len(item_map)}")

        # === –ü–û–°–¢–†–û–ï–ù–ò–ï –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== –ü–û–°–¢–†–û–ï–ù–ò–ï –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –î–ê–ù–ù–´–• ===")

        # –°—Ç—Ä–æ–∏–º co-purchase map
        copurchase_map = build_copurchase_map(train_orders_df)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/copurchase_map.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(copurchase_map, f)
        log_message(f"Co-purchase map –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(copurchase_map)} —Ç–æ–≤–∞—Ä–æ–≤")

        # –°—Ç—Ä–æ–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
        items_df = items_ddf.compute()
        categories_df = categories_ddf.compute()
        item_to_cat, cat_to_items = build_category_maps(items_df, categories_df)
        save_path = "/home/root6/python/e_cup/rec_system/data/processed/item_to_cat.pkl"
        with open(save_path, "wb") as f:
            pickle.dump(item_to_cat, f)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/cat_to_items.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(cat_to_items, f)
        log_message(
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã: {len(item_to_cat)} —Ç–æ–≤–∞—Ä–æ–≤, {len(cat_to_items)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π"
        )

        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø LGBM ===
        stage_start = time.time()
        log_message("=== –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø LGBM ===")

        # User features
        user_start = time.time()
        user_features_dict = build_user_features_dict(interactions_files, orders_ddf)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/user_features_dict.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(user_features_dict, f)
        user_time = time.time() - user_start
        log_message(
            f"User features –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∑–∞ {timedelta(seconds=user_time)}: {len(user_features_dict)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"
        )

        # Item features
        item_start = time.time()
        raw_item_features_dict = build_item_features_dict(
            interactions_files, items_df, orders_ddf, embeddings_dict
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –≤ np.array
        item_features_dict, feature_order = normalize_item_features_dict(
            raw_item_features_dict
        )

        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä
        log_message(
            f"Item features: {len(item_features_dict)} —Ç–æ–≤–∞—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞: {len(feature_order)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
        )
        log_message(
            f"–ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {feature_order[:10]}{'...' if len(feature_order)>10 else ''}"
        )

        sample_items = list(item_features_dict.items())[:5]
        for k, v in sample_items:
            log_message(
                f"item_features_dict[{k}] type={type(v)}, shape={v.shape}, example={v[:5]}"
            )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/item_features_dict.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(item_features_dict, f)

        # –û—Ç–¥–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤–∞–∂–Ω–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞)
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/item_feature_order.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(feature_order, f)

        item_time = time.time() - item_start
        log_message(f"Item features –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∑–∞ {timedelta(seconds=item_time)}")

        # === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø LightGBM ===
        stage_start = time.time()
        log_message("=== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø LightGBM ===")
        recommender = LightGBMRecommender()
        recommender.set_als_embeddings(model)
        recommender.set_additional_data(
            copurchase_map, item_to_cat, cat_to_items, user_map, item_map
        )

        if embeddings_dict:
            recommender.set_external_embeddings(embeddings_dict)

        # –ú–ê–°–®–¢–ê–ë–ò–†–£–ï–ú –¥–∞–Ω–Ω—ã–µ
        if config["sample_users"]:
            sample_test_orders = test_orders_df.sample(
                min(config["sample_users"], len(test_orders_df)), random_state=42
            )
        else:
            sample_test_orders = test_orders_df

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å UI-–ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        train_data, val_data = recommender.prepare_training_data(
            interactions_files=interactions_files,
            orders_ddf=orders_ddf,
            user_map=user_map,
            item_map=item_map,
            popularity_s=popularity_s,
            recent_items_map=recent_items_map,
            copurchase_map=copurchase_map,
            item_to_cat=item_to_cat,
            cat_to_items=cat_to_items,
            user_features_dict=user_features_dict,
            item_features_dict=item_features_dict,
            embeddings_dict=embeddings_dict,
            sample_fraction=config["sample_fraction"],
            negatives_per_positive=3,
            split=0.2,
        )

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation
        users = train_data["user_id"].unique()
        train_users, val_users = train_test_split(users, test_size=0.2, random_state=42)

        train_df = train_data[train_data["user_id"].isin(train_users)]
        val_df = train_data[train_data["user_id"].isin(val_users)]

        log_message(f"–†–∞–∑–º–µ—Ä train: {len(train_df)}, validation: {len(val_df)}")
        log_message(f"–ü—Ä–∏–∑–Ω–∞–∫–∏: {len(recommender.feature_columns)}")
        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LightGBM –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í ===
        stage_start = time.time()
        log_message("=== –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê FEATURE GENERATION ===")

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ user features
        log_message("--- –ü–†–û–í–ï–†–ö–ê USER FEATURES ---")
        if user_features_dict:
            sample_user = list(user_features_dict.keys())[0]
            user_feats = user_features_dict[sample_user]
            log_message(f"–ü—Ä–∏–º–µ—Ä user features –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {sample_user}:")
            for feat, value in user_feats.items():
                log_message(f"  {feat}: {value}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ user features
            users_with_features = len(user_features_dict)
            users_with_real_features = sum(
                1
                for feats in user_features_dict.values()
                if any(v != 0 for v in feats.values())
            )
            log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å features: {users_with_features}")
            log_message(
                f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –ù–ï–Ω—É–ª–µ–≤—ã–º–∏ features: {users_with_real_features}"
            )
        else:
            log_message("‚ö†Ô∏è user_features_dict –ü–£–°–¢–û–ô!")

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ item features
        log_message("--- –ü–†–û–í–ï–†–ö–ê ITEM FEATURES ---")
        if item_features_dict:
            sample_item = list(item_features_dict.keys())[0]
            raw_item_feats = item_features_dict[sample_item]

            # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º (–ø—Ä–µ–≤—Ä–∞—â–∞–µ–º np.ndarray ‚Üí dict)
            item_feats = normalize_item_feats(raw_item_feats, max_emb_dim=30)

            log_message(f"–ü—Ä–∏–º–µ—Ä item features –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {sample_item}:")
            for feat, value in item_feats.items():
                log_message(f"  {feat}: {value}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ item features
            items_with_features = len(item_features_dict)
            items_with_real_features = 0
            for feats in item_features_dict.values():
                norm_feats = normalize_item_feats(feats, max_emb_dim=30)
                if any(v != 0 for v in norm_feats.values()):
                    items_with_real_features += 1

            log_message(f"–¢–æ–≤–∞—Ä–æ–≤ —Å features: {items_with_features}")
            log_message(f"–¢–æ–≤–∞—Ä–æ–≤ —Å –ù–ï–Ω—É–ª–µ–≤—ã–º–∏ features: {items_with_real_features}")
        else:
            log_message("‚ö†Ô∏è item_features_dict –ü–£–°–¢–û–ô!")

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        log_message("--- –ü–†–û–í–ï–†–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ---")
        if embeddings_dict:
            sample_item = list(embeddings_dict.keys())[0]
            embedding = embeddings_dict[sample_item]
            log_message(
                f"–ü—Ä–∏–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {sample_item}: shape {embedding.shape}"
            )
            log_message(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(embeddings_dict)}")
            log_message(f"–ü—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏–π: {embedding[:5]}")
        else:
            log_message("‚ö†Ô∏è embeddings_dict –ü–£–°–¢–û–ô!")

        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ co-purchase map
        log_message("--- –ü–†–û–í–ï–†–ö–ê CO-PURCHASE MAP ---")
        if copurchase_map:
            sample_item = list(copurchase_map.keys())[0]
            co_items = copurchase_map[sample_item]
            log_message(
                f"–ü—Ä–∏–º–µ—Ä co-purchase –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {sample_item}: {len(co_items)} —Ç–æ–≤–∞—Ä–æ–≤"
            )
            log_message(f"Co-purchase –∑–∞–ø–∏—Å–µ–π: {len(copurchase_map)}")
        else:
            log_message("‚ö†Ô∏è copurchase_map –ü–£–°–¢–û–ô!")

        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤
        log_message("--- –ü–†–û–í–ï–†–ö–ê –ö–ê–¢–ï–ì–û–†–ò–ô–ù–´–• –ú–ê–ü–ü–ò–ù–ì–û–í ---")
        if item_to_cat and cat_to_items:
            sample_item = list(item_to_cat.keys())[0]
            cat_id = item_to_cat[sample_item]
            cat_items = cat_to_items.get(cat_id, [])
            log_message(f"–¢–æ–≤–∞—Ä {sample_item} -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è {cat_id}")
            log_message(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {cat_id} -> {len(cat_items)} —Ç–æ–≤–∞—Ä–æ–≤")
            log_message(f"–¢–æ–≤–∞—Ä–æ–≤ –≤ –º–∞–ø–ø–∏–Ω–≥–µ: {len(item_to_cat)}")
            log_message(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –º–∞–ø–ø–∏–Ω–≥–µ: {len(cat_to_items)}")
        else:
            log_message("‚ö†Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ –ü–£–°–¢–´–ï!")

        stage_time = time.time() - stage_start
        log_message(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

        # === –û–ë–£–ß–ï–ù–ò–ï LightGBM ===
        stage_start = time.time()
        log_message("=== –û–ë–£–ß–ï–ù–ò–ï LightGBM ===")

        train_df = load_streaming_data(
            "rec_system/data/processed/train_streaming/*.parquet"
        )
        val_df = load_streaming_data(
            "rec_system/data/processed/val_streaming/*.parquet"
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
        recommender.debug_data_info(train_df, "TRAIN")
        recommender.debug_data_info(val_df, "VAL")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        if hasattr(recommender, "feature_columns"):
            log_message(f"Feature columns: {recommender.feature_columns}")
        else:
            log_message("‚ùå Feature columns –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã!")

        if (
            not train_data.empty
            and hasattr(recommender, "feature_columns")
            and recommender.feature_columns
        ):
            model = recommender.train(train_df, val_df)
        else:
            log_message("‚ùå –ù–µ–ª—å–∑—è –æ–±—É—á–∞—Ç—å: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # model = recommender.train(train_df, val_df)

        stage_time = time.time() - stage_start
        log_message(f"–û–±—É—á–µ–Ω–∏–µ LightGBM –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}")

        # === –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò ===
        stage_start = time.time()
        log_message("=== –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò ===")
        train_ndcg = recommender.evaluate(train_df)
        val_ndcg = recommender.evaluate(val_df)

        log_message(f"NDCG@100 train: {train_ndcg:.4f}")
        log_message(f"NDCG@100 val: {val_ndcg:.4f}")
        stage_time = time.time() - stage_start
        log_message(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        stage_start = time.time()
        log_message("=== –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í ===")
        feature_importance = pd.DataFrame(
            {
                "feature": recommender.feature_columns,
                "importance": recommender.model.feature_importance(),
            }
        )
        feature_importance = feature_importance.sort_values(
            "importance", ascending=False
        )
        top_features = feature_importance.head(20)
        log_message("–¢–æ–ø-20 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for i, row in top_features.iterrows():
            log_message(f"  {row['feature']}: {row['importance']}")
        stage_time = time.time() - stage_start
        log_message(
            f"–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –í–ê–ñ–ù–´–• –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –ü–†–ò–ó–ù–ê–ö–û–í ===")
        save_data = {
            "lgbm_model": recommender.model,
            "feature_columns": recommender.feature_columns,
            "als_model": model,
            "user_map": user_map,
            "item_map": item_map,
            "inv_item_map": inv_item_map,
            "popular_items": popular_items,
            "user_features_dict": user_features_dict,
            "item_features_dict": item_features_dict,
            "recent_items_map": recent_items_map,
            "copurchase_map": copurchase_map,
            "item_to_cat": item_to_cat,
        }

        model_path = (
            "/home/root6/python/e_cup/rec_system/src/models/lgbm_model_full.pkl"
        )
        with open(model_path, "wb") as f:
            pickle.dump(save_data, f, protocol=pickle.HIGHEST_PROTOCOL)

        stage_time = time.time() - stage_start
        log_message(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}")
        log_message(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_path}")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        all_items = set()

        # === –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===
        total_time = time.time() - start_time
        log_message("=== –û–ë–£–ß–ï–ù–ò–ï –ò –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ó–ê–í–ï–†–®–ï–ù–´ –£–°–ü–ï–®–ù–û ===")
        log_message(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {timedelta(seconds=total_time)}")
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_map)}")
        log_message(f"–¢–æ–≤–∞—Ä–æ–≤: {len(item_map)}")
        log_message(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(recommender.feature_columns)}")
        log_message(f"NDCG@100 train: {train_ndcg:.4f}")
        log_message(f"NDCG@100 val: {val_ndcg:.4f}")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
        log_message("=== –°–ò–°–¢–ï–ú–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø ===")
        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
            import torch

            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                log_message(f"–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: {gpu_name}")
                log_message(f"–ü–∞–º—è—Ç—å GPU: {gpu_memory:.1f} GB")
            else:
                log_message("–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
        except Exception:
            log_message("–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CPU
            import multiprocessing

            import psutil

            cpu_freq = psutil.cpu_freq()
            cpu_cores = multiprocessing.cpu_count()
            log_message(f"–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä: {psutil.cpu_percent()}% –∑–∞–≥—Ä—É–∑–∫–∏")
            log_message(f"–Ø–¥—Ä–∞ CPU: {cpu_cores}")
            if cpu_freq:
                log_message(f"–ß–∞—Å—Ç–æ—Ç–∞ CPU: {cpu_freq.current:.1f} MHz")
        except Exception:
            log_message("–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ RAM
            import psutil

            ram = psutil.virtual_memory()
            ram_total = ram.total / 1024**3
            ram_used = ram.used / 1024**3
            log_message(
                f"–û–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å: {ram_total:.1f} GB –≤—Å–µ–≥–æ, {ram_used:.1f} GB –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ"
            )
            log_message(f"–ß–∞—Å—Ç–æ—Ç–∞ RAM: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫")
        except Exception:
            log_message("–û–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        log_message("==========================================")
        log_message("–í–°–ï –≠–¢–ê–ü–´ –í–´–ü–û–õ–ù–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        log_message("==========================================")

    except Exception as e:
        error_time = time.time() - start_time
        log_message(f"!!! –û–®–ò–ë–ö–ê –í–´–ü–û–õ–ù–ï–ù–ò–Ø !!!")
        log_message(f"–û—à–∏–±–∫–∞: {str(e)}")
        log_message(f"–í—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {timedelta(seconds=error_time)}")
        log_message("–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
        import traceback

        traceback_str = traceback.format_exc()
        log_message(traceback_str)

    finally:
        # –í—Å–µ–≥–¥–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –≤—Ä–µ–º—è
        total_time = time.time() - start_time
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(
                f"\n=== –û–ë–©–ï–ï –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø: {timedelta(seconds=total_time)} ===\n"
            )
