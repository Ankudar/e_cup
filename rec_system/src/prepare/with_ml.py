import gc
import json
import os
import pickle
import random
import shutil
import tempfile
import time
import traceback
import uuid
import warnings
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from glob import glob
from pathlib import Path

import dask.dataframe as dd
import lightgbm as lgb
import numpy as np
import pandas as pd
import polars as pl
import psutil
import pyarrow as pa
import pyarrow.parquet as pq
import torch
import torch.nn.functional as F
import torch.sparse
from dask.diagnostics import ProgressBar
from implicit.als import AlternatingLeastSquares
from scipy.sparse import coo_matrix, csr_matrix, vstack
from sklearn.metrics import ndcg_score
from sklearn.model_selection import train_test_split
from torch import nn
from tqdm import tqdm
from tqdm.auto import tqdm

warnings.filterwarnings(
    "ignore", category=FutureWarning, message=".*Downcasting object dtype arrays.*"
)
# tqdm –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å pandas
tqdm.pandas()


# -------------------- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö --------------------
def load_train_data(max_parts=0, max_rows=1000):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º parquet-—Ñ–∞–π–ª—ã orders, tracker, items, categories_tree, test_users.
    –ò—â–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ –ø–∞–ø–∫–∞–º –≤—Å–µ .parquet —Ñ–∞–π–ª—ã. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫.
    """

    paths = {
        "orders": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_orders_data/",
        "tracker": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_tracker_data/",
        "items": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_apparel_items_data/",
        "categories": "/home/root6/python/e_cup/rec_system/data/raw/ml_ozon_recsys_train_final_categories_tree/",
        "test_users": "/home/root6/python/e_cup/rec_system/data/raw/test_users/",
    }

    columns_map = {
        "orders": ["item_id", "user_id", "created_timestamp", "last_status"],
        "tracker": ["item_id", "user_id", "timestamp", "action_type"],
        "items": ["item_id", "itemname", "fclip_embed", "catalogid"],
        "categories": ["catalogid", "catalogpath", "ids"],
        "test_users": ["user_id"],
    }

    dtype_profiles = {
        "orders": {
            "user_id": "int32",
            "item_id": "int32",
            "created_timestamp": "datetime64[ns]",
            "last_status": "category",
        },
        "tracker": {
            "user_id": "int32",
            "item_id": "int32",
            "timestamp": "datetime64[ns]",
            "action_type": "category",
        },
        "items": {
            "item_id": "int32",
            "catalogid": "int32",
            "itemname": "string",
            "fclip_embed": "object",
        },
        "categories": {
            "catalogid": "int32",
            "catalogpath": "string",
            "ids": "string",
        },
        "test_users": {"user_id": "int32"},
    }

    def find_parquet_files(folder):
        files = glob(os.path.join(folder, "**", "*.parquet"), recursive=True)
        files.sort()
        return files

    def read_sample(
        folder, columns=None, name="", max_parts=max_parts, max_rows=max_rows
    ):
        files = find_parquet_files(folder)
        if not files:
            log_message(f"{name}: parquet —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {folder}")
            return dd.from_pandas(pd.DataFrame(), npartitions=1)

        current_dtypes = dtype_profiles.get(name, {})

        try:
            # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
            ddf = dd.read_parquet(
                files,
                engine="pyarrow",
                dtype=current_dtypes,
                gather_statistics=False,
                split_row_groups=True,
            )
        except Exception as e:
            log_message(f"{name}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ parquet ({e}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return dd.from_pandas(pd.DataFrame(), npartitions=1)

        if columns is not None:
            available_cols = [c for c in columns if c in ddf.columns]
            if not available_cols:
                log_message(
                    f"{name}: –Ω–∏ –æ–¥–Ω–∞ –∏–∑ –∫–æ–ª–æ–Ω–æ–∫ {columns} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º"
                )
                return dd.from_pandas(pd.DataFrame(), npartitions=1)
            ddf = ddf[available_cols]

        total_parts = ddf.npartitions

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if max_parts > 0 and max_parts < total_parts:
            ddf = ddf.partitions[:max_parts]
            used_parts = max_parts
        else:
            used_parts = total_parts

        # –ï—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if max_rows == 0:
            count = ddf.shape[0].compute()
            mem_estimate = ddf.memory_usage(deep=True).sum().compute() / (1024**2)
            log_message(
                f"{name}: {count:,} —Å—Ç—Ä–æ–∫ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_parts} –∏–∑ {total_parts} –ø–∞—Ä—Ç–∏—Ü–∏–π), ~{mem_estimate:.1f} MB"
            )
            return ddf

        # –ë—ã—Å—Ç—Ä—ã–π —Å–ø–æ—Å–æ–± –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ max_rows
        # –î–ª—è —ç—Ç–æ–≥–æ —Å–Ω–∞—á–∞–ª–∞ –≤—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
        total_rows = ddf.shape[0].compute()

        if total_rows <= max_rows:
            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫ –º–µ–Ω—å—à–µ –ª–∏–º–∏—Ç–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å—ë
            count = total_rows
            mem_estimate = ddf.memory_usage(deep=True).sum().compute() / (1024**2)
            log_message(
                f"{name}: {count:,} —Å—Ç—Ä–æ–∫ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_parts} –∏–∑ {total_parts} –ø–∞—Ä—Ç–∏—Ü–∏–π), ~{mem_estimate:.1f} MB"
            )
            return ddf
        else:
            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫ –±–æ–ª—å—à–µ - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π ddf —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
            # –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º head —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º
            limited_ddf = ddf.head(max_rows, compute=False)
            count = max_rows
            mem_estimate = limited_ddf.memory_usage(deep=True).sum().compute() / (
                1024**2
            )
            log_message(
                f"{name}: {count:,} —Å—Ç—Ä–æ–∫ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_parts} –∏–∑ {total_parts} –ø–∞—Ä—Ç–∏—Ü–∏–π), ~{mem_estimate:.1f} MB"
            )
            return limited_ddf

    log_message("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    orders_ddf = read_sample(
        paths["orders"], columns=columns_map["orders"], name="orders"
    )
    tracker_ddf = read_sample(
        paths["tracker"], columns=columns_map["tracker"], name="tracker"
    )
    items_ddf = read_sample(paths["items"], columns=columns_map["items"], name="items")
    categories_ddf = read_sample(
        paths["categories"], columns=columns_map["categories"], name="categories"
    )
    test_users_ddf = read_sample(
        paths["test_users"], columns=columns_map["test_users"], name="test_users"
    )
    log_message("–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

    return orders_ddf, tracker_ddf, items_ddf, categories_ddf, test_users_ddf


# -------------------- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö --------------------
def filter_data(orders_ddf, tracker_ddf, items_ddf):
    """
    –§–∏–ª—å—Ç—Ä—É–µ–º: –æ—Å—Ç–∞–≤–ª—è–µ–º delivered_orders (–ø–æ–∑–∏—Ç–∏–≤) –∏ canceled_orders (–Ω–µ–≥–∞—Ç–∏–≤),
    –∞ —Ç–∞–∫–∂–µ –¥–µ–π—Å—Ç–≤–∏—è page_view, favorite, to_cart.
    """
    log_message("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")

    # –ó–∞–∫–∞–∑—ã: –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ delivered –∏ canceled
    orders_ddf = orders_ddf[
        orders_ddf["last_status"].isin(["delivered_orders", "canceled_orders"])
    ].copy()

    # delivered_orders = 1, canceled_orders = 0
    orders_ddf["target"] = orders_ddf["last_status"].apply(
        lambda x: 1 if x == "delivered_orders" else 0, meta=("target", "int8")
    )

    # –î–µ–π—Å—Ç–≤–∏—è
    tracker_ddf = tracker_ddf[
        tracker_ddf["action_type"].isin(["page_view", "favorite", "to_cart"])
    ]

    log_message("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return orders_ddf, tracker_ddf, items_ddf


# -------------------- Train/Test split –ø–æ –≤—Ä–µ–º–µ–Ω–∏ --------------------
def train_test_split_by_time(orders_df, test_size=0.2):
    """
    –î–µ–ª–µ–Ω–∏–µ –ø–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –¥–∞—Ç–µ: train = –ø–µ—Ä–≤—ã–µ (1 - test_size) –ø–æ –≤—Ä–µ–º–µ–Ω–∏,
    test = –ø–æ—Å–ª–µ–¥–Ω–∏–µ test_size –ø–æ –≤—Ä–µ–º–µ–Ω–∏.
    """
    orders_df = orders_df.copy()
    orders_df["created_timestamp"] = pd.to_datetime(orders_df["created_timestamp"])
    orders_df = orders_df.sort_values("created_timestamp")

    cutoff_idx = int(len(orders_df) * (1 - test_size))
    cutoff_ts = orders_df.iloc[cutoff_idx]["created_timestamp"]

    train_df = orders_df[orders_df["created_timestamp"] <= cutoff_ts]
    test_df = orders_df[orders_df["created_timestamp"] > cutoff_ts]

    return (
        train_df.reset_index(drop=True),
        test_df.reset_index(drop=True),
        cutoff_ts,
    )


# -------------------- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π --------------------
def prepare_interactions(
    train_orders_df,
    tracker_ddf,
    cutoff_ts_per_user,
    batch_size=300_000_000,
    action_weights=None,
    scale_days=5,
    output_dir="/home/root6/python/e_cup/rec_system/data/processed/prepare_interactions_batches",
):
    log_message("–§–æ—Ä–º–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø–æ –±–∞—Ç—á–∞–º...")

    if action_weights is None:
        action_weights = {"page_view": 2, "favorite": 5, "to_cart": 10}

    os.makedirs(output_dir, exist_ok=True)
    batch_files = []
    ref_time = train_orders_df["created_timestamp"].max()

    # ====== Orders ======
    log_message("... –¥–ª—è orders")
    n_rows = len(train_orders_df)
    for start in range(0, n_rows, batch_size):
        batch = train_orders_df.iloc[start : start + batch_size].copy()
        days_ago = (ref_time - batch["created_timestamp"]).dt.days.clip(lower=1)
        time_factor = np.log1p(days_ago / scale_days)
        batch = batch.assign(
            timestamp=batch["created_timestamp"],
            weight=5.0 * time_factor,
            action_type="order",
        )[["user_id", "item_id", "weight", "timestamp", "action_type"]]

        path = os.path.join(output_dir, f"orders_batch_{start}.parquet")
        batch.to_parquet(path, index=False, engine="pyarrow")
        batch_files.append(path)
        del batch
        gc.collect()
        log_message(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω orders-–±–∞—Ç—á {start}-{min(start+batch_size, n_rows)}")

    # ====== Tracker ======
    log_message("... –¥–ª—è tracker")
    tracker_ddf = tracker_ddf[["user_id", "item_id", "timestamp", "action_type"]]

    # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º Dask DataFrame
    n_partitions = tracker_ddf.npartitions
    for partition_id in range(n_partitions):
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–¥–Ω—É –ø–∞—Ä—Ç–∏—Ü–∏—é
        part = tracker_ddf.get_partition(partition_id).compute()
        part["timestamp"] = pd.to_datetime(part["timestamp"])

        # cutoff_ts_per_user –∑–¥–µ—Å—å –æ–¥–∏–Ω –≥–ª–æ–±–∞–ª—å–Ω—ã–π timestamp
        cutoff_ts = cutoff_ts_per_user
        mask = part["timestamp"] < cutoff_ts
        part = part.loc[mask]

        if part.empty:
            continue

        aw = part["action_type"].map(action_weights).fillna(0)
        days_ago = (ref_time - part["timestamp"]).dt.days.clip(lower=1)
        time_factor = np.log1p(days_ago / scale_days)
        part = part.assign(weight=aw * time_factor)[
            ["user_id", "item_id", "weight", "timestamp", "action_type"]
        ]

        path = os.path.join(output_dir, f"tracker_part_{partition_id}.parquet")
        part.to_parquet(path, index=False, engine="pyarrow")
        batch_files.append(path)
        del part
        gc.collect()
        log_message(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω tracker-–ø–∞—Ä—Ç–∏—Ü–∏—è {partition_id}")

    log_message("–í—Å–µ –±–∞—Ç—á–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –Ω–∞ –¥–∏—Å–∫.")
    return batch_files


# -------------------- –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å --------------------
def compute_global_popularity(orders_df, cutoff_ts_info):
    """
    –°—á–∏—Ç–∞–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–õ–¨–ö–û —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤.

    Args:
        orders_df: –í—Å–µ –∑–∞–∫–∞–∑—ã (–¥–æ split)
        cutoff_ts_info: –ª–∏–±–æ —Å–ª–æ–≤–∞—Ä—å {user_id: cutoff_ts}, –ª–∏–±–æ –æ–¥–∏–Ω –≥–ª–æ–±–∞–ª—å–Ω—ã–π pd.Timestamp
    """
    log_message("–°—á–∏—Ç–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

    orders_df = orders_df.copy()
    orders_df["created_timestamp"] = pd.to_datetime(orders_df["created_timestamp"])

    if isinstance(cutoff_ts_info, dict):
        # –ü–æ –∫–∞–∂–¥–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å–≤–æ–π cutoff
        train_orders = []
        for user_id, cutoff_ts in cutoff_ts_info.items():
            user_orders = orders_df[
                (orders_df["user_id"] == user_id)
                & (orders_df["created_timestamp"] < cutoff_ts)
            ]
            train_orders.append(user_orders)
        train_orders_df = (
            pd.concat(train_orders, ignore_index=True)
            if train_orders
            else pd.DataFrame(columns=orders_df.columns)
        )

    else:
        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π cutoff (–æ–¥–Ω–∞ –¥–∞—Ç–∞ –¥–ª—è –≤—Å–µ—Ö)
        cutoff_ts = cutoff_ts_info
        train_orders_df = orders_df[orders_df["created_timestamp"] < cutoff_ts]

    # –°—á–∏—Ç–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if train_orders_df.empty:
        log_message("–ù–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏.")
        return pd.Series(dtype=float)

    pop = (
        train_orders_df.groupby("item_id")["item_id"]
        .count()
        .sort_values(ascending=False)
    )
    popularity = pop / pop.max()
    log_message(
        f"–ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ {len(train_orders_df)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–∫–∞–∑–∞—Ö"
    )
    return popularity


# -------------------- –û–±—É—á–µ–Ω–∏–µ ALS --------------------
def train_als(interactions_files, n_factors=64, reg=1e-3, device="cuda"):
    """
    –í–µ—Ä—Å–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –±–∞—Ç—á–µ–π –Ω–∞ –¥–∏—Å–∫ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ item_map.pkl
    """
    # 1. –ü–†–û–•–û–î: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤
    user_set = set()
    item_set = set()
    log_message("–ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤...")

    for f in tqdm(interactions_files):
        df = pl.read_parquet(f, columns=["user_id", "item_id"])
        user_set.update(df["user_id"].unique().to_list())
        item_set.update(df["item_id"].unique().to_list())

    user_map = {u: i for i, u in enumerate(sorted(user_set))}
    item_map = {i: j for j, i in enumerate(sorted(item_set))}
    log_message(
        f"–ú–∞–ø–ø–∏–Ω–≥–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã. –£–Ω–∏–∫–æ–≤: users={len(user_map)}, items={len(item_map)}"
    )

    # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º item_map.pkl
    map_dir = "/home/root6/python/e_cup/rec_system/data/processed/"
    os.makedirs(map_dir, exist_ok=True)
    item_map_path = os.path.join(map_dir, "item_map.pkl")
    with open(item_map_path, "wb") as f:
        pickle.dump(item_map, f)
    log_message(f"item_map —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {item_map_path}")

    # 2. –ü–†–û–•–û–î: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞—Ç—á–µ–π –Ω–∞ –¥–∏—Å–∫
    log_message("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞—Ç—á–µ–π –Ω–∞ –¥–∏—Å–∫...")

    batch_dir = "/home/root6/python/e_cup/rec_system/data/processed/als_batches/"
    os.makedirs(batch_dir, exist_ok=True)

    user_map_df = pl.DataFrame(
        {"user_id": list(user_map.keys()), "user_idx": list(user_map.values())}
    )
    item_map_df = pl.DataFrame(
        {"item_id": list(item_map.keys()), "item_idx": list(item_map.values())}
    )

    batch_files = []
    for i, f in enumerate(tqdm(interactions_files)):
        df = pl.read_parquet(f, columns=["user_id", "item_id", "weight"])

        df = df.join(user_map_df, on="user_id", how="inner")
        df = df.join(item_map_df, on="item_id", how="inner")

        if len(df) > 0:
            batch_path = os.path.join(batch_dir, f"batch_{i:04d}.npz")
            np.savez(
                batch_path,
                rows=df["user_idx"].to_numpy().astype(np.int32),
                cols=df["item_idx"].to_numpy().astype(np.int32),
                vals=df["weight"].to_numpy().astype(np.float32),
            )
            batch_files.append(batch_path)

    # 3. –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ
    log_message("–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")

    als_model = TorchALS(
        len(user_map), len(item_map), n_factors=n_factors, device=device
    )

    for batch_path in tqdm(batch_files):
        try:
            data = np.load(batch_path)
            rows, cols, vals = data["rows"], data["cols"], data["vals"]

            indices_np = np.empty((2, len(rows)), dtype=np.int32)
            indices_np[0] = rows
            indices_np[1] = cols
            indices = torch.tensor(indices_np, dtype=torch.long, device=device)
            values = torch.tensor(vals, dtype=torch.float32, device=device)

            sparse_batch = torch.sparse_coo_tensor(
                indices, values, size=(len(user_map), len(item_map)), device=device
            )

            als_model.partial_fit(sparse_batch, iterations=5, lr=0.005)

            del sparse_batch, indices, values
            if device == "cuda":
                torch.cuda.empty_cache()

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {batch_path}: {e}")
            continue

    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    log_message("–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
    for batch_path in batch_files:
        try:
            os.remove(batch_path)
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {batch_path}: {e}")

    try:
        if not os.listdir(batch_dir):
            os.rmdir(batch_dir)
            log_message("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –±–∞—Ç—á–µ–π —É–¥–∞–ª–µ–Ω–∞")
        else:
            log_message("–í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –æ—Å—Ç–∞–ª–∏—Å—å —Ñ–∞–π–ª—ã, –Ω–µ —É–¥–∞–ª—è–µ–º")
    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {e}")

    log_message("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    return als_model, user_map, item_map


def build_copurchase_map(
    train_orders_df,
    min_co_items=2,
    top_n=10,
    device="cuda",
    max_items=1000,
    output_dir="/home/root6/python/e_cup/rec_system/data/processed/",
):
    """
    —Å—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ–∫—É–ø–æ–∫ –¥–ª—è —Ç–æ–ø-N —Ç–æ–≤–∞—Ä–æ–≤
    –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –≤ JSON
    """
    log_message("–°—Ç—Ä–æ–∏–º co-purchase –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è —Ç–æ–ø-N —Ç–æ–≤–∞—Ä–æ–≤...")

    # 1. –¢–æ–ø –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
    item_popularity = train_orders_df["item_id"].value_counts()
    top_items = item_popularity.head(max_items).index.tolist()
    popular_items_set = set(top_items)

    log_message(f"–¢–æ–ø-{len(top_items)} –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã")

    # 2. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∫–æ—Ä–∑–∏–Ω
    baskets = []
    for items in train_orders_df.groupby(["user_id", "created_timestamp"])[
        "item_id"
    ].apply(list):
        filtered_items = [item for item in items if item in popular_items_set]
        if len(filtered_items) >= min_co_items:
            baskets.append(filtered_items)

    if not baskets:
        log_message("–ù–µ—Ç –∫–æ—Ä–∑–∏–Ω —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏")
        return {}

    log_message(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(baskets)} –∫–æ—Ä–∑–∏–Ω —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏")

    # 3. –°–ª–æ–≤–∞—Ä–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
    item2idx = {it: i for i, it in enumerate(top_items)}
    idx2item = {i: it for it, i in item2idx.items()}
    n_items = len(top_items)

    log_message(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {n_items}")

    # 4. Sparse –º–∞—Ç—Ä–∏—Ü–∞
    rows, cols, values = [], [], []
    for items in tqdm(baskets, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–∑–∏–Ω"):
        idxs = [item2idx[it] for it in items if it in item2idx]
        if len(idxs) < 2:
            continue

        weight = 1.0 / len(idxs)
        for i in range(len(idxs)):
            for j in range(len(idxs)):
                if i != j:
                    rows.append(idxs[i])
                    cols.append(idxs[j])
                    values.append(weight)

    if not rows:
        log_message("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã")
        return {}

    log_message(f"–°–æ–∑–¥–∞–µ–º sparse –º–∞—Ç—Ä–∏—Ü—É –∏–∑ {len(rows)} –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π...")

    rows_tensor = torch.tensor(rows, dtype=torch.long, device=device)
    cols_tensor = torch.tensor(cols, dtype=torch.long, device=device)
    values_tensor = torch.tensor(values, dtype=torch.float32, device=device)

    co_matrix = torch.sparse_coo_tensor(
        torch.stack([rows_tensor, cols_tensor]),
        values_tensor,
        size=(n_items, n_items),
        device=device,
    ).coalesce()

    log_message(
        f"Sparse –º–∞—Ç—Ä–∏—Ü–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞: {co_matrix.shape}, –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {co_matrix._nnz()}"
    )

    # 6. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    row_sums = torch.sparse.sum(co_matrix, dim=1).to_dense().clamp(min=1e-9)

    # 7. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
    final_copurchase = {}
    indices = co_matrix.indices()
    values = co_matrix.values()

    log_message("–§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏...")
    for i in tqdm(range(n_items), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤"):
        mask = indices[0] == i
        if mask.any():
            col_indices = indices[1][mask]
            row_values = values[mask] / row_sums[i]

            if len(row_values) > 0:
                topk_vals, topk_idx = torch.topk(
                    row_values, k=min(top_n, len(row_values))
                )
                final_copurchase[idx2item[i]] = [
                    (idx2item[col_indices[j].item()], topk_vals[j].item())
                    for j in range(len(topk_vals))
                    if topk_vals[j].item() > 0
                ]

    log_message(f"Co-purchase —Å–ª–æ–≤–∞—Ä—å –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {len(final_copurchase)} —Ç–æ–≤–∞—Ä–æ–≤")

    avg_recommendations = sum(len(v) for v in final_copurchase.values()) / max(
        1, len(final_copurchase)
    )
    log_message(f"–í —Å—Ä–µ–¥–Ω–µ–º {avg_recommendations:.1f} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ —Ç–æ–≤–∞—Ä")

    # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "copurchase_map.pkl")

    with open(output_file, "wb") as f:
        pickle.dump(final_copurchase, f)

    log_message(f"Co-purchase —Å–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_file}")

    return final_copurchase


def build_category_maps(
    items_df,
    categories_df,
    save_dir="/home/root6/python/e_cup/rec_system/data/processed/",
):
    """
    –£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: —Å—Ç—Ä–æ–∏–º –º–∞–ø–ø–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª—ã.
    """
    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤...")

    os.makedirs(save_dir, exist_ok=True)

    # –¢–æ–≤–∞—Ä -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    item_to_cat = dict(zip(items_df["item_id"], items_df["catalogid"]))

    # –ö–∞—Ç–µ–≥–æ—Ä–∏—è -> —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤
    cat_to_items = (
        items_df.groupby("catalogid")["item_id"].apply(lambda x: x.to_numpy()).to_dict()
    )

    # –ò–µ—Ä–∞—Ä—Ö–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    cat_tree = dict(zip(categories_df["catalogid"], categories_df["ids"]))

    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—é (–≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ)
    extended_cat_to_items = {}
    for cat_id, items_list in cat_to_items.items():
        all_items = set(items_list)
        parents = cat_tree.get(cat_id, [])
        for parent in parents:
            if parent in cat_to_items:
                all_items.update(cat_to_items[parent])
        extended_cat_to_items[cat_id] = np.array(list(all_items))

    # ---- –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ----
    with open(os.path.join(save_dir, "item_to_cat.pkl"), "wb") as f:
        pickle.dump(item_to_cat, f)

    with open(os.path.join(save_dir, "extended_cat_to_items.pkl"), "wb") as f:
        pickle.dump(extended_cat_to_items, f)

    log_message(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: item_to_cat.pkl –∏ extended_cat_to_items.pkl –≤ {save_dir}")

    return item_to_cat, extended_cat_to_items


# -------------------- –ú–µ—Ç—Ä–∏–∫–∏ --------------------
def ndcg_at_k(recommended, ground_truth, k=100, device="cuda"):
    """
    NDCG@K: —Å—á–∏—Ç–∞–µ–º —á–µ—Ä–µ–∑ torch –Ω–∞ GPU
    recommended: —Å–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö item_id
    ground_truth: –º–Ω–æ–∂–µ—Å—Ç–≤–æ/—Å–ø–∏—Å–æ–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö item_id
    """
    if not ground_truth:
        return 0.0

    # –ë–µ—Ä—ë–º —Ç–æ–ø-k —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    rec_k = torch.tensor(recommended[:k], device=device)

    # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
    gt_set = set(ground_truth)
    gt_mask = torch.tensor(
        [1 if x.item() in gt_set else 0 for x in rec_k],
        dtype=torch.float32,
        device=device,
    )

    # –ü–æ–∑–∏—Ü–∏–∏ (1..k)
    positions = torch.arange(1, len(rec_k) + 1, device=device, dtype=torch.float32)

    # DCG: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å / log2(–ø–æ–∑–∏—Ü–∏—è+1)
    dcg = torch.sum(gt_mask / torch.log2(positions + 1))

    # IDCG: –∏–¥–µ–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
    ideal_len = min(len(ground_truth), k)
    idcg = torch.sum(
        1.0
        / torch.log2(
            torch.arange(1, ideal_len + 1, device=device, dtype=torch.float32) + 1
        )
    )

    return (dcg / idcg).item() if idcg > 0 else 0.0


def build_recent_items_map_from_batches(
    batch_dir,
    recent_n=5,
    save_path="/home/root6/python/e_cup/rec_system/data/processed/recent_items_map.pkl",
):
    """–í–µ—Ä—Å–∏—è –≥–¥–µ weight –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ items.
    save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ None ‚Äî –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º).
    """
    batch_files = sorted(Path(batch_dir).glob("*.parquet"))
    recent_items_map = {}

    for f in tqdm(batch_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π"):
        try:
            df = pl.read_parquet(
                f, columns=["user_id", "item_id", "timestamp", "weight"]
            )

            df = df.with_columns(
                [
                    pl.col("user_id").cast(pl.Int64),
                    pl.col("item_id").cast(pl.Int64),
                    pl.col("timestamp").dt.epoch("s").alias("ts_epoch"),
                    pl.col("weight").cast(pl.Float64),
                ]
            )

            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
            df = df.with_columns(
                (
                    pl.col("weight") * 0.8
                    + pl.col("ts_epoch") / pl.col("ts_epoch").max() * 0.2
                ).alias("score")
            )

            df_sorted = df.sort(["user_id", "score"], descending=[False, True])

            grouped = df_sorted.group_by("user_id").agg(
                pl.col("item_id").head(recent_n).alias("items")
            )

            for row in grouped.iter_rows():
                user_id, items = row[0], row[1]
                if user_id not in recent_items_map:
                    recent_items_map[user_id] = items
                else:
                    combined = (recent_items_map[user_id] + items)[:recent_n]
                    recent_items_map[user_id] = combined

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {f}: {e}")
            continue

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    if save_path is not None:
        try:
            with open(save_path, "wb") as f:
                pickle.dump(recent_items_map, f)
            log_message(f"–°–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {save_path}: {e}")

    return recent_items_map


# -------------------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ --------------------
def save_model(model, user_map, item_map, path="src/models/model_als.pkl"):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–∞–ø–ø–∏–Ω–≥–∏ –≤ pickle.
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)

    data = {
        "model": model,
        "user_map": user_map,
        "item_map": item_map,
    }

    with open(path, "wb") as f:
        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)

    log_message(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")


# -------------------- –ú–µ—Ç—Ä–∏–∫–∏ --------------------
def ndcg_at_k_grouped(predictions, targets, groups, k=100, device="cpu"):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ NDCG@k –¥–ª—è —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch.

    predictions: 1D –º–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö score (list, np.array –∏–ª–∏ torch.tensor)
    targets:     1D –º–∞—Å—Å–∏–≤ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (0/1) —Ç–æ–π –∂–µ –¥–ª–∏–Ω—ã
    groups:      —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –≥—Ä—É–ø–ø (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∫–æ–ª—å–∫–æ –∞–π—Ç–µ–º–æ–≤ —É –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
    k:           —Ç–æ–ø-K –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏
    device:      "cpu" –∏–ª–∏ "cuda"
    """
    preds = torch.as_tensor(predictions, dtype=torch.float32, device=device)
    targs = torch.as_tensor(targets, dtype=torch.float32, device=device)

    ndcg_scores = []
    start_idx = 0

    for group_size in groups:
        if group_size == 0:
            continue

        end_idx = start_idx + group_size
        group_preds = preds[start_idx:end_idx]
        group_targs = targs[start_idx:end_idx]

        # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        sorted_idx = torch.argsort(group_preds, descending=True)
        sorted_targs = group_targs[sorted_idx]

        # DCG
        denom = torch.log2(
            torch.arange(2, 2 + min(k, group_size), device=device, dtype=torch.float32)
        )
        dcg = (sorted_targs[:k] / denom).sum()

        # IDCG
        ideal_sorted = torch.sort(group_targs, descending=True).values
        idcg = (ideal_sorted[:k] / denom).sum()

        ndcg = (dcg / idcg) if idcg > 0 else torch.tensor(0.0, device=device)
        ndcg_scores.append(ndcg)

        start_idx = end_idx

    if not ndcg_scores:
        return 0.0

    return float(torch.stack(ndcg_scores).mean().cpu())


class TorchALS(nn.Module):
    def __init__(
        self,
        n_users,
        n_items,
        n_factors=64,
        reg=1e-3,
        dtype=torch.float32,
        device="cuda",
    ):
        super().__init__()
        self.user_factors = nn.Parameter(
            nn.init.xavier_normal_(
                torch.empty(n_users, n_factors, dtype=dtype, device=device)
            )
        )
        self.item_factors = nn.Parameter(
            nn.init.xavier_normal_(
                torch.empty(n_items, n_factors, dtype=dtype, device=device)
            )
        )
        self.reg = reg
        self.device = device
        self.partial_optimizer = None  # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è partial_fit
        self.to(device)

    def forward(self, user, item):
        return (self.user_factors[user] * self.item_factors[item]).sum(1)

    def partial_fit(self, sparse_batch, iterations=5, lr=0.005, show_progress=False):
        """
        –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞
        """
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º sparse tensor
        if not sparse_batch.is_coalesced():
            sparse_batch = sparse_batch.coalesce()

        users_coo, items_coo = sparse_batch.indices()
        values = sparse_batch.values()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
        if self.partial_optimizer is None:
            self.partial_optimizer = torch.optim.Adam(
                self.parameters(), lr=lr, weight_decay=self.reg
            )
        else:
            # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
            for param_group in self.partial_optimizer.param_groups:
                param_group["lr"] = lr

        # –û–±—É—á–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞ (–±–µ–∑ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è!)
        for epoch in range(iterations):
            self.partial_optimizer.zero_grad()

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –≤ –±–∞—Ç—á–µ
            pred = self.forward(users_coo, items_coo)
            loss = F.mse_loss(pred, values)

            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            user_reg = self.reg * self.user_factors[users_coo].pow(2).mean()
            item_reg = self.reg * self.item_factors[items_coo].pow(2).mean()
            total_loss = loss + user_reg + item_reg

            total_loss.backward()
            self.partial_optimizer.step()

            if show_progress and (epoch % 10 == 0 or epoch == iterations - 1):
                log_message(f"Partial fit epoch {epoch}, Loss: {total_loss.item():.6f}")


class LightGBMRecommender:
    def __init__(self):
        self.model = None
        self.feature_columns = []
        self.user_embeddings = None
        self.item_embeddings = None
        self.external_embeddings_dict = None
        self.copurchase_map = None
        self.item_to_cat = None
        self.cat_to_items = None
        self.user_map = None
        self.item_map = None
        self.covisitation_matrix = None

    def set_als_embeddings(self, als_model):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º ALS —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"""
        self.user_embeddings = als_model.user_factors
        self.item_embeddings = als_model.item_factors

    def set_external_embeddings(self, embeddings_dict):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤"""
        self.external_embeddings_dict = embeddings_dict

    def set_additional_data(
        self, copurchase_map, item_to_cat, cat_to_items, user_map, item_map
    ):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        self.copurchase_map = copurchase_map
        self.item_to_cat = item_to_cat
        self.cat_to_items = cat_to_items
        self.user_map = user_map
        self.item_map = item_map

    def _train_covisitation_matrix(
        self, train_data: pd.DataFrame, min_cooccurrence: int = 5
    ):
        """–û–±—É—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        log_message("–û–±—É—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏–∏ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö...")

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å: user_id -> —Å–ø–∏—Å–æ–∫ item_id —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞–ª
        user_items = {}
        for user_id, group in train_data.groupby("user_id"):
            user_items[user_id] = set(group["item_id"].unique())

        # –°—á–∏—Ç–∞–µ–º –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏—é (—Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –ø–æ—è–≤–ª–µ–Ω–∏—è)
        cooccurrence = defaultdict(int)

        for user_id, items in user_items.items():
            items_list = list(items)
            for i in range(len(items_list)):
                for j in range(i + 1, len(items_list)):
                    item1, item2 = items_list[i], items_list[j]
                    # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –ø–∞—Ä—É –¥–ª—è consistency
                    pair = (min(item1, item2), max(item1, item2))
                    cooccurrence[pair] += 1

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏–π
        self.covisitation_matrix = {}
        for (item1, item2), count in cooccurrence.items():
            if count >= min_cooccurrence:
                self.covisitation_matrix[item1] = (
                    self.covisitation_matrix.get(item1, 0) + count
                )
                self.covisitation_matrix[item2] = (
                    self.covisitation_matrix.get(item2, 0) + count
                )

        log_message(
            f"–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏–∏: {len(self.covisitation_matrix)} —Ç–æ–≤–∞—Ä–æ–≤"
        )

    def _load_ui_features_for_pairs(self, pairs_df, ui_features_path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç UI-–ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞—Ä user-item
        """
        try:
            if pairs_df.empty:
                return None

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–º–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            temp_pairs_path = "/tmp/filter_pairs.parquet"
            pairs_df[["user_id", "item_id"]].to_parquet(temp_pairs_path, index=False)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Polars –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            result = (
                pl.scan_parquet(ui_features_path)
                .join(
                    pl.scan_parquet(temp_pairs_path),
                    on=["user_id", "item_id"],
                    how="inner",
                )
                .collect()
                .to_pandas()
            )

            # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if os.path.exists(temp_pairs_path):
                os.remove(temp_pairs_path)

            return result

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–∞—Ä: {e}")
            return None

    def _add_rich_features(
        self, data: pd.DataFrame, train_only_data: pd.DataFrame = None
    ) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏—á —Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ–º —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""

        log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–æ–≥–∞—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
        if "timestamp" in data.columns:
            try:
                data["is_weekend"] = data["timestamp"].dt.dayofweek >= 5
                data["hour"] = data["timestamp"].dt.hour
            except Exception as e:
                log_message(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å timestamp: {e}")
                data["is_weekend"] = 0
                data["hour"] = -1
        else:
            data["is_weekend"] = np.nan
            data["hour"] = np.nan

        # –ï—Å–ª–∏ train_only_data –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –±–µ—Ä—ë–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ data
        if train_only_data is None or train_only_data.empty:
            train_only_data = data[data.get("target", 0) == 0]

        # --- –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–∞ ---
        if not train_only_data.empty:
            item_pop = (
                train_only_data.groupby("item_id")["user_id"]
                .count()
                .rename("item_popularity")
                .reset_index()
            )
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ item_pop –Ω–µ –ø—É—Å—Ç–æ–π
            if not item_pop.empty:
                data = data.merge(item_pop, on="item_id", how="left")

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç –∏–ª–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
        if "item_popularity" not in data.columns:
            data["item_popularity"] = 0
        else:
            data["item_popularity"] = data["item_popularity"].fillna(0)

        # --- –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ---
        if "category_id" in data.columns:
            if not train_only_data.empty and "category_id" in train_only_data.columns:
                cat_pop = (
                    train_only_data.groupby("category_id")["user_id"]
                    .count()
                    .rename("category_popularity")
                    .reset_index()
                )
                if not cat_pop.empty:
                    data = data.merge(cat_pop, on="category_id", how="left")

            if "category_popularity" not in data.columns:
                data["category_popularity"] = 0
            else:
                data["category_popularity"] = data["category_popularity"].fillna(0)
        else:
            data["category_popularity"] = 0

        # --- –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
        if not train_only_data.empty:
            user_activity = (
                train_only_data.groupby("user_id")["item_id"]
                .count()
                .rename("user_activity")
                .reset_index()
            )
            if not user_activity.empty:
                data = data.merge(user_activity, on="user_id", how="left")

        if "user_activity" not in data.columns:
            data["user_activity"] = 0
        else:
            data["user_activity"] = data["user_activity"].fillna(0)

        # --- –°—Ä–µ–¥–Ω—è—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–æ–≤ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
        if not train_only_data.empty and "item_popularity" in data.columns:
            train_with_pop = (
                train_only_data.merge(item_pop, on="item_id", how="left")
                if not item_pop.empty
                else train_only_data.copy()
            )

            if "item_popularity" in train_with_pop.columns:
                train_with_pop["item_popularity"] = train_with_pop[
                    "item_popularity"
                ].fillna(0)
                user_avg_pop = (
                    train_with_pop.groupby("user_id")["item_popularity"]
                    .mean()
                    .rename("user_avg_item_popularity")
                    .reset_index()
                )
                if not user_avg_pop.empty:
                    data = data.merge(user_avg_pop, on="user_id", how="left")

        if "user_avg_item_popularity" not in data.columns:
            data["user_avg_item_popularity"] = 0
        else:
            data["user_avg_item_popularity"] = data["user_avg_item_popularity"].fillna(
                0
            )

        # --- –ö–æ–≤–∏–∑–∏—Ç–∞—Ü–∏—è ---
        if (
            hasattr(self, "covisitation_matrix")
            and self.covisitation_matrix is not None
        ):
            data["covisitation_score"] = (
                data["item_id"]
                .map(self.covisitation_matrix.get, na_action="ignore")
                .fillna(0)
            )
        else:
            data["covisitation_score"] = 0

        # --- FCLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ---
        if getattr(self, "external_embeddings_dict", None):
            log_message("–£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ FCLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ GPU...")
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            all_item_ids = list(self.external_embeddings_dict.keys())
            embedding_dim = len(next(iter(self.external_embeddings_dict.values())))
            n_fclip_dims = min(10, embedding_dim)

            embeddings_tensor = torch.zeros(
                len(all_item_ids), embedding_dim, device=device
            )
            for idx, item_id in enumerate(all_item_ids):
                embeddings_tensor[idx] = torch.tensor(
                    self.external_embeddings_dict[item_id],
                    device=device,
                    dtype=torch.float32,
                )

            item_id_to_idx = {item_id: idx for idx, item_id in enumerate(all_item_ids)}
            batch_size = 100_000
            total_rows = len(data)

            for i in range(n_fclip_dims):
                log_message(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ FCLIP –∏–∑–º–µ—Ä–µ–Ω–∏—è {i+1}/{n_fclip_dims} –Ω–∞ GPU...")
                data[f"fclip_embed_{i}"] = 0.0

                for start_idx in range(0, total_rows, batch_size):
                    end_idx = min(start_idx + batch_size, total_rows)
                    batch_data = data.iloc[start_idx:end_idx]
                    batch_item_ids = batch_data["item_id"].values
                    valid_mask = np.array(
                        [item_id in item_id_to_idx for item_id in batch_item_ids]
                    )
                    valid_indices = np.where(valid_mask)[0]
                    valid_item_ids = batch_item_ids[valid_mask]

                    if len(valid_item_ids) > 0:
                        tensor_indices = torch.tensor(
                            [item_id_to_idx[item_id] for item_id in valid_item_ids],
                            device=device,
                        )
                        batch_embeddings = (
                            embeddings_tensor[tensor_indices, i].cpu().numpy()
                        )
                        data.iloc[
                            start_idx + valid_indices,
                            data.columns.get_loc(f"fclip_embed_{i}"),
                        ] = batch_embeddings

                    del batch_data, batch_item_ids
                    if start_idx % (batch_size * 5) == 0:
                        torch.cuda.empty_cache()

            del embeddings_tensor, item_id_to_idx
            torch.cuda.empty_cache()

        # --- –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ---
        new_features = [
            "is_weekend",
            "hour",
            "item_popularity",
            "category_popularity",
            "user_activity",
            "user_avg_item_popularity",
            "covisitation_score",
        ]
        if getattr(self, "external_embeddings_dict", None):
            new_features += [f"fclip_embed_{i}" for i in range(n_fclip_dims)]

        existing_features = set(getattr(self, "feature_columns", []))
        for feature in new_features:
            if feature in data.columns and feature not in existing_features:
                self.feature_columns.append(feature)
                existing_features.add(feature)

        log_message(f"–î–æ–±–∞–≤–ª–µ–Ω—ã —Ñ–∏—á–∏: {[f for f in new_features if f in data.columns]}")
        log_message(f"–í—Å–µ–≥–æ —Ñ–∏—á –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {len(self.feature_columns)}")

        return data

    def prepare_training_data(
        self,
        interactions_files,
        orders_ddf,
        user_map,
        item_map,
        popularity_s,
        recent_items_map,
        sample_fraction=0.1,
        negatives_per_positive=1,
        ui_features_dir=None,
        val_days: int = 7,
    ):

        log_message("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LightGBM (streaming)...")

        # –ë–µ—Ä—ë–º fraction –∫–∞–∂–¥–æ–π –ø–∞—Ä—Ç–∏—Ü–∏–∏
        orders_ddf = orders_ddf.map_partitions(
            lambda df: df.sample(frac=sample_fraction, random_state=42)
        )

        base_dir = Path("/home/root6/python/e_cup/rec_system/data/processed/")
        base_dir.mkdir(parents=True, exist_ok=True)
        train_path = base_dir / "train.parquet"
        val_path = base_dir / "val.parquet"

        batch_size = 50_000  # —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        small_batch = []

        # --- –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è ---
        interactions_chunks = []
        for f in tqdm(interactions_files, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π"):
            df = pd.read_parquet(
                f, columns=["user_id", "item_id", "timestamp", "weight"]
            )
            df["user_id"] = df["user_id"].astype("int64")
            df["item_id"] = df["item_id"].astype("int64")
            interactions_chunks.append(df)
        interactions_df = pd.concat(interactions_chunks, ignore_index=True)
        interactions_df["timestamp"] = pd.to_datetime(interactions_df["timestamp"])
        max_timestamp = interactions_df["timestamp"].max()

        split_time = max_timestamp - pd.Timedelta(days=val_days)
        train_timestamp_fill = split_time - pd.Timedelta(seconds=1)

        # –†–∞–∑–¥–µ–ª—è–µ–º interactions –Ω–∞ train –∏ val –¥–ª—è user_interacted_items
        interactions_train = interactions_df[interactions_df["timestamp"] <= split_time]
        interactions_val = interactions_df[interactions_df["timestamp"] > split_time]

        user_interacted_items_train = {
            uid: set(gr["item_id"].unique())
            for uid, gr in interactions_train.groupby("user_id")
        }

        all_items = np.array(list(item_map.keys()), dtype=np.int64)
        popular_items_set = set(popularity_s.nlargest(10000).index.astype(np.int64))

        all_items_tensor = torch.tensor(all_items, device="cuda")
        popular_items_tensor = torch.tensor(list(popular_items_set), device="cuda")

        cols_order = ["user_id", "item_id", "timestamp", "target"]

        # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ParquetWriter ---
        writer_train = None
        writer_val = None

        def write_small_batch(batch_list):
            nonlocal writer_train, writer_val
            batch_df = pd.DataFrame(batch_list, columns=cols_order)
            train_df = batch_df[batch_df["timestamp"] <= split_time]
            val_df = batch_df[batch_df["timestamp"] > split_time]

            if not train_df.empty:
                table_train = pa.Table.from_pandas(train_df, preserve_index=False)
                if writer_train is None:
                    writer_train = pq.ParquetWriter(
                        train_path, schema=table_train.schema
                    )
                writer_train.write_table(table_train)

            if not val_df.empty:
                table_val = pa.Table.from_pandas(val_df, preserve_index=False)
                if writer_val is None:
                    writer_val = pq.ParquetWriter(val_path, schema=table_val.schema)
                writer_val.write_table(table_val)

        # --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è TRAIN –¥–∞–Ω–Ω—ã—Ö ---
        for part_idx, part in enumerate(
            tqdm(orders_ddf.to_delayed(), desc="Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞—Ä—Ç–∏—Ü–∏–π")
        ):
            pdf = part.compute()
            user_groups = pdf.groupby("user_id")

            for uid, gr in tqdm(
                user_groups,
                desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ä—Ç–∏—Ü–∏–∏ {part_idx+1}/{len(orders_ddf.to_delayed())}",
                leave=False,
            ):
                pos_items = set(gr[gr["target"] == 1]["item_id"])
                neg_items_existing = set(gr[gr["target"] == 0]["item_id"])
                interacted = user_interacted_items_train.get(uid, set())
                excluded = pos_items | neg_items_existing | interacted

                # --- available items –Ω–∞ GPU (–±–µ–∑ –ø–∞–¥–µ–Ω–∏–π) ---
                excluded_tensor = (
                    torch.tensor(list(excluded), device="cuda")
                    if excluded
                    else torch.tensor([], device="cuda")
                )
                mask = ~torch.isin(all_items_tensor, excluded_tensor)
                available_items_tensor = all_items_tensor[mask]

                # –ü–æ–∑–∏—Ç–∏–≤—ã
                for it in pos_items:
                    small_batch.append([uid, it, train_timestamp_fill, 1])

                # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–µ–≥–∞—Ç–∏–≤—ã
                for it in neg_items_existing:
                    small_batch.append([uid, it, train_timestamp_fill, 0])

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–µ–≥–∞—Ç–∏–≤—ã
                n_needed = max(
                    0, len(pos_items) * negatives_per_positive - len(neg_items_existing)
                )
                if n_needed > 0 and len(available_items_tensor) > 0:
                    popular_mask = torch.isin(
                        available_items_tensor, popular_items_tensor
                    )
                    popular_candidates = available_items_tensor[popular_mask]
                    random_candidates = available_items_tensor[~popular_mask]

                    n_popular = min(n_needed // 2, len(popular_candidates))
                    n_random = min(n_needed - n_popular, len(random_candidates))

                    sampled_items = []
                    if n_popular > 0:
                        perm = torch.randperm(len(popular_candidates), device="cuda")
                        sampled_items.extend(
                            popular_candidates[perm[:n_popular]].tolist()
                        )
                    if n_random > 0:
                        perm = torch.randperm(len(random_candidates), device="cuda")
                        sampled_items.extend(
                            random_candidates[perm[:n_random]].tolist()
                        )

                    for it in sampled_items:
                        small_batch.append([uid, it, train_timestamp_fill, 0])

                # --- –ó–∞–ø–∏—Å—å –±–∞—Ç—á–∞ ---
                if len(small_batch) >= batch_size:
                    write_small_batch(small_batch)
                    small_batch = []

        # --- –û—Å—Ç–∞—Ç–∫–∏ ---
        if small_batch:
            write_small_batch(small_batch)

        # --- –ó–∞–∫—Ä—ã–≤–∞–µ–º writers ---
        if writer_train:
            writer_train.close()
        if writer_val:
            writer_val.close()
        else:
            empty_val = pd.DataFrame(columns=cols_order)
            empty_val.to_parquet(val_path, engine="pyarrow", index=False)

        # --- –ó–∞–≥—Ä—É–∂–∞–µ–º train/val ---
        train_data = pd.read_parquet(train_path)
        val_data = pd.read_parquet(val_path)
        if len(val_data) == 0:
            cutoff = int(len(train_data) * 0.9)
            train_data, val_data = (
                train_data.iloc[:cutoff].copy(),
                train_data.iloc[cutoff:].copy(),
            )

        # --- –û–±—É—á–∞–µ–º –∫–æ–≤–∏–∑–∏—Ç–∞—Ü–∏—é –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö ---
        self._train_covisitation_matrix(train_data)

        # --- –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏—á–∏ ---
        log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏—á–µ–π –¥–ª—è train –¥–∞–Ω–Ω—ã—Ö...")
        train_data = self._add_rich_features(train_data, train_only_data=train_data)

        log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏—á–µ–π –¥–ª—è val –¥–∞–Ω–Ω—ã—Ö (–Ω–∞ –æ—Å–Ω–æ–≤–µ train —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫)...")
        val_data = self._add_rich_features(val_data, train_only_data=train_data)

        if ui_features_dir and os.path.exists(ui_features_dir):
            train_data, val_data = self._add_ui_features_optimized(
                train_data, val_data, ui_features_dir
            )

        # --- –õ–æ–≥ ---
        def log_message_dist(df, name):
            counts = df["target"].value_counts(dropna=False).to_dict()
            log_message(f"{name}: rows={len(df)}; target_counts={counts}")

        log_message_dist(train_data, "TRAIN")
        log_message_dist(val_data, "VAL")
        log_message(f"split_time = {split_time} (val_days={val_days})")
        log_message(
            f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: train={len(train_data)}, val={len(val_data)}"
        )

        return train_data, val_data

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è UI —Ñ–∏—á
    def _add_ui_features_optimized(self, train_data, val_data, ui_features_dir):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ UI —Ñ–∏—á"""
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è batch processing
            all_data = pd.concat(
                [train_data[["user_id", "item_id"]], val_data[["user_id", "item_id"]]],
                ignore_index=True,
            )

            ui_features_batch = get_ui_features_batch(
                all_data.to_dict("records"), ui_features_dir
            )

            if ui_features_batch:
                ui_features_df = pd.DataFrame(ui_features_batch)
                ui_features_df["user_id"] = ui_features_df["user_id"].astype("int64")
                ui_features_df["item_id"] = ui_features_df["item_id"].astype("int64")

                # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                ui_features_df = ui_features_df.drop_duplicates(
                    subset=["user_id", "item_id"]
                )

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å train –∏ val
                train_data = train_data.merge(
                    ui_features_df, on=["user_id", "item_id"], how="left"
                ).fillna(0)
                val_data = val_data.merge(
                    ui_features_df, on=["user_id", "item_id"], how="left"
                ).fillna(0)

                # –û–±–Ω–æ–≤–ª—è–µ–º feature_columns
                new_features = [
                    col
                    for col in ui_features_df.columns
                    if col not in ["user_id", "item_id"]
                    and col not in self.feature_columns
                ]
                self.feature_columns.extend(new_features)

                log_message(f"–î–æ–±–∞–≤–ª–µ–Ω—ã UI –ø—Ä–∏–∑–Ω–∞–∫–∏: {new_features}")

        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

        return train_data, val_data

    def _get_copurchase_strength(self, item_id):
        """–ü–æ–ª—É—á–∞–µ–º —Å–∏–ª—É co-purchase —Å–≤—è–∑–∏"""
        if not self.copurchase_map or item_id not in self.copurchase_map:
            return 0.0

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∏–ª–∞ —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º
        strengths = [strength for _, strength in self.copurchase_map[item_id]]
        return max(strengths) if strengths else 0.0

    def _get_user_copurchase_affinity(self, user_id, item_id):
        """Affinity –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫ co-purchase —Å–≤—è–∑—è–º"""
        if not self.copurchase_map or not hasattr(self, "user_items_history"):
            return 0.0

        # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Ä–∞—Å—á–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—â—É—é —Å–∏–ª—É
        return self._get_copurchase_strength(item_id)

    def train(self, train_data, val_data=None, params=None):
        """
        –û–±—É—á–µ–Ω–∏–µ LightGBM —Å –±–∏–Ω–∞—Ä–Ω–æ–π —Ü–µ–ª—å—é, —Å –æ—Ü–µ–Ω–∫–æ–π NDCG@100 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
        """
        if params is None:
            params = {
                "objective": "binary",
                "metric": "binary_logloss",
                "learning_rate": 0.05,
                "num_leaves": 31,
                "max_depth": 6,
                "min_data_in_leaf": 20,
                "feature_fraction": 0.8,
                "bagging_fraction": 0.8,
                "bagging_freq": 1,
                "verbosity": 1,
                "force_row_wise": True,
                "device": "cpu",  # –º–æ–∂–Ω–æ "cuda", –Ω–æ —á–µ—Ç –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª–æ—Å—å :(
                "num_threads": 8,
                "max_bin": 200,
                "boosting": "gbdt",
            }

        X_train = train_data[self.feature_columns]
        y_train = train_data["target"]
        train_dataset = lgb.Dataset(
            X_train, label=y_train, feature_name=list(X_train.columns)
        )

        valid_sets = [train_dataset]
        valid_names = ["train"]

        if val_data is not None:
            X_val = val_data[self.feature_columns]
            y_val = val_data["target"]
            val_dataset = lgb.Dataset(X_val, label=y_val, reference=train_dataset)
            valid_sets.append(val_dataset)
            valid_names.append("valid")

        log_message(f"–†–∞–∑–º–µ—Ä train: {len(X_train)}")
        if val_data is not None:
            log_message(f"–†–∞–∑–º–µ—Ä val: {len(X_val)}")

        # Callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        def log_every_N_iter(env):
            if env.iteration % 10 == 0:
                metrics = ", ".join(
                    [
                        f"{name}_{metric}:{val:.4f}"
                        for name, metric, val, _ in env.evaluation_result_list
                    ]
                )
                print(f"[Iter {env.iteration}] {metrics}")

        self.model = lgb.train(
            params,
            train_dataset,
            num_boost_round=1000,
            valid_sets=valid_sets,
            valid_names=valid_names,
            callbacks=[lgb.early_stopping(50), log_every_N_iter],
        )

        # –í—ã—á–∏—Å–ª—è–µ–º NDCG@100 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        if val_data is not None:
            ndcg_val = self.evaluate(val_data)
            log_message(f"NDCG@100 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {ndcg_val:.4f}")

        return self.model

    def evaluate(self, data, k=100):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ NDCG@k"""
        if self.model is None or len(data) == 0:
            return 0.0

        data = data.copy()
        data["score"] = self.model.predict(data[self.feature_columns])

        groups = data.groupby("user_id").size().values
        ndcg = ndcg_at_k_grouped(
            data["score"].values, data["target"].values, groups, k=k
        )
        return ndcg

    def recommend(self, user_items_data, top_k=100):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —Ç–æ–ø-K"""
        data = user_items_data.copy()
        data["score"] = self.model.predict(data[self.feature_columns])

        recommendations = {
            user_id: group.nlargest(top_k, "score")["item_id"].tolist()
            for user_id, group in data.groupby("user_id")
        }
        return recommendations


def build_user_features_dict(
    interactions_files,
    orders_df,
    device="cuda",
    save_path="/home/root6/python/e_cup/rec_system/data/processed/user_features_dict.pkl",
):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Polars
    """
    import pickle

    import polars as pl

    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    # 1. –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –¢–†–ï–ö–ï–†–£ (–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è)
    user_stats_list = []
    for f in tqdm(interactions_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–µ–∫–µ—Ä–∞"):
        df = pl.read_parquet(f)

        chunk_stats = df.group_by("user_id").agg(
            [
                pl.col("weight").count().alias("count"),
                pl.col("weight").sum().alias("sum"),
                pl.col("weight").max().alias("max"),
                pl.col("weight").min().alias("min"),
                pl.col("timestamp").max().alias("last_ts"),
                pl.col("timestamp").min().alias("first_ts"),
            ]
        )
        user_stats_list.append(chunk_stats)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if user_stats_list:
        all_stats = pl.concat(user_stats_list)
        final_stats = all_stats.group_by("user_id").agg(
            [
                pl.col("count").sum().alias("user_count"),
                pl.col("sum").sum().alias("user_sum"),
                pl.col("max").max().alias("user_max"),
                pl.col("min").min().alias("user_min"),
                pl.col("last_ts").max().alias("user_last_ts"),
                pl.col("first_ts").min().alias("user_first_ts"),
            ]
        )
    else:
        final_stats = pl.DataFrame()

    # 2. –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –ó–ê–ö–ê–ó–ê–ú
    log_message("–ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∑–∞–∫–∞–∑–∞–º...")
    if isinstance(orders_df, pl.DataFrame):
        orders_pl = orders_df
    else:
        orders_pl = pl.from_pandas(
            orders_df.compute() if hasattr(orders_df, "compute") else orders_df
        )

    order_stats = orders_pl.group_by("user_id").agg(
        [
            pl.col("item_id").count().alias("user_orders_count"),
            pl.col("created_timestamp").max().alias("user_last_order_ts"),
            pl.col("created_timestamp").min().alias("user_first_order_ts"),
        ]
    )

    # 3. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –î–ê–ù–ù–´–•
    if len(final_stats) > 0 and len(order_stats) > 0:
        user_stats = final_stats.join(order_stats, on="user_id", how="full")
    elif len(final_stats) > 0:
        user_stats = final_stats
    else:
        user_stats = order_stats

    # 4. –í–´–ß–ò–°–õ–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
    current_time = pl.lit(datetime.now())

    user_stats = user_stats.with_columns(
        [
            pl.col("user_count").fill_null(0),
            pl.col("user_sum").fill_null(0),
            pl.col("user_orders_count").fill_null(0),
            (pl.col("user_sum") / pl.col("user_count")).alias("user_mean"),
            ((current_time - pl.col("user_last_ts")).dt.total_days()).alias(
                "user_days_since_last"
            ),
            ((current_time - pl.col("user_first_ts")).dt.total_days()).alias(
                "user_days_since_first"
            ),
            ((current_time - pl.col("user_last_order_ts")).dt.total_days()).alias(
                "user_days_since_last_order"
            ),
        ]
    ).fill_nan(0)

    # 5. –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –°–õ–û–í–ê–†–¨
    user_stats_dict = {}
    for row in user_stats.iter_rows(named=True):
        user_stats_dict[row["user_id"]] = {
            "user_count": row["user_count"],
            "user_mean": row["user_mean"],
            "user_sum": row["user_sum"],
            "user_max": row["user_max"],
            "user_min": row["user_min"],
            "user_last_ts": row["user_last_ts"],
            "user_first_ts": row["user_first_ts"],
            "user_orders_count": row["user_orders_count"],
            "user_last_order_ts": row["user_last_order_ts"],
            "user_first_order_ts": row["user_first_order_ts"],
            "user_days_since_last": row["user_days_since_last"],
            "user_days_since_first": row["user_days_since_first"],
            "user_days_since_last_order": row["user_days_since_last_order"],
        }

    # 6. –°–û–•–†–ê–ù–ï–ù–ò–ï –í PICKLE
    with open(save_path, "wb") as f:
        pickle.dump(user_stats_dict, f)

    log_message(
        f"–°–ª–æ–≤–∞—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {save_path}. –ó–∞–ø–∏—Å–µ–π: {len(user_stats_dict)}"
    )
    return user_stats_dict


def load_ui_features_for_user_item(user_id, item_id, ui_features_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç UI-–ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–∞—Ä—ã user-item
    """
    if not ui_features_path or not os.path.exists(ui_features_path):
        return None

    query = pl.scan_parquet(ui_features_path).filter(
        (pl.col("user_id") == user_id) & (pl.col("item_id") == item_id)
    )
    result = query.collect()

    if len(result) == 0:
        return None

    return result[0].to_dict()


def build_item_features_dict(
    interactions_files, items_df, orders_df, embeddings_dict, device="cuda"
):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Polars + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è item_features_dict.pkl
    """
    import pickle

    import polars as pl

    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–≤–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    # 1. –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –¢–†–ï–ö–ï–†–£ –ò –ó–ê–ö–ê–ó–ê–ú
    item_stats_list = []
    for f in tqdm(interactions_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π"):
        df = pl.read_parquet(f)

        chunk_stats = df.group_by("item_id").agg(
            [
                pl.col("weight").count().alias("count"),
                pl.col("weight").sum().alias("sum"),
                pl.col("weight").max().alias("max"),
                pl.col("weight").min().alias("min"),
                pl.col("timestamp").max().alias("last_ts"),
                pl.col("timestamp").min().alias("first_ts"),
            ]
        )
        item_stats_list.append(chunk_stats)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if item_stats_list:
        all_stats = pl.concat(item_stats_list)
        final_stats = all_stats.group_by("item_id").agg(
            [
                pl.col("count").sum().alias("item_count"),
                pl.col("sum").sum().alias("item_sum"),
                pl.col("max").max().alias("item_max"),
                pl.col("min").min().alias("item_min"),
                pl.col("last_ts").max().alias("item_last_ts"),
                pl.col("first_ts").min().alias("item_first_ts"),
            ]
        )
    else:
        final_stats = pl.DataFrame()

    # 2. –î–û–ë–ê–í–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• –ò–ó –ó–ê–ö–ê–ó–û–í
    if isinstance(orders_df, pl.DataFrame):
        orders_pl = orders_df
    else:
        orders_pl = pl.from_pandas(
            orders_df.compute() if hasattr(orders_df, "compute") else orders_df
        )

    order_stats = orders_pl.group_by("item_id").agg(
        [pl.col("user_id").count().alias("item_orders_count")]
    )

    # 3. –î–û–ë–ê–í–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• –ò–ó items_df
    log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ items_df...")
    if isinstance(items_df, pl.DataFrame):
        items_pl = items_df
    else:
        items_pl = pl.from_pandas(
            items_df.compute() if hasattr(items_df, "compute") else items_df
        )

    items_catalog = items_pl.select(["item_id", "catalogid"]).unique()

    # 4. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í–°–ï–• –î–ê–ù–ù–´–•
    item_stats = final_stats.join(order_stats, on="item_id", how="full")
    item_stats = item_stats.join(items_catalog, on="item_id", how="left")

    # 5. –í–´–ß–ò–°–õ–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
    current_time = pl.lit(datetime.now())

    item_stats = item_stats.with_columns(
        [
            pl.col("item_count").fill_null(0),
            pl.col("item_sum").fill_null(0),
            pl.col("item_orders_count").fill_null(0),
            (pl.col("item_sum") / pl.col("item_count")).alias("item_mean"),
            ((current_time - pl.col("item_last_ts")).dt.total_days()).alias(
                "item_days_since_last"
            ),
            ((current_time - pl.col("item_first_ts")).dt.total_days()).alias(
                "item_days_since_first"
            ),
        ]
    ).fill_nan(0)

    # 6. –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –°–õ–û–í–ê–†–¨
    item_stats_dict = {}
    for row in item_stats.iter_rows(named=True):
        item_stats_dict[row["item_id"]] = {
            "item_count": row["item_count"],
            "item_mean": row["item_mean"],
            "item_sum": row["item_sum"],
            "item_max": row["item_max"],
            "item_min": row["item_min"],
            "item_last_ts": row["item_last_ts"],
            "item_first_ts": row["item_first_ts"],
            "item_orders_count": row["item_orders_count"],
            "item_category": row["catalogid"],
            "item_days_since_last": row["item_days_since_last"],
            "item_days_since_first": row["item_days_since_first"],
        }

    # 7. –î–û–ë–ê–í–õ–ï–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
    log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    for item_id, embedding in embeddings_dict.items():
        if item_id in item_stats_dict:
            for i in range(min(5, len(embedding))):
                item_stats_dict[item_id][f"fclip_embed_{i}"] = float(embedding[i])

    log_message(f"–°–ª–æ–≤–∞—Ä—å —Ç–æ–≤–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω. –ó–∞–ø–∏—Å–µ–π: {len(item_stats_dict)}")

    # 8. –°–û–•–†–ê–ù–ï–ù–ò–ï –í PKL
    output_path = (
        "/home/root6/python/e_cup/rec_system/data/processed/item_features_dict.pkl"
    )
    with open(output_path, "wb") as f:
        pickle.dump(item_stats_dict, f)

    log_message(f"–°–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_path}")
    return item_stats_dict


def get_ui_features_for_user_item(user_id, item_id, ui_features_dir):
    """
    –ò—â–µ—Ç UI-–ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø–∞—Ä—ã user-item –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º
    """
    try:
        metadata_path = os.path.join(ui_features_dir, "metadata.json")
        if not os.path.exists(metadata_path):
            return None

        with open(metadata_path, "r") as f:
            metadata = json.load(f)

        # –ò—â–µ–º –≤ –∫–∞–∂–¥–æ–º —Ñ–∞–π–ª–µ
        for ui_file in metadata["ui_feature_files"]:
            if not os.path.exists(ui_file):
                continue

            result = (
                pl.scan_parquet(ui_file)
                .filter((pl.col("user_id") == user_id) & (pl.col("item_id") == item_id))
                .collect()
            )

            if not result.is_empty():
                return result[0].to_dict()

        return None

    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
        return None


def get_ui_features_batch(user_item_pairs, ui_features_dir, batch_size=1000):
    """
    –ü–æ–ª—É—á–∞–µ—Ç UI-–ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –±–∞—Ç—á–∞ –ø–∞—Ä –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    """
    try:

        metadata_path = os.path.join(ui_features_dir, "metadata.json")
        if not os.path.exists(metadata_path):
            return []

        with open(metadata_path, "r") as f:
            metadata = json.load(f)

        all_results = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        for ui_file in metadata["ui_feature_files"]:
            if not os.path.exists(ui_file):
                continue

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–º–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            temp_pairs_path = "/tmp/filter_pairs.parquet"
            pairs_df = pl.DataFrame(user_item_pairs, schema=["user_id", "item_id"])
            pairs_df.write_parquet(temp_pairs_path)

            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—É—â–µ–º —Ñ–∞–π–ª–µ
            results = (
                pl.scan_parquet(ui_file)
                .join(
                    pl.scan_parquet(temp_pairs_path),
                    on=["user_id", "item_id"],
                    how="inner",
                )
                .collect()
                .to_dicts()
            )

            all_results.extend(results)

            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if os.path.exists(temp_pairs_path):
                os.remove(temp_pairs_path)

        return all_results

    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –±–∞—Ç—á–∞ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
        return []


def build_user_item_features_dict(
    interactions_files,
    output_dir="/home/root6/python/e_cup/rec_system/data/processed/ui_features",
    cleanup=False,
):
    """
    –°–æ–∑–¥–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π.
    –í—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø –¥–ª—è LightGBM.
    """
    import json
    import os
    from datetime import datetime

    import polars as pl
    from tqdm import tqdm

    os.makedirs(output_dir, exist_ok=True)
    log_message("–°–æ–∑–¥–∞–Ω–∏–µ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–∞–Ω–Ω—ã–µ –æ—Å—Ç–∞—é—Ç—Å—è –Ω–∞ –¥–∏—Å–∫–µ)")

    try:
        ui_feature_files = []

        for input_file in tqdm(interactions_files, desc="–°–æ–∑–¥–∞–Ω–∏–µ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤"):
            try:
                input_filename = os.path.basename(input_file)
                output_file = os.path.join(output_dir, f"ui_features_{input_filename}")

                # –ü–æ–ª–∞—Ä—Å –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç UI-–ø—Ä–∏–∑–Ω–∞–∫–∏
                df = (
                    pl.scan_parquet(input_file)
                    .group_by(["user_id", "item_id"])
                    .agg(
                        [
                            pl.col("weight").count().alias("ui_count"),
                            pl.col("weight").sum().alias("ui_sum"),
                            pl.col("weight").max().alias("ui_max"),
                            pl.col("weight").min().alias("ui_min"),
                            pl.col("timestamp").max().alias("ui_last_ts"),
                            pl.col("timestamp").min().alias("ui_first_ts"),
                        ]
                    )
                    .with_columns(
                        [
                            # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                            (pl.col("ui_sum") / pl.col("ui_count"))
                            .fill_nan(0)
                            .alias("ui_mean"),
                            # –î–Ω–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
                            (
                                (
                                    pl.lit(datetime.now()).cast(pl.Datetime)
                                    - pl.col("ui_last_ts").cast(pl.Datetime)
                                )
                                .dt.total_days()  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û: .days() ‚Üí .total_days()
                                .cast(pl.Float64)
                            ).alias("ui_days_since_last"),
                            # –î–Ω–∏ —Å –ø–µ—Ä–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
                            (
                                (
                                    pl.lit(datetime.now()).cast(pl.Datetime)
                                    - pl.col("ui_first_ts").cast(pl.Datetime)
                                )
                                .dt.total_days()  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û: .days() ‚Üí .total_days()
                                .cast(pl.Float64)
                            ).alias("ui_days_since_first"),
                        ]
                    )
                    .select(
                        [
                            "user_id",
                            "item_id",
                            "ui_count",
                            "ui_sum",
                            "ui_max",
                            "ui_min",
                            "ui_mean",
                            "ui_days_since_last",
                            "ui_days_since_first",
                        ]
                    )
                    .fill_null(0)
                )

                df.sink_parquet(output_file)
                ui_feature_files.append(output_file)

            except Exception as e:
                log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {input_file}: {e}")
                continue

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º metadata
        metadata = {
            "created_date": datetime.now().isoformat(),
            "source_files": len(interactions_files),
            "ui_feature_files": ui_feature_files,
            "output_dir": output_dir,
        }
        metadata_path = os.path.join(output_dir, "metadata.json")
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)

        log_message(
            f"–°–æ–∑–¥–∞–Ω–æ {len(ui_feature_files)} —Ñ–∞–π–ª–æ–≤ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤: {output_dir}"
        )
        return output_dir

    except Exception as e:
        log_message(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.log_message_exc()
        return None


def build_category_features_dict(category_df, items_df):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Polars
    """
    import polars as pl

    log_message("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    if not isinstance(category_df, pl.DataFrame):
        category_pl = pl.from_pandas(
            category_df.compute() if hasattr(category_df, "compute") else category_df
        )
    else:
        category_pl = category_df

    if not isinstance(items_df, pl.DataFrame):
        items_pl = pl.from_pandas(
            items_df.compute() if hasattr(items_df, "compute") else items_df
        )
    else:
        items_pl = items_df

    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏—è -> —É—Ä–æ–≤–µ–Ω—å –≤ –∏–µ—Ä–∞—Ä—Ö–∏–∏
    cat_levels = category_pl.with_columns(
        [(pl.col("ids").list.lengths() - 1).alias("category_level")]
    ).select(["catalogid", "category_level"])

    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ —Ç–æ–≤–∞—Ä -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    item_categories = items_pl.select(["item_id", "catalogid"]).unique()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º
    category_features = item_categories.join(cat_levels, on="catalogid", how="left")
    category_features = category_features.with_columns(
        [pl.col("category_level").fill_null(0)]
    )

    # –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –°–õ–û–í–ê–†–¨
    category_features_dict = {}
    for row in category_features.iter_rows(named=True):
        category_features_dict[row["item_id"]] = {
            "item_category": row["catalogid"],
            "category_level": row["category_level"],
        }

    log_message(
        f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã. –ó–∞–ø–∏—Å–µ–π: {len(category_features_dict)}"
    )
    return category_features_dict


def prepare_lgbm_training_data(
    user_features_dict,
    item_features_dict,
    user_item_features_dict,
    category_features_dict,
    test_orders_df,
    all_items,  # —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö item_id
    sample_fraction=0.1,
):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LightGBM:
    1 –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä -> 1 –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–∏–º–µ—Ä.
    """
    log_message("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LightGBM...")

    # –ë–µ—Ä–µ–º sample –æ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–∫–∞–∑–æ–≤
    test_sample = test_orders_df.sample(frac=sample_fraction, random_state=42)

    train_examples = []

    for _, row in test_sample.iterrows():
        user_id = row["user_id"]
        pos_item_id = row["item_id"]

        # ---------- –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π ----------
        features = {}
        if user_id in user_features_dict:
            features.update(user_features_dict[user_id])
        if pos_item_id in item_features_dict:
            features.update(item_features_dict[pos_item_id])
        if (user_id, pos_item_id) in user_item_features_dict:
            features.update(user_item_features_dict[(user_id, pos_item_id)])
        if pos_item_id in category_features_dict:
            features.update(category_features_dict[pos_item_id])

        features["target"] = 1
        features["user_id"] = user_id
        features["item_id"] = pos_item_id
        train_examples.append(features)

        # ---------- –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π ----------
        neg_item_id = random.choice(all_items)
        while neg_item_id == pos_item_id:
            neg_item_id = random.choice(all_items)

        neg_features = {}
        if user_id in user_features_dict:
            neg_features.update(user_features_dict[user_id])
        if neg_item_id in item_features_dict:
            neg_features.update(item_features_dict[neg_item_id])
        if (user_id, neg_item_id) in user_item_features_dict:
            neg_features.update(user_item_features_dict[(user_id, neg_item_id)])
        if neg_item_id in category_features_dict:
            neg_features.update(category_features_dict[neg_item_id])

        neg_features["target"] = 0
        neg_features["user_id"] = user_id
        neg_features["item_id"] = neg_item_id
        train_examples.append(neg_features)

    # –°–æ–∑–¥–∞–µ–º DataFrame
    train_df = pd.DataFrame(train_examples)

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns
    train_df[numeric_cols] = train_df[numeric_cols].fillna(0)

    log_message(
        f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã. –†–∞–∑–º–µ—Ä: {len(train_df)} (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {len(test_sample)}, –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {len(test_sample)})"
    )
    return train_df


def load_and_process_embeddings(
    items_ddf, embedding_column="fclip_embed", device="cuda", max_items=0
):
    """
    –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å item_id -> np.array
    """
    log_message("–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

    if max_items > 0:
        items_sample = items_ddf[["item_id", embedding_column]].head(
            max_items, compute=True
        )
    else:
        items_sample = items_ddf[["item_id", embedding_column]].compute()

    embeddings_dict = {}
    for row in tqdm(
        items_sample.itertuples(index=False),
        total=len(items_sample),
        desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
    ):
        item_id = row.item_id
        embedding_data = getattr(row, embedding_column, None)
        if embedding_data is None:
            continue
        try:
            if isinstance(embedding_data, str):
                embedding = np.fromstring(
                    embedding_data.strip("[]"), sep=",", dtype=np.float32
                )
            elif isinstance(embedding_data, list):
                embedding = np.array(embedding_data, dtype=np.float32)
            elif isinstance(embedding_data, np.ndarray):
                embedding = embedding_data.astype(np.float32)
            else:
                continue
            if embedding.size > 0:
                embeddings_dict[item_id] = embedding
        except Exception:
            continue

    log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(embeddings_dict)} —Ç–æ–≤–∞—Ä–æ–≤")
    return embeddings_dict


# -------------------- –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ --------------------
if __name__ == "__main__":
    start_time = time.time()

    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_file = "/home/root6/python/e_cup/rec_system/training_log.txt"

    def log_message(message):
        """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–∞–π–ª –∏ –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        full_message = f"[{timestamp}] {message}"
        print(full_message)
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(full_message + "\n")

    # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –ª–æ–≥–∞ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ
    with open(log_file, "w", encoding="utf-8") as f:
        f.write(
            f"=== –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n\n"
        )

    try:
        K = 100
        RECENT_N = 5
        TEST_SIZE = 0.2

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        SCALING_STAGE = "full"  # small, medium, large, full

        scaling_config = {
            "small": {"sample_users": 500, "sample_fraction": 0.1},
            "medium": {"sample_users": 5000, "sample_fraction": 0.3},
            "large": {"sample_users": 20000, "sample_fraction": 0.7},
            "full": {"sample_users": None, "sample_fraction": 1.0},
        }

        config = scaling_config[SCALING_STAGE]

        log_message(f"=== –†–ï–ñ–ò–ú –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–Ø: {SCALING_STAGE.upper()} ===")
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {config['sample_users'] or '–≤—Å–µ'}")
        log_message(f"–î–∞–Ω–Ω—ã—Ö: {config['sample_fraction']*100}%")

        # === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===")
        orders_ddf, tracker_ddf, items_ddf, categories_ddf, test_users_ddf = (
            load_train_data()
        )
        orders_ddf, tracker_ddf, items_ddf = filter_data(
            orders_ddf, tracker_ddf, items_ddf
        )
        stage_time = time.time() - stage_start
        log_message(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

        # === –ó–ê–ì–†–£–ó–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ===
        stage_start = time.time()
        log_message("=== –ó–ê–ì–†–£–ó–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ===")
        embeddings_dict = load_and_process_embeddings(items_ddf)
        stage_time = time.time() - stage_start
        log_message(
            f"–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}"
        )
        log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings_dict)}")

        # === SPLIT –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== SPLIT –î–ê–ù–ù–´–• ===")
        orders_df_full = orders_ddf.compute()
        train_orders_df, test_orders_df, cutoff_ts_per_user = train_test_split_by_time(
            orders_df_full, TEST_SIZE
        )
        stage_time = time.time() - stage_start
        log_message(f"Split –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {timedelta(seconds=stage_time)}")
        log_message(
            f"Train orders: {len(train_orders_df)}, Test orders: {len(test_orders_df)}"
        )

        # === –ü–û–î–ì–û–¢–û–í–ö–ê –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–ô ===
        stage_start = time.time()
        log_message("=== –ü–û–î–ì–û–¢–û–í–ö–ê –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–ô ===")
        interactions_files = prepare_interactions(
            train_orders_df, tracker_ddf, cutoff_ts_per_user, scale_days=30
        )
        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}"
        )
        log_message(f"–°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {len(interactions_files)}")

        # === –ü–û–°–õ–ï–î–ù–ò–ï –¢–û–í–ê–†–´ ===
        stage_start = time.time()
        log_message("=== –ü–û–°–õ–ï–î–ù–ò–ï –¢–û–í–ê–†–´ ===")
        batch_dir = "/home/root6/python/e_cup/rec_system/data/processed/prepare_interactions_batches"
        recent_items_map = build_recent_items_map_from_batches(
            batch_dir, recent_n=RECENT_N
        )
        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ recent items map –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}"
        )
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å recent items: {len(recent_items_map)}")

        # === –û–ë–£–ß–ï–ù–ò–ï ALS –î–õ–Ø –ü–†–ò–ó–ù–ê–ö–û–í ===
        stage_start = time.time()
        log_message("=== –û–ë–£–ß–ï–ù–ò–ï ALS –î–õ–Ø –ü–†–ò–ó–ù–ê–ö–û–í ===")
        model, user_map, item_map = train_als(
            interactions_files, n_factors=64, reg=1e-3, device="cuda"
        )
        inv_item_map = {v: k for k, v in item_map.items()}
        popularity_s = compute_global_popularity(
            orders_df_full, cutoff_ts_per_user
        )  # —Ç–µ–ø–µ—Ä—å —ç—Ç–æ pd.Timestamp
        popular_items = popularity_s.index.tolist()
        save_path = (
            "/home/root6/python/e_cup/rec_system/data/processed/popular_items.pkl"
        )
        with open(save_path, "wb") as f:
            pickle.dump(popular_items, f)
        stage_time = time.time() - stage_start
        log_message(f"–û–±—É—á–µ–Ω–∏–µ ALS –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}")
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_map)}, –¢–æ–≤–∞—Ä–æ–≤: {len(item_map)}")

        # === –ü–û–°–¢–†–û–ï–ù–ò–ï –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== –ü–û–°–¢–†–û–ï–ù–ò–ï –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –î–ê–ù–ù–´–• ===")

        # –°—Ç—Ä–æ–∏–º co-purchase map
        copurchase_map = build_copurchase_map(train_orders_df)
        log_message(f"Co-purchase map –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(copurchase_map)} —Ç–æ–≤–∞—Ä–æ–≤")

        # –°—Ç—Ä–æ–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
        items_df = items_ddf.compute()
        categories_df = categories_ddf.compute()
        item_to_cat, cat_to_items = build_category_maps(items_df, categories_df)
        log_message(
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã: {len(item_to_cat)} —Ç–æ–≤–∞—Ä–æ–≤, {len(cat_to_items)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π"
        )

        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø LGBM ===
        stage_start = time.time()
        log_message("=== –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø LGBM ===")

        # User features
        user_start = time.time()
        user_features_dict = build_user_features_dict(interactions_files, orders_ddf)
        user_time = time.time() - user_start
        log_message(
            f"User features –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∑–∞ {timedelta(seconds=user_time)}: {len(user_features_dict)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"
        )

        # Item features
        item_start = time.time()
        item_features_dict = build_item_features_dict(
            interactions_files, items_df, orders_ddf, embeddings_dict
        )
        item_time = time.time() - item_start
        log_message(
            f"Item features –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∑–∞ {timedelta(seconds=item_time)}: {len(item_features_dict)} —Ç–æ–≤–∞—Ä–æ–≤"
        )

        # User-Item features - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        ui_start = time.time()
        ui_features_dir = build_user_item_features_dict(
            interactions_files,
            output_dir="/home/root6/python/e_cup/rec_system/data/processed/ui_features_distributed",
        )

        if ui_features_dir is None:
            log_message("‚ö†Ô∏è User-Item features –Ω–µ —Å–æ–∑–¥–∞–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–∞–ø.")
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ–∑–¥–∞–Ω–∞ –∏ –µ—Å—Ç—å —Ñ–∞–π–ª—ã
            metadata_path = os.path.join(ui_features_dir, "metadata.json")
            if os.path.exists(metadata_path):
                with open(metadata_path, "r") as f:
                    metadata = json.load(f)
                file_count = len(metadata.get("ui_feature_files", []))
                log_message(f"User-Item features —Å–æ–∑–¥–∞–Ω—ã –≤: {ui_features_dir}")
                log_message(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤: {file_count}")
            else:
                log_message("‚ö†Ô∏è –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ UI-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                ui_features_dir = None

        ui_time = time.time() - ui_start
        log_message(f"User-Item features –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∑–∞ {timedelta(seconds=ui_time)}")

        stage_time = time.time() - stage_start
        log_message(
            f"–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø LightGBM ===
        stage_start = time.time()
        log_message("=== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø LightGBM ===")
        recommender = LightGBMRecommender()
        recommender.set_als_embeddings(model)
        recommender.set_additional_data(
            copurchase_map, item_to_cat, cat_to_items, user_map, item_map
        )

        if embeddings_dict:
            recommender.set_external_embeddings(embeddings_dict)

        # –ú–ê–°–®–¢–ê–ë–ò–†–£–ï–ú –¥–∞–Ω–Ω—ã–µ
        if config["sample_users"]:
            sample_test_orders = test_orders_df.sample(
                min(config["sample_users"], len(test_orders_df)), random_state=42
            )
        else:
            sample_test_orders = test_orders_df

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å UI-–ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        train_data, val_data = recommender.prepare_training_data(
            interactions_files=interactions_files,
            orders_ddf=orders_ddf,
            user_map=user_map,
            item_map=item_map,
            popularity_s=popularity_s,
            recent_items_map=recent_items_map,
            sample_fraction=config["sample_fraction"],
            negatives_per_positive=3,
            ui_features_dir=ui_features_dir,
            val_days=7,
        )

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation
        users = train_data["user_id"].unique()
        train_users, val_users = train_test_split(users, test_size=0.2, random_state=42)

        train_df = train_data[train_data["user_id"].isin(train_users)]
        val_df = train_data[train_data["user_id"].isin(val_users)]

        log_message(f"–†–∞–∑–º–µ—Ä train: {len(train_df)}, validation: {len(val_df)}")
        log_message(f"–ü—Ä–∏–∑–Ω–∞–∫–∏: {len(recommender.feature_columns)}")
        stage_time = time.time() - stage_start
        log_message(
            f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LightGBM –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í ===
        stage_start = time.time()
        log_message("=== –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê FEATURE GENERATION ===")

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ user features
        log_message("--- –ü–†–û–í–ï–†–ö–ê USER FEATURES ---")
        if user_features_dict:
            sample_user = list(user_features_dict.keys())[0]
            user_feats = user_features_dict[sample_user]
            log_message(f"–ü—Ä–∏–º–µ—Ä user features –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {sample_user}:")
            for feat, value in user_feats.items():
                log_message(f"  {feat}: {value}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ user features
            users_with_features = len(user_features_dict)
            users_with_real_features = sum(
                1
                for feats in user_features_dict.values()
                if any(v != 0 for v in feats.values())
            )
            log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å features: {users_with_features}")
            log_message(
                f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –ù–ï–Ω—É–ª–µ–≤—ã–º–∏ features: {users_with_real_features}"
            )
        else:
            log_message("‚ö†Ô∏è user_features_dict –ü–£–°–¢–û–ô!")

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ item features
        log_message("--- –ü–†–û–í–ï–†–ö–ê ITEM FEATURES ---")
        if item_features_dict:
            sample_item = list(item_features_dict.keys())[0]
            item_feats = item_features_dict[sample_item]
            log_message(f"–ü—Ä–∏–º–µ—Ä item features –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {sample_item}:")
            for feat, value in item_feats.items():
                log_message(f"  {feat}: {value}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ item features
            items_with_features = len(item_features_dict)
            items_with_real_features = sum(
                1
                for feats in item_features_dict.values()
                if any(v != 0 for v in feats.values())
            )
            log_message(f"–¢–æ–≤–∞—Ä–æ–≤ —Å features: {items_with_features}")
            log_message(f"–¢–æ–≤–∞—Ä–æ–≤ —Å –ù–ï–Ω—É–ª–µ–≤—ã–º–∏ features: {items_with_real_features}")
        else:
            log_message("‚ö†Ô∏è item_features_dict –ü–£–°–¢–û–ô!")

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ UI features
        log_message("--- –ü–†–û–í–ï–†–ö–ê UI FEATURES ---")
        if ui_features_dir and os.path.exists(ui_features_dir):
            metadata_path = os.path.join(ui_features_dir, "metadata.json")
            if os.path.exists(metadata_path):
                with open(metadata_path, "r") as f:
                    metadata = json.load(f)
                log_message(
                    f"UI features files: {len(metadata.get('ui_feature_files', []))}"
                )
                log_message(f"UI features stats: {metadata.get('stats', {})}")
            else:
                log_message("‚ö†Ô∏è –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ UI features –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        else:
            log_message("‚ö†Ô∏è UI features directory –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        log_message("--- –ü–†–û–í–ï–†–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ---")
        if embeddings_dict:
            sample_item = list(embeddings_dict.keys())[0]
            embedding = embeddings_dict[sample_item]
            log_message(
                f"–ü—Ä–∏–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {sample_item}: shape {embedding.shape}"
            )
            log_message(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(embeddings_dict)}")
            log_message(f"–ü—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏–π: {embedding[:5]}")
        else:
            log_message("‚ö†Ô∏è embeddings_dict –ü–£–°–¢–û–ô!")

        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ co-purchase map
        log_message("--- –ü–†–û–í–ï–†–ö–ê CO-PURCHASE MAP ---")
        if copurchase_map:
            sample_item = list(copurchase_map.keys())[0]
            co_items = copurchase_map[sample_item]
            log_message(
                f"–ü—Ä–∏–º–µ—Ä co-purchase –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {sample_item}: {len(co_items)} —Ç–æ–≤–∞—Ä–æ–≤"
            )
            log_message(f"Co-purchase –∑–∞–ø–∏—Å–µ–π: {len(copurchase_map)}")
        else:
            log_message("‚ö†Ô∏è copurchase_map –ü–£–°–¢–û–ô!")

        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤
        log_message("--- –ü–†–û–í–ï–†–ö–ê –ö–ê–¢–ï–ì–û–†–ò–ô–ù–´–• –ú–ê–ü–ü–ò–ù–ì–û–í ---")
        if item_to_cat and cat_to_items:
            sample_item = list(item_to_cat.keys())[0]
            cat_id = item_to_cat[sample_item]
            cat_items = cat_to_items.get(cat_id, [])
            log_message(f"–¢–æ–≤–∞—Ä {sample_item} -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è {cat_id}")
            log_message(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {cat_id} -> {len(cat_items)} —Ç–æ–≤–∞—Ä–æ–≤")
            log_message(f"–¢–æ–≤–∞—Ä–æ–≤ –≤ –º–∞–ø–ø–∏–Ω–≥–µ: {len(item_to_cat)}")
            log_message(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –º–∞–ø–ø–∏–Ω–≥–µ: {len(cat_to_items)}")
        else:
            log_message("‚ö†Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ –ü–£–°–¢–´–ï!")

        stage_time = time.time() - stage_start
        log_message(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

        # === –û–ë–£–ß–ï–ù–ò–ï LightGBM ===
        stage_start = time.time()
        log_message("=== –û–ë–£–ß–ï–ù–ò–ï LightGBM ===")
        model = recommender.train(train_df, val_df)
        stage_time = time.time() - stage_start
        log_message(f"–û–±—É—á–µ–Ω–∏–µ LightGBM –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}")

        # === –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò ===
        stage_start = time.time()
        log_message("=== –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò ===")
        train_ndcg = recommender.evaluate(train_df)
        val_ndcg = recommender.evaluate(val_df)

        log_message(f"NDCG@100 train: {train_ndcg:.4f}")
        log_message(f"NDCG@100 val: {val_ndcg:.4f}")
        stage_time = time.time() - stage_start
        log_message(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timedelta(seconds=stage_time)}")

        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        stage_start = time.time()
        log_message("=== –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í ===")
        feature_importance = pd.DataFrame(
            {
                "feature": recommender.feature_columns,
                "importance": recommender.model.feature_importance(),
            }
        )
        feature_importance = feature_importance.sort_values(
            "importance", ascending=False
        )
        top_features = feature_importance.head(20)
        log_message("–¢–æ–ø-20 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for i, row in top_features.iterrows():
            log_message(f"  {row['feature']}: {row['importance']}")
        stage_time = time.time() - stage_start
        log_message(
            f"–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {timedelta(seconds=stage_time)}"
        )

        # === –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –í–ê–ñ–ù–´–• –î–ê–ù–ù–´–• ===
        stage_start = time.time()
        log_message("=== –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –ü–†–ò–ó–ù–ê–ö–û–í ===")
        save_data = {
            "lgbm_model": recommender.model,
            "feature_columns": recommender.feature_columns,
            "als_model": model,
            "user_map": user_map,
            "item_map": item_map,
            "inv_item_map": inv_item_map,
            "popular_items": popular_items,
            "user_features_dict": user_features_dict,
            "item_features_dict": item_features_dict,
            "ui_features_dir": ui_features_dir,
            "recent_items_map": recent_items_map,
            "copurchase_map": copurchase_map,
            "item_to_cat": item_to_cat,
        }

        model_path = (
            "/home/root6/python/e_cup/rec_system/src/models/lgbm_model_full.pkl"
        )
        with open(model_path, "wb") as f:
            pickle.dump(save_data, f, protocol=pickle.HIGHEST_PROTOCOL)

        stage_time = time.time() - stage_start
        log_message(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {timedelta(seconds=stage_time)}")
        log_message(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_path}")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        all_items = set()

        # === –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===
        total_time = time.time() - start_time
        log_message("=== –û–ë–£–ß–ï–ù–ò–ï –ò –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ó–ê–í–ï–†–®–ï–ù–´ –£–°–ü–ï–®–ù–û ===")
        log_message(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {timedelta(seconds=total_time)}")
        log_message(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_map)}")
        log_message(f"–¢–æ–≤–∞—Ä–æ–≤: {len(item_map)}")
        log_message(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(recommender.feature_columns)}")
        log_message(f"NDCG@100 train: {train_ndcg:.4f}")
        log_message(f"NDCG@100 val: {val_ndcg:.4f}")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
        log_message("=== –°–ò–°–¢–ï–ú–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø ===")
        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
            import torch

            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                log_message(f"–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: {gpu_name}")
                log_message(f"–ü–∞–º—è—Ç—å GPU: {gpu_memory:.1f} GB")
            else:
                log_message("–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
        except Exception:
            log_message("–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CPU
            import multiprocessing

            import psutil

            cpu_freq = psutil.cpu_freq()
            cpu_cores = multiprocessing.cpu_count()
            log_message(f"–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä: {psutil.cpu_percent()}% –∑–∞–≥—Ä—É–∑–∫–∏")
            log_message(f"–Ø–¥—Ä–∞ CPU: {cpu_cores}")
            if cpu_freq:
                log_message(f"–ß–∞—Å—Ç–æ—Ç–∞ CPU: {cpu_freq.current:.1f} MHz")
        except Exception:
            log_message("–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ RAM
            import psutil

            ram = psutil.virtual_memory()
            ram_total = ram.total / 1024**3
            ram_used = ram.used / 1024**3
            log_message(
                f"–û–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å: {ram_total:.1f} GB –≤—Å–µ–≥–æ, {ram_used:.1f} GB –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ"
            )
            log_message(f"–ß–∞—Å—Ç–æ—Ç–∞ RAM: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫")
        except Exception:
            log_message("–û–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        log_message("==========================================")
        log_message("–í–°–ï –≠–¢–ê–ü–´ –í–´–ü–û–õ–ù–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        log_message("==========================================")

    except Exception as e:
        error_time = time.time() - start_time
        log_message(f"!!! –û–®–ò–ë–ö–ê –í–´–ü–û–õ–ù–ï–ù–ò–Ø !!!")
        log_message(f"–û—à–∏–±–∫–∞: {str(e)}")
        log_message(f"–í—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {timedelta(seconds=error_time)}")
        log_message("–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
        import traceback

        traceback_str = traceback.format_exc()
        log_message(traceback_str)

    finally:
        # –í—Å–µ–≥–¥–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –≤—Ä–µ–º—è
        total_time = time.time() - start_time
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(
                f"\n=== –û–ë–©–ï–ï –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø: {timedelta(seconds=total_time)} ===\n"
            )
